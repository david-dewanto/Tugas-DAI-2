{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR1JW69eLfG_"
      },
      "source": [
        "# IF3070 Foundations of Artificial Intelligence | Tugas Besar 2\n",
        "\n",
        "This notebook serves as a template for the assignment. Please create a copy of this notebook to complete your work. You can add more code blocks, markdown blocks, or new sections if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucbaI5rBLtjJ"
      },
      "source": [
        "Group Number: xx\n",
        "\n",
        "Group Members:\n",
        "- Name (NIM)\n",
        "- Name (NIM)\n",
        "- ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzsfETHLfHA"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "jZJU5W_4LfHB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import ipaddress\n",
        "import math\n",
        "import unicodedata\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from urllib.parse import unquote, urlparse, parse_qs\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# Import other libraries if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKbjLIdYLfHC"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "-IWFJ-gdLfHD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (140404, 55)\n",
            "\n",
            "Column info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 140404 entries, 0 to 140403\n",
            "Data columns (total 55 columns):\n",
            " #   Column                      Non-Null Count   Dtype  \n",
            "---  ------                      --------------   -----  \n",
            " 0   id                          140404 non-null  int64  \n",
            " 1   URL                         96917 non-null   object \n",
            " 2   URLLength                   79765 non-null   float64\n",
            " 3   Domain                      70207 non-null   object \n",
            " 4   DomainLength                94085 non-null   float64\n",
            " 5   IsDomainIP                  98274 non-null   float64\n",
            " 6   TLD                         95005 non-null   object \n",
            " 7   CharContinuationRate        92362 non-null   float64\n",
            " 8   TLDLegitimateProb           87531 non-null   float64\n",
            " 9   URLCharProb                 88333 non-null   float64\n",
            " 10  TLDLength                   92673 non-null   float64\n",
            " 11  NoOfSubDomain               96344 non-null   float64\n",
            " 12  HasObfuscation              74684 non-null   float64\n",
            " 13  NoOfObfuscatedChar          73606 non-null   float64\n",
            " 14  ObfuscationRatio            75806 non-null   float64\n",
            " 15  NoOfLettersInURL            77066 non-null   float64\n",
            " 16  LetterRatioInURL            74658 non-null   float64\n",
            " 17  NoOfDegitsInURL             81594 non-null   float64\n",
            " 18  DegitRatioInURL             86896 non-null   float64\n",
            " 19  NoOfEqualsInURL             78826 non-null   float64\n",
            " 20  NoOfQMarkInURL              96303 non-null   float64\n",
            " 21  NoOfAmpersandInURL          95017 non-null   float64\n",
            " 22  NoOfOtherSpecialCharsInURL  92775 non-null   float64\n",
            " 23  SpacialCharRatioInURL       77570 non-null   float64\n",
            " 24  IsHTTPS                     91042 non-null   float64\n",
            " 25  LineOfCode                  71251 non-null   float64\n",
            " 26  LargestLineLength           72476 non-null   float64\n",
            " 27  HasTitle                    95825 non-null   float64\n",
            " 28  Title                       82157 non-null   object \n",
            " 29  DomainTitleMatchScore       90407 non-null   float64\n",
            " 30  URLTitleMatchScore          88188 non-null   float64\n",
            " 31  HasFavicon                  81982 non-null   float64\n",
            " 32  Robots                      93672 non-null   float64\n",
            " 33  IsResponsive                97862 non-null   float64\n",
            " 34  NoOfURLRedirect             73020 non-null   float64\n",
            " 35  NoOfSelfRedirect            73689 non-null   float64\n",
            " 36  HasDescription              85765 non-null   float64\n",
            " 37  NoOfPopup                   97051 non-null   float64\n",
            " 38  NoOfiFrame                  90460 non-null   float64\n",
            " 39  HasExternalFormSubmit       84812 non-null   float64\n",
            " 40  HasSocialNet                72405 non-null   float64\n",
            " 41  HasSubmitButton             78784 non-null   float64\n",
            " 42  HasHiddenFields             96609 non-null   float64\n",
            " 43  HasPasswordField            73869 non-null   float64\n",
            " 44  Bank                        85408 non-null   float64\n",
            " 45  Pay                         97230 non-null   float64\n",
            " 46  Crypto                      90207 non-null   float64\n",
            " 47  HasCopyrightInfo            73059 non-null   float64\n",
            " 48  NoOfImage                   89932 non-null   float64\n",
            " 49  NoOfCSS                     73270 non-null   float64\n",
            " 50  NoOfJS                      79603 non-null   float64\n",
            " 51  NoOfSelfRef                 92272 non-null   float64\n",
            " 52  NoOfEmptyRef                97718 non-null   float64\n",
            " 53  NoOfExternalRef             71025 non-null   float64\n",
            " 54  label                       140404 non-null  int64  \n",
            "dtypes: float64(49), int64(2), object(4)\n",
            "memory usage: 58.9+ MB\n",
            "None\n",
            "\n",
            "Class distribution:\n",
            "label\n",
            "1    0.924831\n",
            "0    0.075169\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>URL</th>\n",
              "      <th>URLLength</th>\n",
              "      <th>Domain</th>\n",
              "      <th>DomainLength</th>\n",
              "      <th>IsDomainIP</th>\n",
              "      <th>TLD</th>\n",
              "      <th>CharContinuationRate</th>\n",
              "      <th>TLDLegitimateProb</th>\n",
              "      <th>URLCharProb</th>\n",
              "      <th>...</th>\n",
              "      <th>Pay</th>\n",
              "      <th>Crypto</th>\n",
              "      <th>HasCopyrightInfo</th>\n",
              "      <th>NoOfImage</th>\n",
              "      <th>NoOfCSS</th>\n",
              "      <th>NoOfJS</th>\n",
              "      <th>NoOfSelfRef</th>\n",
              "      <th>NoOfEmptyRef</th>\n",
              "      <th>NoOfExternalRef</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>https://www.northcm.ac.th</td>\n",
              "      <td>24.0</td>\n",
              "      <td>www.northcm.ac.th</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>69.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>http://uqr.to/1il1z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>to</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>0.036850</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>https://www.woolworthsrewards.com.au</td>\n",
              "      <td>35.0</td>\n",
              "      <td>www.woolworthsrewards.com.au</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>au</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.060894</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>31.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>com</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.522907</td>\n",
              "      <td>0.055829</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>www.nyprowrestling.com</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 55 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                   URL  URLLength  \\\n",
              "0   1             https://www.northcm.ac.th       24.0   \n",
              "1   4                   http://uqr.to/1il1z        NaN   \n",
              "2   5  https://www.woolworthsrewards.com.au       35.0   \n",
              "3   6                                   NaN       31.0   \n",
              "4  11                                   NaN        NaN   \n",
              "\n",
              "                         Domain  DomainLength  IsDomainIP  TLD  \\\n",
              "0             www.northcm.ac.th          17.0         0.0  NaN   \n",
              "1                           NaN           NaN         NaN   to   \n",
              "2  www.woolworthsrewards.com.au          28.0         0.0   au   \n",
              "3                           NaN           NaN         NaN  com   \n",
              "4        www.nyprowrestling.com          22.0         0.0  NaN   \n",
              "\n",
              "   CharContinuationRate  TLDLegitimateProb  URLCharProb  ...  Pay  Crypto  \\\n",
              "0              0.800000                NaN          NaN  ...  0.0     0.0   \n",
              "1              1.000000           0.000896     0.036850  ...  NaN     0.0   \n",
              "2              0.857143                NaN     0.060894  ...  1.0     0.0   \n",
              "3              0.562500           0.522907     0.055829  ...  1.0     0.0   \n",
              "4              1.000000                NaN          NaN  ...  0.0     0.0   \n",
              "\n",
              "   HasCopyrightInfo  NoOfImage  NoOfCSS  NoOfJS  NoOfSelfRef  NoOfEmptyRef  \\\n",
              "0               1.0        NaN      3.0     NaN         69.0           NaN   \n",
              "1               0.0        NaN      NaN     NaN          NaN           NaN   \n",
              "2               1.0       33.0      7.0     8.0         15.0           NaN   \n",
              "3               1.0       24.0      5.0    14.0          NaN           NaN   \n",
              "4               1.0        NaN      NaN    14.0          NaN           0.0   \n",
              "\n",
              "   NoOfExternalRef  label  \n",
              "0              NaN      1  \n",
              "1              1.0      0  \n",
              "2              2.0      1  \n",
              "3              NaN      1  \n",
              "4              NaN      1  \n",
              "\n",
              "[5 rows x 55 columns]"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import PhiUSIIL Phishing URL Dataset\n",
        "df = pd.read_csv('train.csv') # Import The Dataset Given by The Assistants (From The Spesification Docs)\n",
        "df_original = df.copy()\n",
        "\n",
        "# Modify Dataset (Because filename is a feature that's shouldn't be included --> From the metadata of the Dataset)\n",
        "df = df.drop('FILENAME', axis=1)\n",
        "\n",
        "# Number of rows and columns\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "# Column names and their data types\n",
        "print(\"\\nColumn info:\")\n",
        "print(df.info())\n",
        "\n",
        "# Class distribution (legitimate vs phishing)\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df['label'].value_counts(normalize=True))\n",
        "\n",
        "# Print The First 5 Rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_data_types(df):\n",
        "    # Create a dataframe copy to avoid modifying the original\n",
        "    df_correct = df.copy()\n",
        "\n",
        "    # Binary features to convert to boolean\n",
        "    binary_features = [\n",
        "        'HasTitle', 'HasFavicon', 'HasDescription', 'HasSocialNet',\n",
        "        'HasSubmitButton', 'HasHiddenFields', 'HasPasswordField',\n",
        "        'HasCopyrightInfo', 'HasExternalFormSubmit', 'IsDomainIP',\n",
        "        'IsHTTPS', 'IsResponsive', 'HasObfuscation', 'Bank', 'Pay',\n",
        "        'Crypto', 'Robots', 'label'\n",
        "    ]\n",
        "\n",
        "    # Text features to convert to string\n",
        "    text_features = ['URL', 'Domain', 'Title']\n",
        "\n",
        "    # Convert binary features to boolean\n",
        "    for col in binary_features:\n",
        "        df_correct[col] = df_correct[col].astype(\"boolean\")\n",
        "    \n",
        "    # Convert text features to string\n",
        "    for col in text_features:\n",
        "        if col in df_correct.columns:\n",
        "            df_correct[col] = df_correct[col].astype(\"string\")\n",
        "\n",
        "    return df_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (140404, 55)\n",
            "\n",
            "Column info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 140404 entries, 0 to 140403\n",
            "Data columns (total 55 columns):\n",
            " #   Column                      Non-Null Count   Dtype  \n",
            "---  ------                      --------------   -----  \n",
            " 0   id                          140404 non-null  int64  \n",
            " 1   URL                         96917 non-null   string \n",
            " 2   URLLength                   79765 non-null   float64\n",
            " 3   Domain                      70207 non-null   string \n",
            " 4   DomainLength                94085 non-null   float64\n",
            " 5   IsDomainIP                  98274 non-null   boolean\n",
            " 6   TLD                         95005 non-null   object \n",
            " 7   CharContinuationRate        92362 non-null   float64\n",
            " 8   TLDLegitimateProb           87531 non-null   float64\n",
            " 9   URLCharProb                 88333 non-null   float64\n",
            " 10  TLDLength                   92673 non-null   float64\n",
            " 11  NoOfSubDomain               96344 non-null   float64\n",
            " 12  HasObfuscation              74684 non-null   boolean\n",
            " 13  NoOfObfuscatedChar          73606 non-null   float64\n",
            " 14  ObfuscationRatio            75806 non-null   float64\n",
            " 15  NoOfLettersInURL            77066 non-null   float64\n",
            " 16  LetterRatioInURL            74658 non-null   float64\n",
            " 17  NoOfDegitsInURL             81594 non-null   float64\n",
            " 18  DegitRatioInURL             86896 non-null   float64\n",
            " 19  NoOfEqualsInURL             78826 non-null   float64\n",
            " 20  NoOfQMarkInURL              96303 non-null   float64\n",
            " 21  NoOfAmpersandInURL          95017 non-null   float64\n",
            " 22  NoOfOtherSpecialCharsInURL  92775 non-null   float64\n",
            " 23  SpacialCharRatioInURL       77570 non-null   float64\n",
            " 24  IsHTTPS                     91042 non-null   boolean\n",
            " 25  LineOfCode                  71251 non-null   float64\n",
            " 26  LargestLineLength           72476 non-null   float64\n",
            " 27  HasTitle                    95825 non-null   boolean\n",
            " 28  Title                       82157 non-null   string \n",
            " 29  DomainTitleMatchScore       90407 non-null   float64\n",
            " 30  URLTitleMatchScore          88188 non-null   float64\n",
            " 31  HasFavicon                  81982 non-null   boolean\n",
            " 32  Robots                      93672 non-null   boolean\n",
            " 33  IsResponsive                97862 non-null   boolean\n",
            " 34  NoOfURLRedirect             73020 non-null   float64\n",
            " 35  NoOfSelfRedirect            73689 non-null   float64\n",
            " 36  HasDescription              85765 non-null   boolean\n",
            " 37  NoOfPopup                   97051 non-null   float64\n",
            " 38  NoOfiFrame                  90460 non-null   float64\n",
            " 39  HasExternalFormSubmit       84812 non-null   boolean\n",
            " 40  HasSocialNet                72405 non-null   boolean\n",
            " 41  HasSubmitButton             78784 non-null   boolean\n",
            " 42  HasHiddenFields             96609 non-null   boolean\n",
            " 43  HasPasswordField            73869 non-null   boolean\n",
            " 44  Bank                        85408 non-null   boolean\n",
            " 45  Pay                         97230 non-null   boolean\n",
            " 46  Crypto                      90207 non-null   boolean\n",
            " 47  HasCopyrightInfo            73059 non-null   boolean\n",
            " 48  NoOfImage                   89932 non-null   float64\n",
            " 49  NoOfCSS                     73270 non-null   float64\n",
            " 50  NoOfJS                      79603 non-null   float64\n",
            " 51  NoOfSelfRef                 92272 non-null   float64\n",
            " 52  NoOfEmptyRef                97718 non-null   float64\n",
            " 53  NoOfExternalRef             71025 non-null   float64\n",
            " 54  label                       140404 non-null  boolean\n",
            "dtypes: boolean(18), float64(32), int64(1), object(1), string(3)\n",
            "memory usage: 44.5+ MB\n",
            "None\n",
            "\n",
            "Class distribution:\n",
            "label\n",
            "True     0.924831\n",
            "False    0.075169\n",
            "Name: proportion, dtype: Float64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>URL</th>\n",
              "      <th>URLLength</th>\n",
              "      <th>Domain</th>\n",
              "      <th>DomainLength</th>\n",
              "      <th>IsDomainIP</th>\n",
              "      <th>TLD</th>\n",
              "      <th>CharContinuationRate</th>\n",
              "      <th>TLDLegitimateProb</th>\n",
              "      <th>URLCharProb</th>\n",
              "      <th>...</th>\n",
              "      <th>Pay</th>\n",
              "      <th>Crypto</th>\n",
              "      <th>HasCopyrightInfo</th>\n",
              "      <th>NoOfImage</th>\n",
              "      <th>NoOfCSS</th>\n",
              "      <th>NoOfJS</th>\n",
              "      <th>NoOfSelfRef</th>\n",
              "      <th>NoOfEmptyRef</th>\n",
              "      <th>NoOfExternalRef</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>https://www.northcm.ac.th</td>\n",
              "      <td>24.0</td>\n",
              "      <td>www.northcm.ac.th</td>\n",
              "      <td>17.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>69.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>http://uqr.to/1il1z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>to</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>0.036850</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>https://www.woolworthsrewards.com.au</td>\n",
              "      <td>35.0</td>\n",
              "      <td>www.woolworthsrewards.com.au</td>\n",
              "      <td>28.0</td>\n",
              "      <td>False</td>\n",
              "      <td>au</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.060894</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>33.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>31.0</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>com</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.522907</td>\n",
              "      <td>0.055829</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>24.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>NaN</td>\n",
              "      <td>www.nyprowrestling.com</td>\n",
              "      <td>22.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 55 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                   URL  URLLength  \\\n",
              "0   1             https://www.northcm.ac.th       24.0   \n",
              "1   4                   http://uqr.to/1il1z        NaN   \n",
              "2   5  https://www.woolworthsrewards.com.au       35.0   \n",
              "3   6                                  <NA>       31.0   \n",
              "4  11                                  <NA>        NaN   \n",
              "\n",
              "                         Domain  DomainLength  IsDomainIP  TLD  \\\n",
              "0             www.northcm.ac.th          17.0       False  NaN   \n",
              "1                          <NA>           NaN        <NA>   to   \n",
              "2  www.woolworthsrewards.com.au          28.0       False   au   \n",
              "3                          <NA>           NaN        <NA>  com   \n",
              "4        www.nyprowrestling.com          22.0       False  NaN   \n",
              "\n",
              "   CharContinuationRate  TLDLegitimateProb  URLCharProb  ...    Pay  Crypto  \\\n",
              "0              0.800000                NaN          NaN  ...  False   False   \n",
              "1              1.000000           0.000896     0.036850  ...   <NA>   False   \n",
              "2              0.857143                NaN     0.060894  ...   True   False   \n",
              "3              0.562500           0.522907     0.055829  ...   True   False   \n",
              "4              1.000000                NaN          NaN  ...  False   False   \n",
              "\n",
              "   HasCopyrightInfo  NoOfImage  NoOfCSS  NoOfJS  NoOfSelfRef  NoOfEmptyRef  \\\n",
              "0              True        NaN      3.0     NaN         69.0           NaN   \n",
              "1             False        NaN      NaN     NaN          NaN           NaN   \n",
              "2              True       33.0      7.0     8.0         15.0           NaN   \n",
              "3              True       24.0      5.0    14.0          NaN           NaN   \n",
              "4              True        NaN      NaN    14.0          NaN           0.0   \n",
              "\n",
              "   NoOfExternalRef  label  \n",
              "0              NaN   True  \n",
              "1              1.0  False  \n",
              "2              2.0   True  \n",
              "3              NaN   True  \n",
              "4              NaN   True  \n",
              "\n",
              "[5 rows x 55 columns]"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply the function\n",
        "df_correct = correct_data_types(df)\n",
        "\n",
        "# Number of rows and columns\n",
        "print(f\"Dataset shape: {df_correct.shape}\")\n",
        "\n",
        "# Column names and their data types\n",
        "print(\"\\nColumn info:\")\n",
        "print(df_correct.info())\n",
        "\n",
        "# Class distribution (legitimate vs phishing)\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df_correct['label'].value_counts(normalize=True))\n",
        "\n",
        "# Global variables for numerical columns and categorical columns (reusability purposes)\n",
        "numerical_columns = df_correct.select_dtypes(include=['int64', 'float64']).columns \n",
        "categorical_columns = df_correct.select_dtypes(include=['object', 'boolean']).columns\n",
        "text_columns = df_correct.select_dtypes(include=['string']).columns\n",
        "\n",
        "# Drop ID from Numerical Columns\n",
        "numerical_columns = numerical_columns.drop('id')\n",
        "\n",
        "# Global variables for target columns (The Output Variable)\n",
        "target_column = 'label'\n",
        "\n",
        "df_correct.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvx-gT3bLfHM"
      },
      "source": [
        "# 1. Split Training Set and Validation Set\n",
        "\n",
        "Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split.\n",
        "\n",
        "Note: For training, you should use the data contained in the `train` folder given by the TA. The `test` data is only used for kaggle submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "4yWCUFFBLfHM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Split Summary\n",
            "--------------------------------------------------\n",
            "\n",
            "Total samples: 140404\n",
            "Training set size: 112323 (80.00%)\n",
            "Validation set size: 28081 (20.00%)\n",
            "\n",
            "Original class distribution:\n",
            "Legitimate: 92.48%\n",
            "Phishing: 7.52%\n",
            "\n",
            "Training set class distribution:\n",
            "Legitimate: 92.48%\n",
            "Phishing: 7.52%\n",
            "\n",
            "Validation set class distribution:\n",
            "Legitimate: 92.48%\n",
            "Phishing: 7.52%\n",
            "\n",
            "Feature completeness in training set:\n",
            "Domain                        49.964834\n",
            "NoOfExternalRef               49.454698\n",
            "LineOfCode                    49.250821\n",
            "LargestLineLength             48.408607\n",
            "HasSocialNet                  48.398814\n",
            "NoOfURLRedirect               48.008867\n",
            "HasCopyrightInfo              47.967914\n",
            "NoOfCSS                       47.760476\n",
            "NoOfObfuscatedChar            47.579748\n",
            "NoOfSelfRedirect              47.561052\n",
            "HasPasswordField              47.282391\n",
            "LetterRatioInURL              46.890664\n",
            "HasObfuscation                46.739314\n",
            "ObfuscationRatio              45.832999\n",
            "NoOfLettersInURL              45.112755\n",
            "SpacialCharRatioInURL         44.695209\n",
            "HasSubmitButton               43.834299\n",
            "NoOfEqualsInURL               43.815603\n",
            "NoOfJS                        43.325054\n",
            "URLLength                     43.195071\n",
            "NoOfDegitsInURL               41.860527\n",
            "HasFavicon                    41.607685\n",
            "Title                         41.571183\n",
            "HasExternalFormSubmit         39.588508\n",
            "Bank                          39.162950\n",
            "HasDescription                38.864703\n",
            "DegitRatioInURL               38.232597\n",
            "TLDLegitimateProb             37.655689\n",
            "URLTitleMatchScore            37.227460\n",
            "URLCharProb                   37.083233\n",
            "NoOfImage                     35.846621\n",
            "Crypto                        35.775398\n",
            "DomainTitleMatchScore         35.644525\n",
            "NoOfiFrame                    35.516323\n",
            "IsHTTPS                       35.208283\n",
            "NoOfSelfRef                   34.245880\n",
            "CharContinuationRate          34.149729\n",
            "TLDLength                     33.970781\n",
            "NoOfOtherSpecialCharsInURL    33.871068\n",
            "Robots                        33.389422\n",
            "DomainLength                  33.010158\n",
            "NoOfAmpersandInURL            32.335319\n",
            "TLD                           32.323745\n",
            "HasTitle                      31.660479\n",
            "NoOfQMarkInURL                31.433455\n",
            "NoOfSubDomain                 31.226908\n",
            "HasHiddenFields               31.089804\n",
            "NoOfPopup                     30.894830\n",
            "URL                           30.888598\n",
            "Pay                           30.690063\n",
            "NoOfEmptyRef                  30.404281\n",
            "IsResponsive                  30.263615\n",
            "IsDomainIP                    30.033920\n",
            "id                             0.000000\n",
            "label                          0.000000\n",
            "dtype: float64\n",
            "\n",
            "Feature completeness in validation set:\n",
            "Domain                        50.122859\n",
            "LineOfCode                    49.261066\n",
            "NoOfExternalRef               49.250383\n",
            "HasSocialNet                  48.559524\n",
            "LargestLineLength             48.267512\n",
            "NoOfCSS                       48.032477\n",
            "HasCopyrightInfo              47.954133\n",
            "NoOfURLRedirect               47.929205\n",
            "HasPasswordField              47.811688\n",
            "NoOfObfuscatedChar            47.558848\n",
            "NoOfSelfRedirect              47.338058\n",
            "HasObfuscation                47.081657\n",
            "ObfuscationRatio              46.711299\n",
            "LetterRatioInURL              46.568854\n",
            "NoOfLettersInURL              45.105231\n",
            "SpacialCharRatioInURL         44.980592\n",
            "HasSubmitButton               44.100994\n",
            "NoOfEqualsInURL               44.026210\n",
            "NoOfJS                        43.221395\n",
            "URLLength                     43.164417\n",
            "NoOfDegitsInURL               41.989245\n",
            "HasFavicon                    41.618888\n",
            "Title                         41.141697\n",
            "HasExternalFormSubmit         39.617535\n",
            "Bank                          39.197322\n",
            "HasDescription                39.118977\n",
            "TLDLegitimateProb             37.666038\n",
            "DegitRatioInURL               37.619743\n",
            "URLCharProb                   37.099818\n",
            "URLTitleMatchScore            37.039279\n",
            "NoOfImage                     36.351982\n",
            "NoOfiFrame                    35.792885\n",
            "Crypto                        35.657562\n",
            "DomainTitleMatchScore         35.468822\n",
            "IsHTTPS                       34.952459\n",
            "CharContinuationRate          34.485951\n",
            "NoOfSelfRef                   34.421851\n",
            "NoOfOtherSpecialCharsInURL    34.129839\n",
            "TLDLength                     34.094227\n",
            "DomainLength                  32.908372\n",
            "Robots                        32.862078\n",
            "TLD                           32.377764\n",
            "NoOfAmpersandInURL            32.288736\n",
            "HasTitle                      32.110680\n",
            "NoOfSubDomain                 31.996724\n",
            "HasHiddenFields               31.601439\n",
            "NoOfQMarkInURL                31.316549\n",
            "URL                           31.309426\n",
            "Pay                           30.988925\n",
            "NoOfPopup                     30.807307\n",
            "IsResponsive                  30.444073\n",
            "NoOfEmptyRef                  30.394217\n",
            "IsDomainIP                    29.895659\n",
            "id                             0.000000\n",
            "label                          0.000000\n",
            "dtype: float64\n",
            "\n",
            "Datasets saved to train_data.csv and validation_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Split dataset into training and validation set with basic (stratified) splitting method\n",
        "\n",
        "def create_train_val_split(df, target_col='label', test_size=0.2, random_state=42, verbose=True):\n",
        "    \"\"\"\n",
        "    Creates training and validation splits while preserving class distributions.\n",
        "    \n",
        "    Args:\n",
        "    df: DataFrame containing the complete dataset\n",
        "    target_col: Name of the target column (default: 'label')\n",
        "    test_size: Proportion of dataset to include in the validation split (default: 0.2)\n",
        "    random_state: Random state for reproducibility (default: 42)\n",
        "    verbose: Whether to print split statistics (default: True)\n",
        "    \n",
        "    Returns:\n",
        "    tuple: (train_data, validation_data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Separate features and target\n",
        "    X = df.drop(target_col, axis=1)\n",
        "    y = df[target_col]\n",
        "    \n",
        "    # Create stratified split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=y\n",
        "    )\n",
        "    \n",
        "    # Reconstruct complete dataframes with all features including target\n",
        "    train_data = X_train.copy()\n",
        "    train_data[target_col] = y_train\n",
        "    \n",
        "    validation_data = X_val.copy()\n",
        "    validation_data[target_col] = y_val\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Dataset Split Summary\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"\\nTotal samples: {len(df)}\")\n",
        "        print(f\"Training set size: {len(train_data)} ({len(train_data)/len(df)*100:.2f}%)\")\n",
        "        print(f\"Validation set size: {len(validation_data)} ({len(validation_data)/len(df)*100:.2f}%)\")\n",
        "        \n",
        "        # Class distribution in original dataset\n",
        "        print(\"\\nOriginal class distribution:\")\n",
        "        orig_dist = df[target_col].value_counts(normalize=True)\n",
        "        print(f\"Legitimate: {orig_dist[True]*100:.2f}%\")\n",
        "        print(f\"Phishing: {orig_dist[False]*100:.2f}%\")\n",
        "        \n",
        "        # Class distribution in training set\n",
        "        print(\"\\nTraining set class distribution:\")\n",
        "        train_dist = train_data[target_col].value_counts(normalize=True)\n",
        "        print(f\"Legitimate: {train_dist[True]*100:.2f}%\")\n",
        "        print(f\"Phishing: {train_dist[False]*100:.2f}%\")\n",
        "        \n",
        "        # Class distribution in validation set\n",
        "        print(\"\\nValidation set class distribution:\")\n",
        "        val_dist = validation_data[target_col].value_counts(normalize=True)\n",
        "        print(f\"Legitimate: {val_dist[True]*100:.2f}%\")\n",
        "        print(f\"Phishing: {val_dist[False]*100:.2f}%\")\n",
        "        \n",
        "        # Feature completeness check\n",
        "        print(\"\\nFeature completeness in training set:\")\n",
        "        train_nulls = train_data.isnull().sum() / len(train_data) * 100\n",
        "        print(train_nulls.sort_values(ascending=False))\n",
        "        \n",
        "        print(\"\\nFeature completeness in validation set:\")\n",
        "        val_nulls = validation_data.isnull().sum() / len(validation_data) * 100\n",
        "        print(val_nulls.sort_values(ascending=False))\n",
        "    \n",
        "    return train_data, validation_data\n",
        "\n",
        "train_data, validation_data = create_train_val_split(df_correct)\n",
        "\n",
        "def save_split_datasets(train_data, validation_data, train_path='train_data.csv', val_path='validation_data.csv'):\n",
        "    \"\"\"\n",
        "    Saves the training and validation sets to CSV files\n",
        "    \n",
        "    Args:\n",
        "    train_data: Training DataFrame\n",
        "    validation_data: Validation DataFrame\n",
        "    train_path: Path to save training data\n",
        "    val_path: Path to save validation data\n",
        "    \"\"\"\n",
        "    train_data.to_csv(train_path, index=False)\n",
        "    validation_data.to_csv(val_path, index=False)\n",
        "    print(f\"\\nDatasets saved to {train_path} and {val_path}\")\n",
        "\n",
        "save_split_datasets(train_data, validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC14lmo_LfHN"
      },
      "source": [
        "# 2. Data Cleaning and Preprocessing\n",
        "\n",
        "This step is the first thing to be done once a Data Scientist have grasped a general knowledge of the data. Raw data is **seldom ready for training**, therefore steps need to be taken to clean and format the data for the Machine Learning model to interpret.\n",
        "\n",
        "By performing data cleaning and preprocessing, you ensure that your dataset is ready for model training, leading to more accurate and reliable machine learning results. These steps are essential for transforming raw data into a format that machine learning algorithms can effectively learn from and make predictions.\n",
        "\n",
        "We will give some common methods for you to try, but you only have to **at least implement one method for each process**. For each step that you will do, **please explain the reason why did you do that process. Write it in a markdown cell under the code cell you wrote.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p95_A8hSLfHY"
      },
      "source": [
        "## A. Data Cleaning\n",
        "\n",
        "**Data cleaning** is the crucial first step in preparing your dataset for machine learning. Raw data collected from various sources is often messy and may contain errors, missing values, and inconsistencies. Data cleaning involves the following steps:\n",
        "\n",
        "1. **Handling Missing Data:** Identify and address missing values in the dataset. This can include imputing missing values, removing rows or columns with excessive missing data, or using more advanced techniques like interpolation.\n",
        "\n",
        "2. **Dealing with Outliers:** Identify and handle outliers, which are data points significantly different from the rest of the dataset. Outliers can be removed or transformed to improve model performance.\n",
        "\n",
        "3. **Data Validation:** Check for data integrity and consistency. Ensure that data types are correct, categorical variables have consistent labels, and numerical values fall within expected ranges.\n",
        "\n",
        "4. **Removing Duplicates:** Identify and remove duplicate rows, as they can skew the model's training process and evaluation metrics.\n",
        "\n",
        "5. **Feature Engineering**: Create new features or modify existing ones to extract relevant information. This step can involve scaling, normalizing, or encoding features for better model interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wruGao9LfHZ"
      },
      "source": [
        "### I. Handling Missing Data\n",
        "\n",
        "Missing data can adversely affect the performance and accuracy of machine learning models. There are several strategies to handle missing data in machine learning:\n",
        "\n",
        "1. **Data Imputation:**\n",
        "\n",
        "    a. **Mean, Median, or Mode Imputation:** For numerical features, you can replace missing values with the mean, median, or mode of the non-missing values in the same feature. This method is simple and often effective when data is missing at random.\n",
        "\n",
        "    b. **Constant Value Imputation:** You can replace missing values with a predefined constant value (e.g., 0) if it makes sense for your dataset and problem.\n",
        "\n",
        "    c. **Imputation Using Predictive Models:** More advanced techniques involve using predictive models to estimate missing values. For example, you can train a regression model to predict missing numerical values or a classification model to predict missing categorical values.\n",
        "\n",
        "2. **Deletion of Missing Data:**\n",
        "\n",
        "    a. **Listwise Deletion:** In cases where the amount of missing data is relatively small, you can simply remove rows with missing values from your dataset. However, this approach can lead to a loss of valuable information.\n",
        "\n",
        "    b. **Column (Feature) Deletion:** If a feature has a large number of missing values and is not critical for your analysis, you can consider removing that feature altogether.\n",
        "\n",
        "3. **Domain-Specific Strategies:**\n",
        "\n",
        "    a. **Domain Knowledge:** In some cases, domain knowledge can guide the imputation process. For example, if you know that missing values are related to a specific condition, you can impute them accordingly.\n",
        "\n",
        "4. **Imputation Libraries:**\n",
        "\n",
        "    a. **Scikit-Learn:** Scikit-Learn provides a `SimpleImputer` class that can handle basic imputation strategies like mean, median, and mode imputation.\n",
        "\n",
        "    b. **Fancyimpute:** Fancyimpute is a Python library that offers more advanced imputation techniques, including matrix factorization, k-nearest neighbors, and deep learning-based methods.\n",
        "\n",
        "The choice of imputation method should be guided by the nature of your data, the amount of missing data, the problem you are trying to solve, and the assumptions you are willing to make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Split Summary\n",
            "--------------------------------------------------\n",
            "\n",
            "Original dataset size: 112,323 rows\n",
            "Complete data size: 77,628 rows (69.11%)\n",
            "Missing URL data size: 34,695 rows (30.89%)\n",
            "\n",
            "Class distribution in complete data:\n",
            "Legitimate: 92.53%\n",
            "Phishing: 7.47%\n",
            "\n",
            "Class distribution in missing URL data:\n",
            "Legitimate: 92.38%\n",
            "Phishing: 7.62%\n",
            "\n",
            "Feature completeness in complete data:\n",
            "Domain                        49.953625\n",
            "NoOfExternalRef               49.466687\n",
            "LineOfCode                    49.248982\n",
            "HasSocialNet                  48.389756\n",
            "LargestLineLength             48.361416\n",
            "HasCopyrightInfo              47.866749\n",
            "NoOfURLRedirect               47.844850\n",
            "NoOfCSS                       47.651621\n",
            "NoOfObfuscatedChar            47.641315\n",
            "NoOfSelfRedirect              47.565311\n",
            "HasPasswordField              47.265162\n",
            "HasObfuscation                46.850363\n",
            "LetterRatioInURL              46.792394\n",
            "ObfuscationRatio              45.685835\n",
            "NoOfLettersInURL              45.121606\n",
            "SpacialCharRatioInURL         44.829185\n",
            "HasSubmitButton               43.899109\n",
            "NoOfEqualsInURL               43.873345\n",
            "URLLength                     43.212501\n",
            "NoOfJS                        43.144226\n",
            "NoOfDegitsInURL               41.792910\n",
            "HasFavicon                    41.733653\n",
            "Title                         41.554594\n",
            "HasExternalFormSubmit         39.639048\n",
            "Bank                          39.079971\n",
            "HasDescription                38.839079\n",
            "DegitRatioInURL               38.205287\n",
            "TLDLegitimateProb             37.446540\n",
            "URLTitleMatchScore            37.176019\n",
            "URLCharProb                   37.065234\n",
            "Crypto                        35.872366\n",
            "NoOfImage                     35.825991\n",
            "DomainTitleMatchScore         35.500077\n",
            "NoOfiFrame                    35.390581\n",
            "IsHTTPS                       35.248879\n",
            "CharContinuationRate          34.218323\n",
            "NoOfSelfRef                   34.211882\n",
            "TLDLength                     33.940073\n",
            "NoOfOtherSpecialCharsInURL    33.848611\n",
            "Robots                        33.666976\n",
            "DomainLength                  33.040913\n",
            "TLD                           32.353017\n",
            "NoOfAmpersandInURL            32.212604\n",
            "HasTitle                      31.783635\n",
            "NoOfQMarkInURL                31.461586\n",
            "NoOfSubDomain                 31.243881\n",
            "HasHiddenFields               31.078992\n",
            "Pay                           30.829082\n",
            "NoOfPopup                     30.655176\n",
            "NoOfEmptyRef                  30.422013\n",
            "IsResponsive                  30.258412\n",
            "IsDomainIP                    30.060030\n",
            "id                             0.000000\n",
            "URL                            0.000000\n",
            "label                          0.000000\n",
            "dtype: float64\n",
            "\n",
            "Feature completeness in missing URL data:\n",
            "URL                           100.000000\n",
            "Domain                         49.989912\n",
            "NoOfExternalRef                49.427871\n",
            "LineOfCode                     49.254936\n",
            "LargestLineLength              48.514195\n",
            "HasSocialNet                   48.419081\n",
            "NoOfURLRedirect                48.375847\n",
            "HasCopyrightInfo               48.194264\n",
            "NoOfCSS                        48.004035\n",
            "NoOfSelfRedirect               47.551520\n",
            "NoOfObfuscatedChar             47.441995\n",
            "HasPasswordField               47.320940\n",
            "LetterRatioInURL               47.110535\n",
            "HasObfuscation                 46.490849\n",
            "ObfuscationRatio               46.162271\n",
            "NoOfLettersInURL               45.092953\n",
            "SpacialCharRatioInURL          44.395446\n",
            "NoOfJS                         43.729644\n",
            "HasSubmitButton                43.689292\n",
            "NoOfEqualsInURL                43.686410\n",
            "URLLength                      43.156074\n",
            "NoOfDegitsInURL                42.011817\n",
            "Title                          41.608301\n",
            "HasFavicon                     41.325839\n",
            "HasExternalFormSubmit          39.475429\n",
            "Bank                           39.348609\n",
            "HasDescription                 38.922035\n",
            "DegitRatioInURL                38.293702\n",
            "TLDLegitimateProb              38.123649\n",
            "URLTitleMatchScore             37.342557\n",
            "URLCharProb                    37.123505\n",
            "DomainTitleMatchScore          35.967719\n",
            "NoOfImage                      35.892780\n",
            "NoOfiFrame                     35.797665\n",
            "Crypto                         35.558438\n",
            "IsHTTPS                        35.117452\n",
            "NoOfSelfRef                    34.321948\n",
            "TLDLength                      34.039487\n",
            "CharContinuationRate           33.996253\n",
            "NoOfOtherSpecialCharsInURL     33.921314\n",
            "DomainLength                   32.941346\n",
            "Robots                         32.768410\n",
            "NoOfAmpersandInURL             32.609886\n",
            "TLD                            32.258250\n",
            "NoOfPopup                      31.431042\n",
            "HasTitle                       31.384926\n",
            "NoOfQMarkInURL                 31.370514\n",
            "NoOfSubDomain                  31.188932\n",
            "HasHiddenFields                31.113993\n",
            "Pay                            30.379017\n",
            "NoOfEmptyRef                   30.364606\n",
            "IsResponsive                   30.275256\n",
            "IsDomainIP                     29.975501\n",
            "id                              0.000000\n",
            "label                           0.000000\n",
            "dtype: float64\n",
            "\n",
            "Datasets saved to complete_url_data.csv and missing_url_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Split dataset into two: dataset with available URL data and dataset without URL data\n",
        "\n",
        "def split_dataset_by_url(df, verbose=True):\n",
        "    \"\"\"\n",
        "    Splits the dataset into two parts based on URL availability\n",
        "    \n",
        "    Args:\n",
        "    df: Original DataFrame\n",
        "    verbose: Whether to print summary statistics\n",
        "    \n",
        "    Returns:\n",
        "    tuple: (complete_url_data, missing_url_data)\n",
        "    \"\"\"\n",
        "    # Create the splits\n",
        "    complete_url_data = df[df['URL'].notna()].copy()\n",
        "    missing_url_data = df[df['URL'].isna()].copy()\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Dataset Split Summary\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Overall statistics\n",
        "        print(f\"\\nOriginal dataset size: {len(df):,} rows\")\n",
        "        print(f\"Complete data size: {len(complete_url_data):,} rows ({len(complete_url_data)/len(df)*100:.2f}%)\")\n",
        "        print(f\"Missing URL data size: {len(missing_url_data):,} rows ({len(missing_url_data)/len(df)*100:.2f}%)\")\n",
        "        \n",
        "        # Class distribution in complete data\n",
        "        complete_dist = complete_url_data['label'].value_counts(normalize=True)\n",
        "        print(\"\\nClass distribution in complete data:\")\n",
        "        print(f\"Legitimate: {complete_dist[True]*100:.2f}%\")\n",
        "        print(f\"Phishing: {complete_dist[False]*100:.2f}%\")\n",
        "        \n",
        "        # Class distribution in missing URL data\n",
        "        missing_dist = missing_url_data['label'].value_counts(normalize=True)\n",
        "        print(\"\\nClass distribution in missing URL data:\")\n",
        "        print(f\"Legitimate: {missing_dist[True]*100:.2f}%\")\n",
        "        print(f\"Phishing: {missing_dist[False]*100:.2f}%\")\n",
        "        \n",
        "        # Feature completeness in both datasets\n",
        "        print(\"\\nFeature completeness in complete data:\")\n",
        "        complete_nulls = complete_url_data.isnull().sum() / len(complete_url_data) * 100\n",
        "        print(complete_nulls.sort_values(ascending=False))\n",
        "        \n",
        "        print(\"\\nFeature completeness in missing URL data:\")\n",
        "        missing_nulls = missing_url_data.isnull().sum() / len(missing_url_data) * 100\n",
        "        print(missing_nulls.sort_values(ascending=False))\n",
        "    \n",
        "    return complete_url_data, missing_url_data\n",
        "\n",
        "complete_url_data, missing_url_data = split_dataset_by_url(train_data)\n",
        "\n",
        "def save_split_datasets(complete_url_data, missing_url_data, complete_path='complete_url_data.csv', missing_path='missing_url_data.csv'):\n",
        "    \"\"\"\n",
        "    Saves the split datasets to CSV files\n",
        "    \n",
        "    Args:\n",
        "    complete_url_data: DataFrame with complete URL data\n",
        "    missing_url_data: DataFrame with missing URL data\n",
        "    complete_path: Path to save complete data\n",
        "    missing_path: Path to save missing URL data\n",
        "    \"\"\"\n",
        "    complete_url_data.to_csv(complete_path, index=False)\n",
        "    missing_url_data.to_csv(missing_path, index=False)\n",
        "    print(f\"\\nDatasets saved to {complete_path} and {missing_path}\")\n",
        "\n",
        "save_split_datasets(complete_url_data, missing_url_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features in dataset by deriving the data from available URL data\n",
        "\n",
        "class URLFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        # Regex for IP address detection\n",
        "        self.ip_pattern = r'^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$'\n",
        "        \n",
        "    def extract_features(self, url):\n",
        "        \"\"\"Extract all features from a single URL\"\"\"\n",
        "        if pd.isna(url):\n",
        "            return {}\n",
        "            \n",
        "        try:\n",
        "            # Parse URL\n",
        "            parsed = urlparse(url)\n",
        "            domain = parsed.netloc\n",
        "            \n",
        "            # Basic features\n",
        "            features = {\n",
        "                'URLLength': len(url),\n",
        "                'Domain': domain,\n",
        "                'DomainLength': len(domain),\n",
        "                'TLD': self._get_tld(domain),\n",
        "                'TLDLength': len(self._get_tld(domain)),\n",
        "                'NoOfSubDomain': self._count_subdomains(domain),\n",
        "                'IsHTTPS': url.startswith('https://'),\n",
        "                'IsDomainIP': self._is_ip_address(domain)\n",
        "            }\n",
        "            \n",
        "            # Character counts and ratios\n",
        "            char_features = self._analyze_characters(url)\n",
        "            features.update(char_features)\n",
        "            \n",
        "            # Calculate character continuation rate\n",
        "            features['CharContinuationRate'] = self._calc_char_continuation(url)\n",
        "            \n",
        "            return features\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing URL {url}: {str(e)}\")\n",
        "            return {}\n",
        "    \n",
        "    def _get_tld(self, domain):\n",
        "        \"\"\"Extract TLD from domain\"\"\"\n",
        "        if not domain:\n",
        "            return ''\n",
        "        parts = domain.split('.')\n",
        "        return parts[-1] if len(parts) > 0 else ''\n",
        "    \n",
        "    def _count_subdomains(self, domain):\n",
        "        \"\"\"Count number of subdomains\"\"\"\n",
        "        if not domain:\n",
        "            return 0\n",
        "        # Subtract 1 from dot count (as dots = subdomains + 1)\n",
        "        return domain.count('.') if domain.count('.') > 0 else 0\n",
        "    \n",
        "    def _is_ip_address(self, domain):\n",
        "        \"\"\"Check if domain is an IP address\"\"\"\n",
        "        try:\n",
        "            # Try to create IP address object\n",
        "            ipaddress.ip_address(domain)\n",
        "            return True\n",
        "        except:\n",
        "            # Check against regex pattern\n",
        "            return bool(re.match(self.ip_pattern, domain))\n",
        "    \n",
        "    def _analyze_characters(self, url):\n",
        "        \"\"\"Analyze character types and their ratios in URL\"\"\"\n",
        "        total_len = len(url)\n",
        "        if total_len == 0:\n",
        "            return {}\n",
        "            \n",
        "        letters = sum(c.isalpha() for c in url)\n",
        "        digits = sum(c.isdigit() for c in url)\n",
        "        equals = url.count('=')\n",
        "        qmarks = url.count('?')\n",
        "        ampersands = url.count('&')\n",
        "        \n",
        "        # Count other special characters\n",
        "        special_chars = sum(not c.isalnum() for c in url) - equals - qmarks - ampersands\n",
        "        \n",
        "        return {\n",
        "            'NoOfLettersInURL': letters,\n",
        "            'LetterRatioInURL': letters / total_len,\n",
        "            'NoOfDegitsInURL': digits,\n",
        "            'DegitRatioInURL': digits / total_len,\n",
        "            'NoOfEqualsInURL': equals,\n",
        "            'NoOfQMarkInURL': qmarks,\n",
        "            'NoOfAmpersandInURL': ampersands,\n",
        "            'NoOfOtherSpecialCharsInURL': special_chars,\n",
        "            'SpacialCharRatioInURL': (special_chars + equals + qmarks + ampersands) / total_len\n",
        "        }\n",
        "    \n",
        "    def _calc_char_continuation(self, url):\n",
        "        \"\"\"Calculate character continuation rate\"\"\"\n",
        "        if not url:\n",
        "            return 0\n",
        "            \n",
        "        # Find longest sequences of same type (letter/digit/special)\n",
        "        def get_type(c):\n",
        "            if c.isalpha(): return 'alpha'\n",
        "            if c.isdigit(): return 'digit'\n",
        "            return 'special'\n",
        "            \n",
        "        current_type = get_type(url[0])\n",
        "        current_length = 1\n",
        "        max_lengths = {'alpha': 1, 'digit': 1, 'special': 1}\n",
        "        \n",
        "        for c in url[1:]:\n",
        "            c_type = get_type(c)\n",
        "            if c_type == current_type:\n",
        "                current_length += 1\n",
        "                max_lengths[c_type] = max(max_lengths[c_type], current_length)\n",
        "            else:\n",
        "                current_type = c_type\n",
        "                current_length = 1\n",
        "        \n",
        "        # Sum of longest sequences divided by URL length\n",
        "        total_continuation = sum(max_lengths.values())\n",
        "        return total_continuation / len(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate features in dataset by partially deriving the data from available URL data \n",
        "\n",
        "class URLProbabilityFeatures:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize probability calculators\"\"\"\n",
        "        self.char_probabilities = {}  # Character frequency dictionary\n",
        "        self.tld_probabilities = {}   # TLD frequency dictionary\n",
        "        self.min_probability = 1e-5   # Minimum probability for unknown features\n",
        "        \n",
        "    def fit(self, urls, labels):\n",
        "        \"\"\"\n",
        "        Calculate probabilities using legitimate URLs from training data\n",
        "        \n",
        "        Args:\n",
        "        urls: Series of URLs\n",
        "        labels: Series of labels (True for legitimate, False for phishing)\n",
        "        \"\"\"\n",
        "        # Filter legitimate URLs only\n",
        "        legitimate_urls = urls[labels == True].dropna()\n",
        "        \n",
        "        # Calculate character probabilities\n",
        "        self._calculate_char_probabilities(legitimate_urls)\n",
        "        \n",
        "        # Calculate TLD probabilities\n",
        "        self._calculate_tld_probabilities(legitimate_urls)\n",
        "        \n",
        "    def _calculate_char_probabilities(self, legitimate_urls):\n",
        "        \"\"\"Calculate character frequencies from legitimate URLs\"\"\"\n",
        "        # Combine all characters from legitimate URLs\n",
        "        all_chars = ''.join(legitimate_urls.astype(str).str.lower())\n",
        "        total_chars = len(all_chars)\n",
        "        \n",
        "        # Count each character\n",
        "        char_counts = {}\n",
        "        for char in all_chars:\n",
        "            if char.isalnum():  # Only consider alphanumeric characters\n",
        "                char_counts[char] = char_counts.get(char, 0) + 1\n",
        "                \n",
        "        # Convert to probabilities\n",
        "        self.char_probabilities = {\n",
        "            char: count/total_chars \n",
        "            for char, count in char_counts.items()\n",
        "        }\n",
        "        \n",
        "        # Print probabilities for verification\n",
        "        print(\"Character Probabilities:\")\n",
        "        sorted_probs = dict(sorted(self.char_probabilities.items(), \n",
        "                                 key=lambda x: x[1], reverse=True))\n",
        "        for char, prob in sorted_probs.items():\n",
        "            print(f\"'{char}': {prob:.4f}\")\n",
        "            \n",
        "    def _calculate_tld_probabilities(self, legitimate_urls):\n",
        "        \"\"\"Calculate TLD frequencies from legitimate URLs\"\"\"\n",
        "        # Extract and count TLDs\n",
        "        tld_counts = {}\n",
        "        total_tlds = 0\n",
        "        \n",
        "        for url in legitimate_urls:\n",
        "            try:\n",
        "                tld = urlparse(url).netloc.split('.')[-1].lower()\n",
        "                tld_counts[tld] = tld_counts.get(tld, 0) + 1\n",
        "                total_tlds += 1\n",
        "            except:\n",
        "                continue\n",
        "                \n",
        "        # Convert to probabilities\n",
        "        self.tld_probabilities = {\n",
        "            tld: count/total_tlds \n",
        "            for tld, count in tld_counts.items()\n",
        "        }\n",
        "        \n",
        "        # Print probabilities for verification\n",
        "        print(\"\\nTLD Probabilities:\")\n",
        "        sorted_probs = dict(sorted(self.tld_probabilities.items(), \n",
        "                                 key=lambda x: x[1], reverse=True))\n",
        "        for tld, prob in sorted_probs.items():\n",
        "            print(f\"'.{tld}': {prob:.4f}\")\n",
        "            \n",
        "    def calculate_url_char_prob(self, url):\n",
        "        \"\"\"\n",
        "        Calculate geometric mean of character probabilities\n",
        "        \n",
        "        Args:\n",
        "        url: URL string\n",
        "        \n",
        "        Returns:\n",
        "        float: URL character probability score\n",
        "        \"\"\"\n",
        "        if pd.isna(url):\n",
        "            return np.nan\n",
        "            \n",
        "        try:\n",
        "            # Get probabilities for each character\n",
        "            char_probs = []\n",
        "            for char in str(url).lower():\n",
        "                if char.isalnum():\n",
        "                    prob = self.char_probabilities.get(char, self.min_probability)\n",
        "                    char_probs.append(prob)\n",
        "                    \n",
        "            if not char_probs:\n",
        "                return 0.0\n",
        "                \n",
        "            # Calculate geometric mean\n",
        "            n = len(char_probs)\n",
        "            log_sum = sum(math.log(p) for p in char_probs)\n",
        "            return math.exp(log_sum / n)\n",
        "            \n",
        "        except:\n",
        "            return np.nan\n",
        "            \n",
        "    def calculate_tld_prob(self, url):\n",
        "        \"\"\"\n",
        "        Get TLD probability or minimum value for unknown TLDs\n",
        "        \n",
        "        Args:\n",
        "        url: URL string\n",
        "        \n",
        "        Returns:\n",
        "        float: TLD legitimacy probability score\n",
        "        \"\"\"\n",
        "        if pd.isna(url):\n",
        "            return np.nan\n",
        "            \n",
        "        try:\n",
        "            # Extract TLD\n",
        "            tld = urlparse(url).netloc.split('.')[-1].lower()\n",
        "            \n",
        "            # Return probability or minimum value\n",
        "            return self.tld_probabilities.get(tld, self.min_probability)\n",
        "            \n",
        "        except:\n",
        "            return np.nan\n",
        "            \n",
        "    def transform(self, urls):\n",
        "        \"\"\"\n",
        "        Calculate both probability features for a series of URLs\n",
        "        \n",
        "        Args:\n",
        "        urls: Series of URLs\n",
        "        \n",
        "        Returns:\n",
        "        DataFrame with URLCharProb and TLDLegitimateProb columns\n",
        "        \"\"\"\n",
        "        return pd.DataFrame({\n",
        "            'URLCharProb': urls.apply(self.calculate_url_char_prob),\n",
        "            'TLDLegitimateProb': urls.apply(self.calculate_tld_prob)\n",
        "        })\n",
        "\n",
        "class URLObfuscationDetector:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize detector with patterns\"\"\"\n",
        "        # URL encoding patterns\n",
        "        self.url_encoding_pattern = r'%[0-9A-Fa-f]{2}'\n",
        "        self.double_encoding_pattern = r'%25[0-9A-Fa-f]{2}'\n",
        "        \n",
        "        # Unicode/punycode patterns\n",
        "        self.punycode_pattern = r'xn--'\n",
        "        \n",
        "        # Common character substitutions\n",
        "        self.substitutions = {\n",
        "            '0': 'o',\n",
        "            '1': 'l',\n",
        "            '3': 'e',\n",
        "            '4': 'a',\n",
        "            '5': 's',\n",
        "            '6': 'b',\n",
        "            '7': 't',\n",
        "            '8': 'b',\n",
        "            '9': 'g'\n",
        "        }\n",
        "        \n",
        "        # Similar-looking Unicode characters\n",
        "        self.unicode_substitutions = {\n",
        "            'а': 'a',  # Cyrillic 'a'\n",
        "            'е': 'e',  # Cyrillic 'e'\n",
        "            'о': 'o',  # Cyrillic 'o'\n",
        "            'р': 'p',  # Cyrillic 'p'\n",
        "            'с': 'c',  # Cyrillic 'c'\n",
        "            'ѕ': 's',  # Cyrillic 's'\n",
        "            'і': 'i',  # Cyrillic 'i'\n",
        "        }\n",
        "        \n",
        "    def detect_obfuscation(self, url):\n",
        "        \"\"\"\n",
        "        Detect various types of URL obfuscation\n",
        "        \n",
        "        Args:\n",
        "        url: URL string\n",
        "        \n",
        "        Returns:\n",
        "        dict: Detection results and counts\n",
        "        \"\"\"\n",
        "        if pd.isna(url):\n",
        "            return {\n",
        "                'HasObfuscation': False,\n",
        "                'NoOfObfuscatedChar': 0,\n",
        "                'ObfuscationRatio': 0.0,\n",
        "                'techniques': []\n",
        "            }\n",
        "            \n",
        "        url = str(url)\n",
        "        techniques = []\n",
        "        obfuscated_chars = 0\n",
        "        \n",
        "        # Check URL encoding\n",
        "        url_encoded_chars = self._detect_url_encoding(url)\n",
        "        if url_encoded_chars > 0:\n",
        "            techniques.append('url_encoding')\n",
        "            obfuscated_chars += url_encoded_chars\n",
        "            \n",
        "        # Check double encoding\n",
        "        double_encoded_chars = self._detect_double_encoding(url)\n",
        "        if double_encoded_chars > 0:\n",
        "            techniques.append('double_encoding')\n",
        "            obfuscated_chars += double_encoded_chars\n",
        "            \n",
        "        # Check punycode\n",
        "        if self._detect_punycode(url):\n",
        "            techniques.append('punycode')\n",
        "            # Count entire domain as obfuscated if punycode is used\n",
        "            domain = urlparse(url).netloc\n",
        "            obfuscated_chars += len(domain)\n",
        "            \n",
        "        # Check character substitutions\n",
        "        substituted_chars = self._detect_char_substitution(url)\n",
        "        if substituted_chars > 0:\n",
        "            techniques.append('char_substitution')\n",
        "            obfuscated_chars += substituted_chars\n",
        "            \n",
        "        # Check Unicode substitutions\n",
        "        unicode_chars = self._detect_unicode_substitution(url)\n",
        "        if unicode_chars > 0:\n",
        "            techniques.append('unicode_substitution')\n",
        "            obfuscated_chars += unicode_chars\n",
        "            \n",
        "        # Calculate ratio\n",
        "        total_chars = len(url)\n",
        "        obfuscation_ratio = obfuscated_chars / total_chars if total_chars > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            'HasObfuscation': len(techniques) > 0,\n",
        "            'NoOfObfuscatedChar': obfuscated_chars,\n",
        "            'ObfuscationRatio': obfuscation_ratio,\n",
        "            'techniques': techniques\n",
        "        }\n",
        "        \n",
        "    def _detect_url_encoding(self, url):\n",
        "        \"\"\"Detect standard URL encoding\"\"\"\n",
        "        matches = re.findall(self.url_encoding_pattern, url)\n",
        "        return len(matches)\n",
        "        \n",
        "    def _detect_double_encoding(self, url):\n",
        "        \"\"\"Detect double URL encoding\"\"\"\n",
        "        matches = re.findall(self.double_encoding_pattern, url)\n",
        "        return len(matches)\n",
        "        \n",
        "    def _detect_punycode(self, url):\n",
        "        \"\"\"Detect punycode encoding in domain\"\"\"\n",
        "        return self.punycode_pattern in url.lower()\n",
        "        \n",
        "    def _detect_char_substitution(self, url):\n",
        "        \"\"\"Detect numeric/character substitutions\"\"\"\n",
        "        count = 0\n",
        "        domain = urlparse(url).netloc.lower()\n",
        "        \n",
        "        for digit, letter in self.substitutions.items():\n",
        "            # Check if digit appears in domain portion\n",
        "            digit_count = domain.count(digit)\n",
        "            if digit_count > 0:\n",
        "                # Verify it's likely a substitution by checking surrounding chars\n",
        "                for idx in range(len(domain) - 1):\n",
        "                    if domain[idx] == digit:\n",
        "                        # Check if surrounded by letters\n",
        "                        if (idx > 0 and domain[idx-1].isalpha()) or \\\n",
        "                           (idx < len(domain)-1 and domain[idx+1].isalpha()):\n",
        "                            count += 1\n",
        "                            \n",
        "        return count\n",
        "        \n",
        "    def _detect_unicode_substitution(self, url):\n",
        "        \"\"\"Detect Unicode character substitutions\"\"\"\n",
        "        count = 0\n",
        "        for char in url:\n",
        "            # Check if character has a similar-looking ASCII version\n",
        "            if char in self.unicode_substitutions:\n",
        "                count += 1\n",
        "        return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Character Probabilities:\n",
            "'w': 0.1157\n",
            "'t': 0.1016\n",
            "'s': 0.0649\n",
            "'o': 0.0568\n",
            "'h': 0.0486\n",
            "'p': 0.0483\n",
            "'e': 0.0448\n",
            "'c': 0.0404\n",
            "'a': 0.0402\n",
            "'r': 0.0349\n",
            "'m': 0.0333\n",
            "'i': 0.0321\n",
            "'n': 0.0291\n",
            "'l': 0.0219\n",
            "'u': 0.0167\n",
            "'g': 0.0161\n",
            "'d': 0.0149\n",
            "'b': 0.0092\n",
            "'k': 0.0082\n",
            "'f': 0.0073\n",
            "'y': 0.0073\n",
            "'v': 0.0061\n",
            "'j': 0.0026\n",
            "'z': 0.0023\n",
            "'x': 0.0016\n",
            "'q': 0.0007\n",
            "'2': 0.0004\n",
            "'1': 0.0004\n",
            "'0': 0.0003\n",
            "'4': 0.0002\n",
            "'3': 0.0002\n",
            "'5': 0.0001\n",
            "'6': 0.0001\n",
            "'9': 0.0001\n",
            "'7': 0.0001\n",
            "'8': 0.0001\n",
            "\n",
            "TLD Probabilities:\n",
            "'.com': 0.5102\n",
            "'.org': 0.1240\n",
            "'.uk': 0.0451\n",
            "'.net': 0.0290\n",
            "'.de': 0.0243\n",
            "'.au': 0.0192\n",
            "'.jp': 0.0148\n",
            "'.edu': 0.0141\n",
            "'.it': 0.0124\n",
            "'.ca': 0.0112\n",
            "'.nl': 0.0112\n",
            "'.fr': 0.0099\n",
            "'.gov': 0.0094\n",
            "'.br': 0.0087\n",
            "'.ru': 0.0066\n",
            "'.in': 0.0061\n",
            "'.us': 0.0052\n",
            "'.eu': 0.0051\n",
            "'.info': 0.0050\n",
            "'.be': 0.0045\n",
            "'.se': 0.0042\n",
            "'.cz': 0.0041\n",
            "'.es': 0.0041\n",
            "'.pl': 0.0040\n",
            "'.nz': 0.0040\n",
            "'.co': 0.0037\n",
            "'.ch': 0.0036\n",
            "'.ie': 0.0032\n",
            "'.at': 0.0032\n",
            "'.gr': 0.0032\n",
            "'.io': 0.0031\n",
            "'.no': 0.0030\n",
            "'.ro': 0.0029\n",
            "'.za': 0.0025\n",
            "'.fi': 0.0024\n",
            "'.mil': 0.0023\n",
            "'.tr': 0.0022\n",
            "'.dk': 0.0021\n",
            "'.cn': 0.0021\n",
            "'.mx': 0.0020\n",
            "'.il': 0.0019\n",
            "'.sg': 0.0018\n",
            "'.id': 0.0018\n",
            "'.tv': 0.0017\n",
            "'.kr': 0.0017\n",
            "'.ar': 0.0017\n",
            "'.pt': 0.0016\n",
            "'.hu': 0.0015\n",
            "'.tw': 0.0014\n",
            "'.my': 0.0014\n",
            "'.me': 0.0013\n",
            "'.sk': 0.0013\n",
            "'.si': 0.0012\n",
            "'.hr': 0.0012\n",
            "'.cl': 0.0012\n",
            "'.lt': 0.0011\n",
            "'.app': 0.0011\n",
            "'.bg': 0.0011\n",
            "'.ua': 0.0011\n",
            "'.ph': 0.0010\n",
            "'.rs': 0.0010\n",
            "'.vn': 0.0010\n",
            "'.pk': 0.0009\n",
            "'.hk': 0.0009\n",
            "'.biz': 0.0008\n",
            "'.ae': 0.0008\n",
            "'.ee': 0.0008\n",
            "'.cat': 0.0007\n",
            "'.is': 0.0007\n",
            "'.am': 0.0006\n",
            "'.th': 0.0006\n",
            "'.fm': 0.0005\n",
            "'.lv': 0.0005\n",
            "'.ng': 0.0005\n",
            "'.ir': 0.0005\n",
            "'.ke': 0.0005\n",
            "'.ai': 0.0004\n",
            "'.lk': 0.0004\n",
            "'.lu': 0.0004\n",
            "'.az': 0.0004\n",
            "'.cc': 0.0004\n",
            "'.pe': 0.0004\n",
            "'.asia': 0.0004\n",
            "'.kz': 0.0004\n",
            "'.by': 0.0004\n",
            "'.ba': 0.0003\n",
            "'.travel': 0.0003\n",
            "'.tz': 0.0003\n",
            "'.mobi': 0.0003\n",
            "'.int': 0.0003\n",
            "'.bd': 0.0003\n",
            "'.uy': 0.0003\n",
            "'.ge': 0.0003\n",
            "'.np': 0.0003\n",
            "'.nu': 0.0003\n",
            "'.ug': 0.0003\n",
            "'.ec': 0.0002\n",
            "'.news': 0.0002\n",
            "'.coop': 0.0002\n",
            "'.wales': 0.0002\n",
            "'.aero': 0.0002\n",
            "'.dev': 0.0002\n",
            "'.xyz': 0.0002\n",
            "'.club': 0.0002\n",
            "'.to': 0.0002\n",
            "'.scot': 0.0002\n",
            "'.ws': 0.0002\n",
            "'.online': 0.0002\n",
            "'.al': 0.0002\n",
            "'.cy': 0.0002\n",
            "'.mk': 0.0002\n",
            "'.md': 0.0002\n",
            "'.sa': 0.0002\n",
            "'.zw': 0.0002\n",
            "'.gg': 0.0002\n",
            "'.ps': 0.0002\n",
            "'.ma': 0.0002\n",
            "'.mt': 0.0001\n",
            "'.pro': 0.0001\n",
            "'.nyc': 0.0001\n",
            "'.cu': 0.0001\n",
            "'.eus': 0.0001\n",
            "'.life': 0.0001\n",
            "'.live': 0.0001\n",
            "'.cr': 0.0001\n",
            "'.na': 0.0001\n",
            "'.jo': 0.0001\n",
            "'.py': 0.0001\n",
            "'.tech': 0.0001\n",
            "'.art': 0.0001\n",
            "'.name': 0.0001\n",
            "'.kh': 0.0001\n",
            "'.im': 0.0001\n",
            "'.ve': 0.0001\n",
            "'.li': 0.0001\n",
            "'.bm': 0.0001\n",
            "'.sh': 0.0001\n",
            "'.london': 0.0001\n",
            "'.shop': 0.0001\n",
            "'.lb': 0.0001\n",
            "'.bo': 0.0001\n",
            "'.bank': 0.0001\n",
            "'.one': 0.0001\n",
            "'.do': 0.0001\n",
            "'.eg': 0.0001\n",
            "'.rw': 0.0001\n",
            "'.network': 0.0001\n",
            "'.mu': 0.0001\n",
            "'.om': 0.0001\n",
            "'.world': 0.0001\n",
            "'.global': 0.0001\n",
            "'.blog': 0.0001\n",
            "'.fj': 0.0001\n",
            "'.design': 0.0001\n",
            "'.bh': 0.0001\n",
            "'.jm': 0.0001\n",
            "'.vc': 0.0001\n",
            "'.ad': 0.0001\n",
            "'.bt': 0.0001\n",
            "'.su': 0.0001\n",
            "'.gt': 0.0001\n",
            "'.sv': 0.0001\n",
            "'.sn': 0.0001\n",
            "'.hn': 0.0001\n",
            "'.ni': 0.0001\n",
            "'.store': 0.0001\n",
            "'.mv': 0.0001\n",
            "'.tn': 0.0001\n",
            "'.mn': 0.0001\n",
            "'.uz': 0.0001\n",
            "'.gi': 0.0001\n",
            "'.pg': 0.0001\n",
            "'.pa': 0.0001\n",
            "'.agency': 0.0001\n",
            "'.gy': 0.0001\n",
            "'.re': 0.0001\n",
            "'.qa': 0.0001\n",
            "'.google': 0.0001\n",
            "'.ky': 0.0001\n",
            "'.fo': 0.0001\n",
            "'.space': 0.0001\n",
            "'.zm': 0.0001\n",
            "'.bw': 0.0001\n",
            "'.mm': 0.0001\n",
            "'.tt': 0.0001\n",
            "'.media': 0.0001\n",
            "'.cm': 0.0001\n",
            "'.africa': 0.0001\n",
            "'.sc': 0.0001\n",
            "'.earth': 0.0001\n",
            "'.ly': 0.0001\n",
            "'.st': 0.0001\n",
            "'.plus': 0.0001\n",
            "'.cash': 0.0001\n",
            "'.pub': 0.0001\n",
            "'.eco': 0.0001\n",
            "'.bz': 0.0001\n",
            "'.af': 0.0001\n",
            "'.ax': 0.0001\n",
            "'.game': 0.0001\n",
            "'.ao': 0.0001\n",
            "'.cd': 0.0001\n",
            "'.ht': 0.0001\n",
            "'.tokyo': 0.0001\n",
            "'.band': 0.0001\n",
            "'.mc': 0.0001\n",
            "'.guru': 0.0001\n",
            "'.museum': 0.0001\n",
            "'.cloud': 0.0001\n",
            "'.games': 0.0001\n",
            "'.ngo': 0.0001\n",
            "'.paris': 0.0000\n",
            "'.chat': 0.0000\n",
            "'.mz': 0.0000\n",
            "'.gal': 0.0000\n",
            "'.church': 0.0000\n",
            "'.swiss': 0.0000\n",
            "'.la': 0.0000\n",
            "'.gh': 0.0000\n",
            "'.team': 0.0000\n",
            "'.tj': 0.0000\n",
            "'.kn': 0.0000\n",
            "'.city': 0.0000\n",
            "'.jobs': 0.0000\n",
            "'.corsica': 0.0000\n",
            "'.pn': 0.0000\n",
            "'.college': 0.0000\n",
            "'.tm': 0.0000\n",
            "'.iq': 0.0000\n",
            "'.sport': 0.0000\n",
            "'.health': 0.0000\n",
            "'.sy': 0.0000\n",
            "'.je': 0.0000\n",
            "'.academy': 0.0000\n",
            "'.lr': 0.0000\n",
            "'.cv': 0.0000\n",
            "'.vip': 0.0000\n",
            "'.care': 0.0000\n",
            "'.run': 0.0000\n",
            "'.ki': 0.0000\n",
            "'.ci': 0.0000\n",
            "'.rentals': 0.0000\n",
            "'.lgbt': 0.0000\n",
            "'.menu': 0.0000\n",
            "'.tk': 0.0000\n",
            "'.so': 0.0000\n",
            "'.pw': 0.0000\n",
            "'.press': 0.0000\n",
            "'.codes': 0.0000\n",
            "'.work': 0.0000\n",
            "'.kg': 0.0000\n",
            "'.bs': 0.0000\n",
            "'.kw': 0.0000\n",
            "'.technology': 0.0000\n",
            "'.green': 0.0000\n",
            "'.bio': 0.0000\n",
            "'.berlin': 0.0000\n",
            "'.mg': 0.0000\n",
            "'.watch': 0.0000\n",
            "'.cool': 0.0000\n",
            "'.pet': 0.0000\n",
            "'.aw': 0.0000\n",
            "'.clothing': 0.0000\n",
            "'.digital': 0.0000\n",
            "'.gd': 0.0000\n",
            "'.rugby': 0.0000\n",
            "'.bb': 0.0000\n",
            "'.youtube': 0.0000\n",
            "'.energy': 0.0000\n",
            "'.tc': 0.0000\n",
            "'.click': 0.0000\n",
            "'.wiki': 0.0000\n",
            "'.build': 0.0000\n",
            "'.bf': 0.0000\n",
            "'.photography': 0.0000\n",
            "'.marketing': 0.0000\n",
            "'.foundation': 0.0000\n",
            "'.sb': 0.0000\n",
            "'.dz': 0.0000\n",
            "'.ls': 0.0000\n",
            "'.top': 0.0000\n",
            "'.today': 0.0000\n",
            "'.tools': 0.0000\n",
            "'.tg': 0.0000\n",
            "'.company': 0.0000\n",
            "'.systems': 0.0000\n",
            "'.school': 0.0000\n",
            "'.house': 0.0000\n",
            "'.sz': 0.0000\n",
            "'.casa': 0.0000\n",
            "'.center': 0.0000\n",
            "'.lighting': 0.0000\n",
            "'.post': 0.0000\n",
            "'.ink': 0.0000\n",
            "'.holiday': 0.0000\n",
            "'.hamburg': 0.0000\n",
            "'.cab': 0.0000\n",
            "'.faith': 0.0000\n",
            "'.istanbul': 0.0000\n",
            "'.radio': 0.0000\n",
            "'.ltd': 0.0000\n",
            "'.weber': 0.0000\n",
            "'.dj': 0.0000\n",
            "'.ist': 0.0000\n",
            "'.link': 0.0000\n",
            "'.theater': 0.0000\n",
            "'.immo': 0.0000\n",
            "'.report': 0.0000\n",
            "'.ag': 0.0000\n",
            "'.as': 0.0000\n",
            "'.express': 0.0000\n",
            "'.direct': 0.0000\n",
            "'.services': 0.0000\n",
            "'.mq': 0.0000\n",
            "'.zone': 0.0000\n",
            "'.management': 0.0000\n",
            "'.party': 0.0000\n",
            "'.fitness': 0.0000\n",
            "'.vu': 0.0000\n",
            "'.ye': 0.0000\n",
            "'.domains': 0.0000\n",
            "'.vegas': 0.0000\n",
            "'.institute': 0.0000\n",
            "'.love': 0.0000\n",
            "'.fund': 0.0000\n",
            "'.krd': 0.0000\n",
            "'.wtf': 0.0000\n",
            "'.uno': 0.0000\n",
            "'.garden': 0.0000\n",
            "'.science': 0.0000\n",
            "'.et': 0.0000\n",
            "'.beer': 0.0000\n",
            "'.madrid': 0.0000\n",
            "'.gives': 0.0000\n",
            "'.pm': 0.0000\n",
            "'.vote': 0.0000\n",
            "'.family': 0.0000\n",
            "'.shoes': 0.0000\n",
            "'.dog': 0.0000\n",
            "'.mo': 0.0000\n",
            "'.golf': 0.0000\n",
            "'.va': 0.0000\n",
            "'.tf': 0.0000\n",
            "'.rocks': 0.0000\n",
            "'.car': 0.0000\n",
            "'.camp': 0.0000\n",
            "'.cx': 0.0000\n",
            "'.bn': 0.0000\n",
            "'.help': 0.0000\n",
            "'.taxi': 0.0000\n",
            "'.ga': 0.0000\n",
            "'.ruhr': 0.0000\n",
            "'.events': 0.0000\n",
            "'.tl': 0.0000\n",
            "'.lc': 0.0000\n",
            "'.cymru': 0.0000\n",
            "'.education': 0.0000\n",
            "'.best': 0.0000\n",
            "'.gallery': 0.0000\n",
            "'.schule': 0.0000\n",
            "'.ooo': 0.0000\n",
            "'.bike': 0.0000\n",
            "'.furniture': 0.0000\n",
            "'.moe': 0.0000\n",
            "'.gs': 0.0000\n",
            "'.movie': 0.0000\n",
            "'.computer': 0.0000\n",
            "'.cafe': 0.0000\n",
            "'.wf': 0.0000\n",
            "'.crs': 0.0000\n",
            "'.red': 0.0000\n",
            "'.business': 0.0000\n",
            "'.sd': 0.0000\n",
            "'.rip': 0.0000\n",
            "'.basketball': 0.0000\n",
            "'.fk': 0.0000\n",
            "'.market': 0.0000\n",
            "'.mma': 0.0000\n",
            "'.finance': 0.0000\n",
            "'.ck': 0.0000\n",
            "'.kiwi': 0.0000\n",
            "'.careers': 0.0000\n",
            "'.nagoya': 0.0000\n",
            "'.exchange': 0.0000\n",
            "'.group': 0.0000\n",
            "'.solutions': 0.0000\n",
            "'.law': 0.0000\n",
            "'.coffee': 0.0000\n",
            "'.mr': 0.0000\n",
            "'.community': 0.0000\n",
            "'.sm': 0.0000\n",
            "'.tirol': 0.0000\n",
            "'.taipei': 0.0000\n",
            "'.mp': 0.0000\n",
            "'.bayern': 0.0000\n",
            "'.sr': 0.0000\n",
            "'.studio': 0.0000\n",
            "'.pf': 0.0000\n",
            "'.diamonds': 0.0000\n",
            "'.dm': 0.0000\n",
            "'.guide': 0.0000\n",
            "'.nc': 0.0000\n",
            "'.associates': 0.0000\n",
            "'.reviews': 0.0000\n",
            "'.ninja': 0.0000\n",
            "Feature Filling Analysis\n",
            "--------------------------------------------------\n",
            "\n",
            "URL-derived Features:\n",
            "\n",
            "URLLength:\n",
            "Original missing values: 33545\n",
            "Remaining missing values: 0\n",
            "Filled values: 33545\n",
            "\n",
            "Distribution of filled values:\n",
            "count    33545.000000\n",
            "mean        28.541899\n",
            "std         11.661085\n",
            "min         15.000000\n",
            "25%         24.000000\n",
            "50%         27.000000\n",
            "75%         31.000000\n",
            "max        437.000000\n",
            "Name: URLLength, dtype: float64\n",
            "\n",
            "Domain:\n",
            "Original missing values: 38778\n",
            "Remaining missing values: 0\n",
            "Filled values: 38778\n",
            "\n",
            "Distribution of filled values:\n",
            "count       38778\n",
            "unique      38594\n",
            "top       ipfs.io\n",
            "freq           42\n",
            "Name: Domain, dtype: object\n",
            "\n",
            "DomainLength:\n",
            "Original missing values: 25649\n",
            "Remaining missing values: 0\n",
            "Filled values: 25649\n",
            "\n",
            "Distribution of filled values:\n",
            "count    25649.000000\n",
            "mean        19.575695\n",
            "std          5.829338\n",
            "min          4.000000\n",
            "25%         16.000000\n",
            "50%         19.000000\n",
            "75%         23.000000\n",
            "max        110.000000\n",
            "Name: DomainLength, dtype: float64\n",
            "\n",
            "TLD:\n",
            "Original missing values: 25115\n",
            "Remaining missing values: 0\n",
            "Filled values: 25115\n",
            "\n",
            "Distribution of filled values:\n",
            "count     25115\n",
            "unique      323\n",
            "top         com\n",
            "freq      12678\n",
            "Name: TLD, dtype: object\n",
            "\n",
            "TLDLength:\n",
            "Original missing values: 26347\n",
            "Remaining missing values: 0\n",
            "Filled values: 26347\n",
            "\n",
            "Distribution of filled values:\n",
            "count    26347.000000\n",
            "mean         2.731582\n",
            "std          0.520357\n",
            "min          2.000000\n",
            "25%          2.000000\n",
            "50%          3.000000\n",
            "75%          3.000000\n",
            "max         10.000000\n",
            "Name: TLDLength, dtype: float64\n",
            "\n",
            "NoOfSubDomain:\n",
            "Original missing values: 24254\n",
            "Remaining missing values: 0\n",
            "Filled values: 24254\n",
            "\n",
            "Distribution of filled values:\n",
            "count    24254.000000\n",
            "mean         2.158654\n",
            "std          0.436997\n",
            "min          1.000000\n",
            "25%          2.000000\n",
            "50%          2.000000\n",
            "75%          2.000000\n",
            "max          6.000000\n",
            "Name: NoOfSubDomain, dtype: float64\n",
            "\n",
            "IsHTTPS:\n",
            "Original missing values: 27363\n",
            "Remaining missing values: 0\n",
            "Filled values: 27363\n",
            "\n",
            "Distribution of filled values:\n",
            "count     27363\n",
            "unique        2\n",
            "top        True\n",
            "freq      26298\n",
            "Name: IsHTTPS, dtype: object\n",
            "\n",
            "IsDomainIP:\n",
            "Original missing values: 23335\n",
            "Remaining missing values: 0\n",
            "Filled values: 23335\n",
            "\n",
            "Distribution of filled values:\n",
            "count     23335\n",
            "unique        2\n",
            "top       False\n",
            "freq      23321\n",
            "Name: IsDomainIP, dtype: object\n",
            "\n",
            "NoOfLettersInURL:\n",
            "Original missing values: 35027\n",
            "Remaining missing values: 0\n",
            "Filled values: 35027\n",
            "\n",
            "Distribution of filled values:\n",
            "count    35027.000000\n",
            "mean        22.777343\n",
            "std          9.809370\n",
            "min          6.000000\n",
            "25%         18.000000\n",
            "50%         22.000000\n",
            "75%         25.000000\n",
            "max        529.000000\n",
            "Name: NoOfLettersInURL, dtype: float64\n",
            "\n",
            "LetterRatioInURL:\n",
            "Original missing values: 36324\n",
            "Remaining missing values: 0\n",
            "Filled values: 36324\n",
            "\n",
            "Distribution of filled values:\n",
            "count    36324.000000\n",
            "mean         0.796358\n",
            "std          0.047410\n",
            "min          0.181818\n",
            "25%          0.772727\n",
            "50%          0.800000\n",
            "75%          0.827586\n",
            "max          0.910714\n",
            "Name: LetterRatioInURL, dtype: float64\n",
            "\n",
            "NoOfDegitsInURL:\n",
            "Original missing values: 32443\n",
            "Remaining missing values: 0\n",
            "Filled values: 32443\n",
            "\n",
            "Distribution of filled values:\n",
            "count    32443.000000\n",
            "mean         0.347378\n",
            "std          2.966248\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          0.000000\n",
            "max        264.000000\n",
            "Name: NoOfDegitsInURL, dtype: float64\n",
            "\n",
            "DegitRatioInURL:\n",
            "Original missing values: 29658\n",
            "Remaining missing values: 0\n",
            "Filled values: 29658\n",
            "\n",
            "Distribution of filled values:\n",
            "count    29658.000000\n",
            "mean         0.006607\n",
            "std          0.033714\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          0.000000\n",
            "max          0.604119\n",
            "Name: DegitRatioInURL, dtype: float64\n",
            "\n",
            "NoOfEqualsInURL:\n",
            "Original missing values: 34058\n",
            "Remaining missing values: 0\n",
            "Filled values: 34058\n",
            "\n",
            "Distribution of filled values:\n",
            "count    34058.000000\n",
            "mean         0.007751\n",
            "std          0.175750\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          0.000000\n",
            "max         11.000000\n",
            "Name: NoOfEqualsInURL, dtype: float64\n",
            "\n",
            "NoOfQMarkInURL:\n",
            "Original missing values: 24423\n",
            "Remaining missing values: 0\n",
            "Filled values: 24423\n",
            "\n",
            "Distribution of filled values:\n",
            "count    24423.000000\n",
            "mean         0.004832\n",
            "std          0.081804\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          0.000000\n",
            "max          3.000000\n",
            "Name: NoOfQMarkInURL, dtype: float64\n",
            "\n",
            "NoOfAmpersandInURL:\n",
            "Original missing values: 25006\n",
            "Remaining missing values: 0\n",
            "Filled values: 25006\n",
            "\n",
            "Distribution of filled values:\n",
            "count    25006.000000\n",
            "mean         0.005639\n",
            "std          0.159007\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          0.000000\n",
            "max          9.000000\n",
            "Name: NoOfAmpersandInURL, dtype: float64\n",
            "\n",
            "NoOfOtherSpecialCharsInURL:\n",
            "Original missing values: 26276\n",
            "Remaining missing values: 0\n",
            "Filled values: 26276\n",
            "\n",
            "Distribution of filled values:\n",
            "count    26276.000000\n",
            "mean         5.398767\n",
            "std          1.207520\n",
            "min          4.000000\n",
            "25%          5.000000\n",
            "50%          5.000000\n",
            "75%          6.000000\n",
            "max         47.000000\n",
            "Name: NoOfOtherSpecialCharsInURL, dtype: float64\n",
            "\n",
            "SpacialCharRatioInURL:\n",
            "Original missing values: 34800\n",
            "Remaining missing values: 0\n",
            "Filled values: 34800\n",
            "\n",
            "Distribution of filled values:\n",
            "count    34800.000000\n",
            "mean         0.197232\n",
            "std          0.036918\n",
            "min          0.036199\n",
            "25%          0.172414\n",
            "50%          0.192308\n",
            "75%          0.222222\n",
            "max          0.360000\n",
            "Name: SpacialCharRatioInURL, dtype: float64\n",
            "\n",
            "CharContinuationRate:\n",
            "Original missing values: 26563\n",
            "Remaining missing values: 0\n",
            "Filled values: 26563\n",
            "\n",
            "Distribution of filled values:\n",
            "count    26563.000000\n",
            "mean         0.518118\n",
            "std          0.093541\n",
            "min          0.056497\n",
            "25%          0.454545\n",
            "50%          0.521739\n",
            "75%          0.586207\n",
            "max          0.800000\n",
            "Name: CharContinuationRate, dtype: float64\n",
            "\n",
            "Probability Features:\n",
            "\n",
            "URLCharProb:\n",
            "Original missing: 28773\n",
            "Remaining missing: 0\n",
            "Filled values: 28773\n",
            "\n",
            "Distribution of filled values:\n",
            "count    28773.000000\n",
            "mean         0.045772\n",
            "std          0.008999\n",
            "min          0.000860\n",
            "25%          0.041964\n",
            "50%          0.046514\n",
            "75%          0.051089\n",
            "max          0.079991\n",
            "Name: URLCharProb, dtype: float64\n",
            "\n",
            "TLDLegitimateProb:\n",
            "Original missing: 29069\n",
            "Remaining missing: 0\n",
            "Filled values: 29069\n",
            "\n",
            "Distribution of filled values:\n",
            "count    29069.000000\n",
            "mean         0.278057\n",
            "std          0.237738\n",
            "min          0.000010\n",
            "25%          0.014145\n",
            "50%          0.510163\n",
            "75%          0.510163\n",
            "max          0.510163\n",
            "Name: TLDLegitimateProb, dtype: float64\n",
            "\n",
            "Obfuscation Features:\n",
            "\n",
            "HasObfuscation:\n",
            "Original missing: 36369\n",
            "Remaining missing: 0\n",
            "Filled values: 36369\n",
            "\n",
            "Distribution of filled values:\n",
            "HasObfuscation\n",
            "False    35063\n",
            "True      1306\n",
            "Name: count, dtype: Int64\n",
            "\n",
            "NoOfObfuscatedChar:\n",
            "Original missing: 36983\n",
            "Remaining missing: 0\n",
            "Filled values: 36983\n",
            "\n",
            "Distribution of filled values:\n",
            "count    36983.000000\n",
            "mean         0.074142\n",
            "std          0.784681\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          0.000000\n",
            "max         70.000000\n",
            "Name: NoOfObfuscatedChar, dtype: float64\n",
            "\n",
            "ObfuscationRatio:\n",
            "Original missing: 35465\n",
            "Remaining missing: 0\n",
            "Filled values: 35465\n",
            "\n",
            "Distribution of filled values:\n",
            "count    35465.000000\n",
            "mean         0.001763\n",
            "std          0.012288\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          0.000000\n",
            "max          0.733333\n",
            "Name: ObfuscationRatio, dtype: float64\n",
            "\n",
            "Datasets saved to derived_url_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Fill dataset with URL-derived features\n",
        "\n",
        "def fill_url_derived_features(df_original, train_mode=True):\n",
        "    \"\"\"\n",
        "    Fill URL-based and URL-partially based features\n",
        "    \n",
        "    Args:\n",
        "    df_original: Original DataFrame\n",
        "    train_mode: If True, use this data to train probability calculator\n",
        "    \n",
        "    Returns:\n",
        "    DataFrame with filled features\n",
        "    \"\"\"\n",
        "    # Create copy to avoid modifying original\n",
        "    df = df_original.copy()\n",
        "    \n",
        "    # Initialize extractor\n",
        "    extractor = URLFeatureExtractor()\n",
        "    \n",
        "    # Get URLs that need processing (have at least one missing derived feature)\n",
        "    derived_features = [\n",
        "        'URLLength', 'Domain', 'DomainLength', 'TLD', 'TLDLength',\n",
        "        'NoOfSubDomain', 'IsHTTPS', 'IsDomainIP', 'NoOfLettersInURL',\n",
        "        'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL',\n",
        "        'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL',\n",
        "        'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL',\n",
        "        'CharContinuationRate'\n",
        "    ]\n",
        "    \n",
        "    # Process each URL with missing features\n",
        "    for idx, row in df.iterrows():\n",
        "        if row['URL'] is not None and any(pd.isna(row[feat]) for feat in derived_features):\n",
        "            features = extractor.extract_features(row['URL'])\n",
        "            for feat, value in features.items():\n",
        "                if pd.isna(df.at[idx, feat]):\n",
        "                    df.at[idx, feat] = value\n",
        "\n",
        "    # Initialize feature calculators\n",
        "    prob_calculator = URLProbabilityFeatures()\n",
        "    obfuscation_detector = URLObfuscationDetector()\n",
        "    \n",
        "    # Train probability calculator if in train mode\n",
        "    if train_mode:\n",
        "        prob_calculator.fit(df['URL'], df['label'])\n",
        "    \n",
        "    # Process URLs in batches to avoid memory issues\n",
        "    batch_size = 1000\n",
        "    total_rows = len(df)\n",
        "    \n",
        "    for start_idx in range(0, total_rows, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, total_rows)\n",
        "        batch_df = df.iloc[start_idx:end_idx]\n",
        "        \n",
        "        # Calculate probability features\n",
        "        if 'URLCharProb' in df.columns or 'TLDLegitimateProb' in df.columns:\n",
        "            prob_features = prob_calculator.transform(batch_df['URL'])\n",
        "            \n",
        "            # Fill missing values in URLCharProb\n",
        "            if 'URLCharProb' in df.columns:\n",
        "                mask = df.iloc[start_idx:end_idx]['URLCharProb'].isna()\n",
        "                df.loc[df.index[start_idx:end_idx][mask], 'URLCharProb'] = \\\n",
        "                    prob_features.loc[mask, 'URLCharProb']\n",
        "            \n",
        "            # Fill missing values in TLDLegitimateProb\n",
        "            if 'TLDLegitimateProb' in df.columns:\n",
        "                mask = df.iloc[start_idx:end_idx]['TLDLegitimateProb'].isna()\n",
        "                df.loc[df.index[start_idx:end_idx][mask], 'TLDLegitimateProb'] = \\\n",
        "                    prob_features.loc[mask, 'TLDLegitimateProb']\n",
        "        \n",
        "        # Detect obfuscation\n",
        "        for idx, row in batch_df.iterrows():\n",
        "            if pd.notna(row['URL']):\n",
        "                obfuscation_results = obfuscation_detector.detect_obfuscation(row['URL'])\n",
        "                \n",
        "                # Fill HasObfuscation\n",
        "                if 'HasObfuscation' in df.columns and pd.isna(df.at[idx, 'HasObfuscation']):\n",
        "                    df.at[idx, 'HasObfuscation'] = obfuscation_results['HasObfuscation']\n",
        "                \n",
        "                # Fill NoOfObfuscatedChar\n",
        "                if 'NoOfObfuscatedChar' in df.columns and pd.isna(df.at[idx, 'NoOfObfuscatedChar']):\n",
        "                    df.at[idx, 'NoOfObfuscatedChar'] = obfuscation_results['NoOfObfuscatedChar']\n",
        "                \n",
        "                # Fill ObfuscationRatio\n",
        "                if 'ObfuscationRatio' in df.columns and pd.isna(df.at[idx, 'ObfuscationRatio']):\n",
        "                    df.at[idx, 'ObfuscationRatio'] = obfuscation_results['ObfuscationRatio']\n",
        "    \n",
        "    return df\n",
        "\n",
        "derived_url_data = fill_url_derived_features(complete_url_data)\n",
        "\n",
        "def analyze_filling_results(df_original, df_filled):\n",
        "    \"\"\"\n",
        "    Analyze the results of feature filling\n",
        "    \n",
        "    Args:\n",
        "    df_original: Original DataFrame\n",
        "    df_filled: Filled DataFrame\n",
        "    \"\"\"\n",
        "    print(\"Feature Filling Analysis\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Features to analyze\n",
        "    derived_features = [\n",
        "        'URLLength', 'Domain', 'DomainLength', 'TLD', 'TLDLength',\n",
        "        'NoOfSubDomain', 'IsHTTPS', 'IsDomainIP', 'NoOfLettersInURL',\n",
        "        'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL',\n",
        "        'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL',\n",
        "        'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL',\n",
        "        'CharContinuationRate'\n",
        "    ]\n",
        "    prob_features = ['URLCharProb', 'TLDLegitimateProb']\n",
        "    obfuscation_features = ['HasObfuscation', 'NoOfObfuscatedChar', 'ObfuscationRatio']\n",
        "\n",
        "    # Analyze URL-derived features\n",
        "    print(\"\\nURL-derived Features:\")\n",
        "    for feature in derived_features:\n",
        "        if feature in df_original.columns:\n",
        "            orig_missing = df_original[feature].isna().sum()\n",
        "            filled_missing = df_filled[feature].isna().sum()\n",
        "            filled_count = orig_missing - filled_missing\n",
        "\n",
        "            print(f\"\\n{feature}:\")\n",
        "            print(f\"Original missing values: {orig_missing}\")\n",
        "            print(f\"Remaining missing values: {filled_missing}\")\n",
        "            print(f\"Filled values: {filled_count}\")\n",
        "\n",
        "            if filled_count > 0:\n",
        "                print(\"\\nDistribution of filled values:\")\n",
        "                filled_mask = df_original[feature].isna() & df_filled[feature].notna()\n",
        "                print(df_filled.loc[filled_mask, feature].describe())\n",
        "\n",
        "    # Analyze probability features\n",
        "    print(\"\\nProbability Features:\")\n",
        "    for feature in prob_features:\n",
        "        if feature in df_original.columns:\n",
        "            orig_missing = df_original[feature].isna().sum()\n",
        "            filled_missing = df_filled[feature].isna().sum()\n",
        "            filled_count = orig_missing - filled_missing\n",
        "            \n",
        "            print(f\"\\n{feature}:\")\n",
        "            print(f\"Original missing: {orig_missing}\")\n",
        "            print(f\"Remaining missing: {filled_missing}\")\n",
        "            print(f\"Filled values: {filled_count}\")\n",
        "            \n",
        "            if filled_count > 0:\n",
        "                print(\"\\nDistribution of filled values:\")\n",
        "                filled_mask = df_original[feature].isna() & df_filled[feature].notna()\n",
        "                print(df_filled.loc[filled_mask, feature].describe())\n",
        "    \n",
        "    # Analyze obfuscation features\n",
        "    print(\"\\nObfuscation Features:\")\n",
        "    for feature in obfuscation_features:\n",
        "        if feature in df_original.columns:\n",
        "            orig_missing = df_original[feature].isna().sum()\n",
        "            filled_missing = df_filled[feature].isna().sum()\n",
        "            filled_count = orig_missing - filled_missing\n",
        "            \n",
        "            print(f\"\\n{feature}:\")\n",
        "            print(f\"Original missing: {orig_missing}\")\n",
        "            print(f\"Remaining missing: {filled_missing}\")\n",
        "            print(f\"Filled values: {filled_count}\")\n",
        "            \n",
        "            if filled_count > 0:\n",
        "                if feature == 'HasObfuscation':\n",
        "                    filled_mask = df_original[feature].isna() & df_filled[feature].notna()\n",
        "                    value_counts = df_filled.loc[filled_mask, feature].value_counts()\n",
        "                    print(\"\\nDistribution of filled values:\")\n",
        "                    print(value_counts)\n",
        "                else:\n",
        "                    print(\"\\nDistribution of filled values:\")\n",
        "                    filled_mask = df_original[feature].isna() & df_filled[feature].notna()\n",
        "                    print(df_filled.loc[filled_mask, feature].describe())\n",
        "\n",
        "analyze_filling_results(complete_url_data, derived_url_data)\n",
        "\n",
        "def save_url_derived_datasets(derived_data, derived_path='derived_url_data.csv'):\n",
        "    \"\"\"\n",
        "    Saves the URL-derived dataset to CSV files\n",
        "    \n",
        "    Args:\n",
        "    complete_data: DataFrame with complete URL data\n",
        "    missing_url_data: DataFrame with missing URL data\n",
        "    complete_path: Path to save complete data\n",
        "    missing_path: Path to save missing URL data\n",
        "    \"\"\"\n",
        "    derived_data.to_csv(derived_path, index=False)\n",
        "    print(f\"\\nDatasets saved to {derived_path}\")\n",
        "\n",
        "save_url_derived_datasets(derived_url_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "             Missing Values Analysis              \n",
            "==================================================\n",
            "\n",
            "Missing Values Summary:\n",
            "                                 Data Type  Missing Count  Missing Percentage\n",
            "NoOfExternalRef                    float64          38400               49.47\n",
            "LineOfCode                         float64          38231               49.25\n",
            "HasSocialNet                       boolean          37564               48.39\n",
            "LargestLineLength                  float64          37542               48.36\n",
            "HasCopyrightInfo                   boolean          37158               47.87\n",
            "NoOfURLRedirect                    float64          37141               47.84\n",
            "NoOfCSS                            float64          36991               47.65\n",
            "NoOfSelfRedirect                   float64          36924               47.57\n",
            "HasPasswordField                   boolean          36691               47.27\n",
            "HasSubmitButton                    boolean          34078               43.90\n",
            "NoOfJS                             float64          33492               43.14\n",
            "HasFavicon                         boolean          32397               41.73\n",
            "Title                       string[python]          32258               41.55\n",
            "HasExternalFormSubmit              boolean          30771               39.64\n",
            "Bank                               boolean          30337               39.08\n",
            "HasDescription                     boolean          30150               38.84\n",
            "URLTitleMatchScore                 float64          28859               37.18\n",
            "Crypto                             boolean          27847               35.87\n",
            "NoOfImage                          float64          27811               35.83\n",
            "DomainTitleMatchScore              float64          27558               35.50\n",
            "NoOfiFrame                         float64          27473               35.39\n",
            "NoOfSelfRef                        float64          26558               34.21\n",
            "Robots                             boolean          26135               33.67\n",
            "HasTitle                           boolean          24673               31.78\n",
            "HasHiddenFields                    boolean          24126               31.08\n",
            "Pay                                boolean          23932               30.83\n",
            "NoOfPopup                          float64          23797               30.66\n",
            "NoOfEmptyRef                       float64          23616               30.42\n",
            "IsResponsive                       boolean          23489               30.26\n",
            "id                                   int64              0                0.00\n",
            "URL                         string[python]              0                0.00\n",
            "HasObfuscation                     boolean              0                0.00\n",
            "URLLength                          float64              0                0.00\n",
            "Domain                      string[python]              0                0.00\n",
            "DomainLength                       float64              0                0.00\n",
            "IsDomainIP                         boolean              0                0.00\n",
            "TLD                                 object              0                0.00\n",
            "CharContinuationRate               float64              0                0.00\n",
            "TLDLegitimateProb                  float64              0                0.00\n",
            "URLCharProb                        float64              0                0.00\n",
            "TLDLength                          float64              0                0.00\n",
            "NoOfSubDomain                      float64              0                0.00\n",
            "NoOfObfuscatedChar                 float64              0                0.00\n",
            "IsHTTPS                            boolean              0                0.00\n",
            "ObfuscationRatio                   float64              0                0.00\n",
            "NoOfLettersInURL                   float64              0                0.00\n",
            "LetterRatioInURL                   float64              0                0.00\n",
            "NoOfDegitsInURL                    float64              0                0.00\n",
            "DegitRatioInURL                    float64              0                0.00\n",
            "NoOfEqualsInURL                    float64              0                0.00\n",
            "NoOfQMarkInURL                     float64              0                0.00\n",
            "NoOfAmpersandInURL                 float64              0                0.00\n",
            "NoOfOtherSpecialCharsInURL         float64              0                0.00\n",
            "SpacialCharRatioInURL              float64              0                0.00\n",
            "label                              boolean              0                0.00\n",
            "\n",
            "==================================================\n",
            "              Distribution Analysis               \n",
            "==================================================\n",
            "\n",
            "Feature Imputation Summary:\n",
            "\n",
            "Column: URLLength\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Imputation using Predictive Models\n",
            "    Reason: Strong correlations with NoOfLettersInURL, NoOfDegitsInURL\n",
            "    Top Correlations:\n",
            "        - NoOfLettersInURL: 0.943\n",
            "        - NoOfDegitsInURL: 0.811\n",
            "\n",
            "Column: DomainLength\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: CharContinuationRate\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Left, Near-Symmetric Distribution\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Left, Near-Symmetric Distribution\n",
            "\n",
            "Column: TLDLegitimateProb\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Left, Near-Symmetric Distribution\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Left, Near-Symmetric Distribution\n",
            "\n",
            "Column: URLCharProb\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), Moderate Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), Moderate Skewness\n",
            "\n",
            "Column: TLDLength\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Left, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Left, High Skewness\n",
            "\n",
            "Column: NoOfSubDomain\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: NoOfObfuscatedChar\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: ObfuscationRatio\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfLettersInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Imputation using Predictive Models\n",
            "    Reason: Strong correlations with URLLength\n",
            "    Top Correlations:\n",
            "        - URLLength: 0.943\n",
            "\n",
            "Column: LetterRatioInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), Near-Symmetric Distribution\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), Near-Symmetric Distribution\n",
            "\n",
            "Column: NoOfDegitsInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Imputation using Predictive Models\n",
            "    Reason: Strong correlations with URLLength\n",
            "    Top Correlations:\n",
            "        - URLLength: 0.811\n",
            "\n",
            "Column: DegitRatioInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfEqualsInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfQMarkInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfAmpersandInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfOtherSpecialCharsInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: SpacialCharRatioInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, Near-Symmetric Distribution\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, Near-Symmetric Distribution\n",
            "\n",
            "Column: LineOfCode\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: LargestLineLength\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: DomainTitleMatchScore\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Left, Moderate Skewness\n",
            "    Imputation Strategy: Imputation using Predictive Models\n",
            "    Reason: Strong correlations with URLTitleMatchScore\n",
            "    Top Correlations:\n",
            "        - URLTitleMatchScore: 0.991\n",
            "\n",
            "Column: URLTitleMatchScore\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Left, Moderate Skewness\n",
            "    Imputation Strategy: Imputation using Predictive Models\n",
            "    Reason: Strong correlations with DomainTitleMatchScore\n",
            "    Top Correlations:\n",
            "        - DomainTitleMatchScore: 0.991\n",
            "\n",
            "Column: NoOfURLRedirect\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: NoOfSelfRedirect\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfPopup\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfiFrame\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: NoOfImage\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfCSS\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: NoOfJS\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfSelfRef\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfEmptyRef\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfExternalRef\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: HasTitle\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasFavicon\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: Robots\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: IsResponsive\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasDescription\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasExternalFormSubmit\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasSocialNet\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasSubmitButton\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasHiddenFields\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasPasswordField\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: Bank\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: Pay\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: Crypto\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasCopyrightInfo\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "==================================================\n",
            "          ADVANCED IMPUTATION STRATEGIES          \n",
            "==================================================\n",
            "\n",
            "Memulai imputasi kolom boolean...\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasTitle:\n",
            "  - Jumlah nilai hilang: 24673\n",
            "  - Rasio True: 97.49%\n",
            "  - Strategi: majority_true\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasFavicon:\n",
            "  - Jumlah nilai hilang: 32397\n",
            "  - Rasio True: 53.35%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk Robots:\n",
            "  - Jumlah nilai hilang: 26135\n",
            "  - Rasio True: 38.90%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk IsResponsive:\n",
            "  - Jumlah nilai hilang: 23489\n",
            "  - Rasio True: 81.15%\n",
            "  - Strategi: majority_true\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasDescription:\n",
            "  - Jumlah nilai hilang: 30150\n",
            "  - Rasio True: 68.60%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasExternalFormSubmit:\n",
            "  - Jumlah nilai hilang: 30771\n",
            "  - Rasio True: 6.83%\n",
            "  - Strategi: majority_false\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasSocialNet:\n",
            "  - Jumlah nilai hilang: 37564\n",
            "  - Rasio True: 73.57%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasSubmitButton:\n",
            "  - Jumlah nilai hilang: 34078\n",
            "  - Rasio True: 61.47%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasHiddenFields:\n",
            "  - Jumlah nilai hilang: 24126\n",
            "  - Rasio True: 55.17%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasPasswordField:\n",
            "  - Jumlah nilai hilang: 36691\n",
            "  - Rasio True: 13.01%\n",
            "  - Strategi: majority_false\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk Bank:\n",
            "  - Jumlah nilai hilang: 30337\n",
            "  - Rasio True: 17.26%\n",
            "  - Strategi: majority_false\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk Pay:\n",
            "  - Jumlah nilai hilang: 23932\n",
            "  - Rasio True: 34.55%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk Crypto:\n",
            "  - Jumlah nilai hilang: 27847\n",
            "  - Rasio True: 3.44%\n",
            "  - Strategi: majority_false\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasCopyrightInfo:\n",
            "  - Jumlah nilai hilang: 37158\n",
            "  - Rasio True: 75.06%\n",
            "  - Strategi: majority_true\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Handling column: URLLength (Data Type: float64)\n",
            "\n",
            "Handling column: DomainLength (Data Type: float64)\n",
            "\n",
            "Handling column: CharContinuationRate (Data Type: float64)\n",
            "\n",
            "Handling column: TLDLegitimateProb (Data Type: float64)\n",
            "\n",
            "Handling column: URLCharProb (Data Type: float64)\n",
            "\n",
            "Handling column: TLDLength (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfSubDomain (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfObfuscatedChar (Data Type: float64)\n",
            "\n",
            "Handling column: ObfuscationRatio (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfLettersInURL (Data Type: float64)\n",
            "\n",
            "Handling column: LetterRatioInURL (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfDegitsInURL (Data Type: float64)\n",
            "\n",
            "Handling column: DegitRatioInURL (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfEqualsInURL (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfQMarkInURL (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfAmpersandInURL (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfOtherSpecialCharsInURL (Data Type: float64)\n",
            "\n",
            "Handling column: SpacialCharRatioInURL (Data Type: float64)\n",
            "\n",
            "Handling column: LineOfCode (Data Type: float64)\n",
            "\n",
            "Handling column: LargestLineLength (Data Type: float64)\n",
            "\n",
            "Handling column: DomainTitleMatchScore (Data Type: float64)\n",
            "\n",
            "Handling column: URLTitleMatchScore (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfURLRedirect (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfSelfRedirect (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfPopup (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfiFrame (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfImage (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfCSS (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfJS (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfSelfRef (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfEmptyRef (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfExternalRef (Data Type: float64)\n",
            "\n",
            "Imputation process completed successfully.\n",
            "Successfully imputed columns:\n",
            "['LineOfCode', 'LargestLineLength', 'DomainTitleMatchScore', 'URLTitleMatchScore', 'NoOfURLRedirect', 'NoOfSelfRedirect', 'NoOfPopup', 'NoOfiFrame', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef', 'NoOfExternalRef']\n",
            "\n",
            "Datasets saved to completed_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.impute import SimpleImputer, IterativeImputer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from scipy.stats import chi2_contingency\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Optional, Tuple, Any\n",
        "\n",
        "@dataclass\n",
        "class FeatureStats:\n",
        "    \"\"\"Data class to store feature statistics\"\"\"\n",
        "    data_type: str\n",
        "    distribution_type: Optional[str] = None\n",
        "    imputation_strategy: str = \"\"\n",
        "    reason: str = \"\"\n",
        "    top_correlations: Optional[Dict[str, float]] = None\n",
        "\n",
        "@dataclass\n",
        "class BooleanImputationStats:\n",
        "    \"\"\"Data class untuk menyimpan statistik imputasi boolean\"\"\"\n",
        "    original_count: int\n",
        "    missing_count: int\n",
        "    true_ratio: float\n",
        "    imputation_strategy: str\n",
        "    imputed_value: bool\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"Class for comprehensive data analysis and imputation\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame, verbose: int = 1):\n",
        "        self.df = df.copy()\n",
        "        self.verbose = verbose\n",
        "        self.numerical_columns, self.categorical_columns = self._define_column_types()\n",
        "        self.strategy_summary = {}\n",
        "        self.boolean_columns = self._get_boolean_columns()\n",
        "        self.boolean_stats = {}\n",
        "        warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "    def _get_boolean_columns(self) -> list:\n",
        "        \"\"\"Identify boolean columns in DataFrame\"\"\"\n",
        "        return list(self.df.select_dtypes(include=['bool', 'boolean']).columns)\n",
        "\n",
        "    def _analyze_boolean_distribution(self, column: str) -> Tuple[float, str, bool]:\n",
        "        \"\"\"Analyze boolean distribution and determine imputation strategy\"\"\"\n",
        "        true_ratio = self.df[column].mean(skipna=True)\n",
        "\n",
        "        if pd.isna(true_ratio):\n",
        "            strategy = \"default_false\"\n",
        "            imputed_value = False\n",
        "        elif true_ratio == 0.5:\n",
        "            strategy = \"balanced_false\"\n",
        "            imputed_value = False\n",
        "        elif true_ratio > 0.75:\n",
        "            strategy = \"majority_true\"\n",
        "            imputed_value = True\n",
        "        elif true_ratio < 0.25:\n",
        "            strategy = \"majority_false\"\n",
        "            imputed_value = False\n",
        "        else:\n",
        "            strategy = \"mode\"\n",
        "            imputed_value = true_ratio > 0.5\n",
        "\n",
        "        return true_ratio, strategy, imputed_value\n",
        "    \n",
        "    def _define_column_types(self) -> Tuple[pd.Index, pd.Index]:\n",
        "        \"\"\"Identify numerical and categorical columns\"\"\"\n",
        "        numerical = self.df.select_dtypes(include=['int64', 'float64']).columns\n",
        "        categorical = self.df.select_dtypes(include=['object', 'boolean', 'bool']).columns\n",
        "\n",
        "        # Exclude non-analytical columns\n",
        "        numerical = numerical.drop('id', errors='ignore')\n",
        "        categorical = categorical.drop(['URL', 'Domain', 'FILENAME', 'Title'], errors='ignore')\n",
        "\n",
        "        return numerical, categorical\n",
        "    \n",
        "    def analyze_missing_values(self) -> pd.DataFrame:\n",
        "        \"\"\"Analyze missing values in the dataset\"\"\"\n",
        "        self._print_section(\"Missing Values Analysis\")\n",
        "\n",
        "        missing_stats = pd.DataFrame({\n",
        "            'Data Type': self.df.dtypes,\n",
        "            'Missing Count': self.df.isnull().sum(),\n",
        "            'Missing Percentage': (self.df.isnull().sum() / len(self.df) * 100).round(2)\n",
        "        }).sort_values('Missing Percentage', ascending=False)\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print(\"\\nMissing Values Summary:\")\n",
        "            with pd.option_context('display.max_rows', None):\n",
        "                print(missing_stats.to_string())\n",
        "\n",
        "        return missing_stats\n",
        "\n",
        "    def _print_section(self, title: str) -> None:\n",
        "        \"\"\"Print formatted section header\"\"\"\n",
        "        if self.verbose > 0:\n",
        "            print(f\"\\n{'='*50}\\n{title.center(50)}\\n{'='*50}\")\n",
        "\n",
        "\n",
        "\n",
        "    def impute_boolean_values(self, df_imputed: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Handle boolean imputation specifically\"\"\"\n",
        "        if self.verbose > 0:\n",
        "            print(\"\\nMemulai imputasi kolom boolean...\")\n",
        "\n",
        "        for column in self.boolean_columns:\n",
        "            if self.verbose > 1:\n",
        "                print(f\"\\nMemproses kolom boolean: {column}\")\n",
        "\n",
        "            missing_count = df_imputed[column].isnull().sum()\n",
        "\n",
        "            if missing_count == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Analyze distribution and determine strategy\n",
        "                true_ratio, strategy, imputed_value = self._analyze_boolean_distribution(column)\n",
        "\n",
        "                # Perform imputation\n",
        "                df_imputed[column] = df_imputed[column].fillna(imputed_value)\n",
        "\n",
        "                # Ensure boolean type\n",
        "                df_imputed[column] = df_imputed[column].astype('boolean')\n",
        "\n",
        "                # Store statistics\n",
        "                self.boolean_stats[column] = BooleanImputationStats(\n",
        "                    original_count=len(df_imputed[column]),\n",
        "                    missing_count=missing_count,\n",
        "                    true_ratio=true_ratio,\n",
        "                    imputation_strategy=strategy,\n",
        "                    imputed_value=imputed_value\n",
        "                )\n",
        "\n",
        "                if self.verbose > 0:\n",
        "                    self._print_boolean_summary(column)\n",
        "\n",
        "            except Exception as e:\n",
        "                if self.verbose > 0:\n",
        "                    print(f\"Error pada imputasi kolom boolean {column}: {str(e)}\")\n",
        "                    print(\"Menggunakan nilai default False\")\n",
        "                df_imputed[column] = df_imputed[column].fillna(False).astype('boolean')\n",
        "\n",
        "        return df_imputed\n",
        "    \n",
        "    def analyze_feature_distributions(self) -> Dict[str, FeatureStats]:\n",
        "        \"\"\"Analyze distributions and determine imputation strategies\"\"\"\n",
        "        self._print_section(\"Distribution Analysis\")\n",
        "\n",
        "        # Analyze numerical features\n",
        "        for col in self.numerical_columns:\n",
        "            clean_data = self.df[col].dropna()\n",
        "            if len(clean_data) > 0:\n",
        "                stats = {\n",
        "                    'mean': clean_data.mean(),\n",
        "                    'median': clean_data.median(),\n",
        "                    'std': clean_data.std(),\n",
        "                    'skew': clean_data.skew(),\n",
        "                    'kurtosis': clean_data.kurtosis()\n",
        "                }\n",
        "\n",
        "                distribution_type = self._interpret_distribution(stats)\n",
        "\n",
        "                correlations = self.df[self.numerical_columns].corr()[col].abs().sort_values(ascending=False)\n",
        "                # Adjust correlation thresholds to 0.75 (strong correlation as per rules of thumb)\n",
        "                top_corr = correlations[correlations >= 0.75].drop(col, errors='ignore')\n",
        "\n",
        "                strategy = \"Imputation using Predictive Models\" if len(top_corr) > 0 else stats['suggested_strategy']\n",
        "                reason = (f\"Strong correlations with {', '.join(top_corr.index)}\" \n",
        "                        if len(top_corr) > 0 \n",
        "                        else f\"Based on {distribution_type}\")\n",
        "\n",
        "                self.strategy_summary[col] = FeatureStats(\n",
        "                    data_type=str(self.df[col].dtype),\n",
        "                    distribution_type=distribution_type,\n",
        "                    imputation_strategy=strategy,\n",
        "                    reason=reason,\n",
        "                    top_correlations=top_corr[:5].to_dict()\n",
        "                )\n",
        "\n",
        "        # Analyze categorical features\n",
        "        for col in self.categorical_columns:\n",
        "            if self.df[col].isnull().any():\n",
        "                self.strategy_summary[col] = FeatureStats(\n",
        "                    data_type=str(self.df[col].dtype),\n",
        "                    imputation_strategy=\"Mode Imputation\",\n",
        "                    reason=\"Standard approach for categorical data\"\n",
        "                )\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            self._print_summary()\n",
        "\n",
        "        return self.strategy_summary\n",
        "\n",
        "\n",
        "    def _print_boolean_summary(self, column: str) -> None:\n",
        "        \"\"\"Print boolean imputation summary\"\"\"\n",
        "        stats = self.boolean_stats[column]\n",
        "        print(f\"\\nRingkasan imputasi boolean untuk {column}:\")\n",
        "        print(f\"  - Jumlah nilai hilang: {stats.missing_count}\")\n",
        "        print(f\"  - Rasio True: {stats.true_ratio:.2%}\")\n",
        "        print(f\"  - Strategi: {stats.imputation_strategy}\")\n",
        "        print(f\"  - Nilai imputasi: {stats.imputed_value}\")\n",
        "\n",
        "    def impute_missing_values(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Handle missing values for both boolean and non-boolean columns\n",
        "        and combine the results into a single DataFrame.\n",
        "        \"\"\"\n",
        "        warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "        df_imputed = self.df.copy().replace({pd.NA: np.nan})\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print(\"\\n\" + \"=\" * 50)\n",
        "            print(\"ADVANCED IMPUTATION STRATEGIES\".center(50))\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "        # Handle boolean columns\n",
        "        if self.boolean_columns:\n",
        "            df_imputed = self.impute_boolean_values(df_imputed)\n",
        "\n",
        "        # Handle non-boolean columns\n",
        "        imputed_columns = []\n",
        "\n",
        "        for col, details in self.strategy_summary.items():\n",
        "            if col in self.boolean_columns:\n",
        "                continue  # Skip boolean columns as they're already handled\n",
        "\n",
        "            if self.verbose > 0:\n",
        "                print(f\"\\nHandling column: {col} (Data Type: {details.data_type})\")\n",
        "\n",
        "            missing_count = df_imputed[col].isnull().sum()\n",
        "            if missing_count == 0:\n",
        "                continue  # Skip columns without missing values\n",
        "\n",
        "            try:\n",
        "                # Apply the suggested imputation strategy\n",
        "                if details.imputation_strategy == \"Mean Imputation\":\n",
        "                    imputer = SimpleImputer(strategy=\"mean\")\n",
        "                elif details.imputation_strategy == \"Median Imputation\":\n",
        "                    imputer = SimpleImputer(strategy=\"median\")\n",
        "                elif details.imputation_strategy == \"Mode Imputation\":\n",
        "                    imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "                else:\n",
        "                    # For more complex strategies like predictive modeling\n",
        "                    imputer = IterativeImputer(max_iter=10, random_state=0)\n",
        "\n",
        "                # Fit and transform the column\n",
        "                df_imputed[col] = imputer.fit_transform(df_imputed[[col]])\n",
        "\n",
        "                imputed_columns.append(col)\n",
        "            except Exception as e:\n",
        "                print(f\"Error imputing column {col}: {str(e)}\")\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print(\"\\nImputation process completed successfully.\")\n",
        "            print(\"Successfully imputed columns:\")\n",
        "            print(imputed_columns)\n",
        "\n",
        "        return df_imputed\n",
        "\n",
        "\n",
        "    def get_boolean_imputation_report(self) -> pd.DataFrame:\n",
        "        \"\"\"Generate boolean imputation report\"\"\"\n",
        "        report_data = []\n",
        "        for column, stats in self.boolean_stats.items():\n",
        "            report_data.append({\n",
        "                'Column': column,\n",
        "                'Missing Values': stats.missing_count,\n",
        "                'True Ratio': f\"{stats.true_ratio:.2%}\",\n",
        "                'Strategy': stats.imputation_strategy,\n",
        "                'Imputed Value': stats.imputed_value\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(report_data)\n",
        "\n",
        "    def _interpret_distribution(self, stats_dict: Dict[str, float]) -> str:\n",
        "        \"\"\"Interpret statistical distributions\"\"\"\n",
        "        interpretation = []\n",
        "\n",
        "        # Perbedaan antara mean dan median terhadap standar deviasi\n",
        "        mean_median_diff = abs(stats_dict['mean'] - stats_dict['median']) / stats_dict['std']\n",
        "        if mean_median_diff < 0.2:\n",
        "            interpretation.append(\"Relatively Symmetric (Normal Distribution)\")\n",
        "            stats_dict['suggested_strategy'] = \"Mean Imputation\"\n",
        "        else:\n",
        "            direction = \"Right\" if stats_dict['mean'] > stats_dict['median'] else \"Left\"\n",
        "            interpretation.append(f\"Skewed to {direction}\")\n",
        "            stats_dict['suggested_strategy'] = \"Median Imputation\"\n",
        "\n",
        "        # Analisis skewness\n",
        "        if abs(stats_dict['skew']) < 0.5:\n",
        "            interpretation.append(\"Near-Symmetric Distribution\")\n",
        "        elif abs(stats_dict['skew']) < 1:\n",
        "            interpretation.append(\"Moderate Skewness\")\n",
        "        else:\n",
        "            interpretation.append(\"High Skewness\")\n",
        "\n",
        "        return \", \".join(interpretation)\n",
        "    \n",
        "    def _print_summary(self) -> None:\n",
        "        \"\"\"Print formatted analysis summary\"\"\"\n",
        "        print(\"\\nFeature Imputation Summary:\")\n",
        "        for col, stats in self.strategy_summary.items():\n",
        "            print(f\"\\nColumn: {col}\")\n",
        "            print(f\"    Data Type: {stats.data_type}\")\n",
        "            if stats.distribution_type:\n",
        "                print(f\"    Distribution: {stats.distribution_type}\")\n",
        "            print(f\"    Imputation Strategy: {stats.imputation_strategy}\")\n",
        "            print(f\"    Reason: {stats.reason}\")\n",
        "            if stats.top_correlations:\n",
        "                print(\"    Top Correlations:\")\n",
        "                for feature, corr in stats.top_correlations.items():\n",
        "                    print(f\"        - {feature}: {corr:.3f}\")\n",
        "\n",
        "    def _validate_imputation(self, df_before: pd.DataFrame, df_after: pd.DataFrame) -> None:\n",
        "        \"\"\"\n",
        "        Validate the imputation process by comparing the number of missing values\n",
        "        before and after imputation.\n",
        "\n",
        "        Args:\n",
        "            df_before: DataFrame before imputation.\n",
        "            df_after: DataFrame after imputation.\n",
        "        \"\"\"\n",
        "        missing_before = df_before.isnull().sum()\n",
        "        missing_after = df_after.isnull().sum()\n",
        "\n",
        "        comparison = pd.DataFrame({\n",
        "            'Missing Before': missing_before,\n",
        "            'Missing After': missing_after,\n",
        "            'Values Imputed': missing_before - missing_after\n",
        "        })\n",
        "\n",
        "        print(\"\\nImputation Validation:\")\n",
        "        print(comparison[comparison['Values Imputed'] > 0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_dataset(df: pd.DataFrame, verbose: int = 1) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Main function to analyze and process dataset\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        verbose: Verbosity level\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing processed DataFrame and analysis results\n",
        "    \"\"\"\n",
        "    processor = DataPreprocessor(df, verbose)\n",
        "\n",
        "    # Analyze missing values and distributions\n",
        "    missing_analysis = processor.analyze_missing_values()\n",
        "    feature_analysis = processor.analyze_feature_distributions()\n",
        "\n",
        "    # Perform imputation\n",
        "    df_processed = processor.impute_missing_values()\n",
        "\n",
        "    results = {\n",
        "        'missing_analysis': missing_analysis,\n",
        "        'feature_analysis': feature_analysis,\n",
        "        'boolean_imputation_report': processor.get_boolean_imputation_report()\n",
        "    }\n",
        "\n",
        "    return df_processed, results\n",
        "\n",
        "\n",
        "\n",
        "# Process the dataset (analysis + imputation)\n",
        "df_completed, results = process_dataset(derived_url_data, verbose=1)\n",
        "\n",
        "# Access specific results\n",
        "missing_analysis = results['missing_analysis']\n",
        "feature_analysis = results['feature_analysis']\n",
        "boolean_report = results['boolean_imputation_report']\n",
        "\n",
        "# Save results\n",
        "def save_completed_datasets(completed_data, completed_path='completed_data.csv'):\n",
        "    \"\"\"\n",
        "    Saves the completed dataset to CSV files\n",
        "\n",
        "    Args:\n",
        "        completed_data: DataFrame with complete URL data\n",
        "        completed_path: Path to save complete data\n",
        "    \"\"\"\n",
        "    completed_data.to_csv(completed_path, index=False)\n",
        "    print(f\"\\nDatasets saved to {completed_path}\")\n",
        "\n",
        "save_completed_datasets(df_completed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id                                0\n",
            "URL                               0\n",
            "URLLength                         0\n",
            "Domain                            0\n",
            "DomainLength                      0\n",
            "IsDomainIP                        0\n",
            "TLD                               0\n",
            "CharContinuationRate              0\n",
            "TLDLegitimateProb                 0\n",
            "URLCharProb                       0\n",
            "TLDLength                         0\n",
            "NoOfSubDomain                     0\n",
            "HasObfuscation                    0\n",
            "NoOfObfuscatedChar                0\n",
            "ObfuscationRatio                  0\n",
            "NoOfLettersInURL                  0\n",
            "LetterRatioInURL                  0\n",
            "NoOfDegitsInURL                   0\n",
            "DegitRatioInURL                   0\n",
            "NoOfEqualsInURL                   0\n",
            "NoOfQMarkInURL                    0\n",
            "NoOfAmpersandInURL                0\n",
            "NoOfOtherSpecialCharsInURL        0\n",
            "SpacialCharRatioInURL             0\n",
            "IsHTTPS                           0\n",
            "LineOfCode                    38231\n",
            "LargestLineLength             37542\n",
            "HasTitle                      24673\n",
            "Title                         32258\n",
            "DomainTitleMatchScore         27558\n",
            "URLTitleMatchScore            28859\n",
            "HasFavicon                    32397\n",
            "Robots                        26135\n",
            "IsResponsive                  23489\n",
            "NoOfURLRedirect               37141\n",
            "NoOfSelfRedirect              36924\n",
            "HasDescription                30150\n",
            "NoOfPopup                     23797\n",
            "NoOfiFrame                    27473\n",
            "HasExternalFormSubmit         30771\n",
            "HasSocialNet                  37564\n",
            "HasSubmitButton               34078\n",
            "HasHiddenFields               24126\n",
            "HasPasswordField              36691\n",
            "Bank                          30337\n",
            "Pay                           23932\n",
            "Crypto                        27847\n",
            "HasCopyrightInfo              37158\n",
            "NoOfImage                     27811\n",
            "NoOfCSS                       36991\n",
            "NoOfJS                        33492\n",
            "NoOfSelfRef                   26558\n",
            "NoOfEmptyRef                  23616\n",
            "NoOfExternalRef               38400\n",
            "label                             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Muat dataset dari file CSV\n",
        "derived_url_data = pd.read_csv(\"derived_url_data.csv\")\n",
        "\n",
        "# Periksa apakah ada nilai yang hilang\n",
        "print(derived_url_data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_boolean_columns(df: pd.DataFrame, boolean_columns: list) -> pd.DataFrame:\n",
        "    \"\"\"Konversi kolom boolean agar sesuai tipe data\"\"\"\n",
        "    for col in boolean_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(\"boolean\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "boolean_columns = [\n",
        "    \"IsDomainIP\", \"HasObfuscation\", \"IsHTTPS\", \"HasTitle\", \"HasFavicon\", \"Robots\", \n",
        "    \"IsResponsive\", \"HasDescription\", \"HasExternalFormSubmit\", \"HasSocialNet\",\n",
        "    \"HasSubmitButton\", \"HasHiddenFields\", \"HasPasswordField\", \"Bank\", \"Pay\", \n",
        "    \"Crypto\", \"HasCopyrightInfo\", \"label\"\n",
        "]\n",
        "derived_url_data = ensure_boolean_columns(derived_url_data, boolean_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "processor = DataPreprocessor(derived_url_data, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "             Missing Values Analysis              \n",
            "==================================================\n",
            "\n",
            "Missing Values Summary:\n",
            "                           Data Type  Missing Count  Missing Percentage\n",
            "NoOfExternalRef              float64          38400               49.47\n",
            "LineOfCode                   float64          38231               49.25\n",
            "HasSocialNet                 boolean          37564               48.39\n",
            "LargestLineLength            float64          37542               48.36\n",
            "HasCopyrightInfo             boolean          37158               47.87\n",
            "NoOfURLRedirect              float64          37141               47.84\n",
            "NoOfCSS                      float64          36991               47.65\n",
            "NoOfSelfRedirect             float64          36924               47.57\n",
            "HasPasswordField             boolean          36691               47.27\n",
            "HasSubmitButton              boolean          34078               43.90\n",
            "NoOfJS                       float64          33492               43.14\n",
            "HasFavicon                   boolean          32397               41.73\n",
            "Title                         object          32258               41.55\n",
            "HasExternalFormSubmit        boolean          30771               39.64\n",
            "Bank                         boolean          30337               39.08\n",
            "HasDescription               boolean          30150               38.84\n",
            "URLTitleMatchScore           float64          28859               37.18\n",
            "Crypto                       boolean          27847               35.87\n",
            "NoOfImage                    float64          27811               35.83\n",
            "DomainTitleMatchScore        float64          27558               35.50\n",
            "NoOfiFrame                   float64          27473               35.39\n",
            "NoOfSelfRef                  float64          26558               34.21\n",
            "Robots                       boolean          26135               33.67\n",
            "HasTitle                     boolean          24673               31.78\n",
            "HasHiddenFields              boolean          24126               31.08\n",
            "Pay                          boolean          23932               30.83\n",
            "NoOfPopup                    float64          23797               30.66\n",
            "NoOfEmptyRef                 float64          23616               30.42\n",
            "IsResponsive                 boolean          23489               30.26\n",
            "id                             int64              0                0.00\n",
            "URL                           object              0                0.00\n",
            "HasObfuscation               boolean              0                0.00\n",
            "URLLength                    float64              0                0.00\n",
            "Domain                        object              0                0.00\n",
            "DomainLength                 float64              0                0.00\n",
            "IsDomainIP                   boolean              0                0.00\n",
            "TLD                           object              0                0.00\n",
            "CharContinuationRate         float64              0                0.00\n",
            "TLDLegitimateProb            float64              0                0.00\n",
            "URLCharProb                  float64              0                0.00\n",
            "TLDLength                    float64              0                0.00\n",
            "NoOfSubDomain                float64              0                0.00\n",
            "NoOfObfuscatedChar           float64              0                0.00\n",
            "IsHTTPS                      boolean              0                0.00\n",
            "ObfuscationRatio             float64              0                0.00\n",
            "NoOfLettersInURL             float64              0                0.00\n",
            "LetterRatioInURL             float64              0                0.00\n",
            "NoOfDegitsInURL              float64              0                0.00\n",
            "DegitRatioInURL              float64              0                0.00\n",
            "NoOfEqualsInURL              float64              0                0.00\n",
            "NoOfQMarkInURL               float64              0                0.00\n",
            "NoOfAmpersandInURL           float64              0                0.00\n",
            "NoOfOtherSpecialCharsInURL   float64              0                0.00\n",
            "SpacialCharRatioInURL        float64              0                0.00\n",
            "label                        boolean              0                0.00\n"
          ]
        }
      ],
      "source": [
        "missing_analysis = processor.analyze_missing_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "              Distribution Analysis               \n",
            "==================================================\n",
            "\n",
            "Feature Imputation Summary:\n",
            "\n",
            "Column: URLLength\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Imputation using Predictive Models\n",
            "    Reason: Strong correlations with NoOfLettersInURL, NoOfDegitsInURL\n",
            "    Top Correlations:\n",
            "        - NoOfLettersInURL: 0.943\n",
            "        - NoOfDegitsInURL: 0.811\n",
            "\n",
            "Column: DomainLength\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: CharContinuationRate\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Left, Near-Symmetric Distribution\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Left, Near-Symmetric Distribution\n",
            "\n",
            "Column: TLDLegitimateProb\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Left, Near-Symmetric Distribution\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Left, Near-Symmetric Distribution\n",
            "\n",
            "Column: URLCharProb\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), Moderate Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), Moderate Skewness\n",
            "\n",
            "Column: TLDLength\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Left, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Left, High Skewness\n",
            "\n",
            "Column: NoOfSubDomain\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: NoOfObfuscatedChar\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: ObfuscationRatio\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfLettersInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Imputation using Predictive Models\n",
            "    Reason: Strong correlations with URLLength\n",
            "    Top Correlations:\n",
            "        - URLLength: 0.943\n",
            "\n",
            "Column: LetterRatioInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), Near-Symmetric Distribution\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), Near-Symmetric Distribution\n",
            "\n",
            "Column: NoOfDegitsInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Imputation using Predictive Models\n",
            "    Reason: Strong correlations with URLLength\n",
            "    Top Correlations:\n",
            "        - URLLength: 0.811\n",
            "\n",
            "Column: DegitRatioInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfEqualsInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfQMarkInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfAmpersandInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfOtherSpecialCharsInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: SpacialCharRatioInURL\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, Near-Symmetric Distribution\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, Near-Symmetric Distribution\n",
            "\n",
            "Column: LineOfCode\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: LargestLineLength\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: DomainTitleMatchScore\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Left, Moderate Skewness\n",
            "    Imputation Strategy: Imputation using Predictive Models\n",
            "    Reason: Strong correlations with URLTitleMatchScore\n",
            "    Top Correlations:\n",
            "        - URLTitleMatchScore: 0.991\n",
            "\n",
            "Column: URLTitleMatchScore\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Left, Moderate Skewness\n",
            "    Imputation Strategy: Imputation using Predictive Models\n",
            "    Reason: Strong correlations with DomainTitleMatchScore\n",
            "    Top Correlations:\n",
            "        - DomainTitleMatchScore: 0.991\n",
            "\n",
            "Column: NoOfURLRedirect\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: NoOfSelfRedirect\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfPopup\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfiFrame\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: NoOfImage\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfCSS\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: NoOfJS\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfSelfRef\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfEmptyRef\n",
            "    Data Type: float64\n",
            "    Distribution: Relatively Symmetric (Normal Distribution), High Skewness\n",
            "    Imputation Strategy: Mean Imputation\n",
            "    Reason: Based on Relatively Symmetric (Normal Distribution), High Skewness\n",
            "\n",
            "Column: NoOfExternalRef\n",
            "    Data Type: float64\n",
            "    Distribution: Skewed to Right, High Skewness\n",
            "    Imputation Strategy: Median Imputation\n",
            "    Reason: Based on Skewed to Right, High Skewness\n",
            "\n",
            "Column: HasTitle\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasFavicon\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: Robots\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: IsResponsive\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasDescription\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasExternalFormSubmit\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasSocialNet\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasSubmitButton\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasHiddenFields\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasPasswordField\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: Bank\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: Pay\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: Crypto\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n",
            "\n",
            "Column: HasCopyrightInfo\n",
            "    Data Type: boolean\n",
            "    Imputation Strategy: Mode Imputation\n",
            "    Reason: Standard approach for categorical data\n"
          ]
        }
      ],
      "source": [
        "feature_analysis = processor.analyze_feature_distributions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "          ADVANCED IMPUTATION STRATEGIES          \n",
            "==================================================\n",
            "\n",
            "Memulai imputasi kolom boolean...\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasTitle:\n",
            "  - Jumlah nilai hilang: 24673\n",
            "  - Rasio True: 97.49%\n",
            "  - Strategi: majority_true\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasFavicon:\n",
            "  - Jumlah nilai hilang: 32397\n",
            "  - Rasio True: 53.35%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk Robots:\n",
            "  - Jumlah nilai hilang: 26135\n",
            "  - Rasio True: 38.90%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk IsResponsive:\n",
            "  - Jumlah nilai hilang: 23489\n",
            "  - Rasio True: 81.15%\n",
            "  - Strategi: majority_true\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasDescription:\n",
            "  - Jumlah nilai hilang: 30150\n",
            "  - Rasio True: 68.60%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasExternalFormSubmit:\n",
            "  - Jumlah nilai hilang: 30771\n",
            "  - Rasio True: 6.83%\n",
            "  - Strategi: majority_false\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasSocialNet:\n",
            "  - Jumlah nilai hilang: 37564\n",
            "  - Rasio True: 73.57%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasSubmitButton:\n",
            "  - Jumlah nilai hilang: 34078\n",
            "  - Rasio True: 61.47%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasHiddenFields:\n",
            "  - Jumlah nilai hilang: 24126\n",
            "  - Rasio True: 55.17%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasPasswordField:\n",
            "  - Jumlah nilai hilang: 36691\n",
            "  - Rasio True: 13.01%\n",
            "  - Strategi: majority_false\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk Bank:\n",
            "  - Jumlah nilai hilang: 30337\n",
            "  - Rasio True: 17.26%\n",
            "  - Strategi: majority_false\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk Pay:\n",
            "  - Jumlah nilai hilang: 23932\n",
            "  - Rasio True: 34.55%\n",
            "  - Strategi: mode\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk Crypto:\n",
            "  - Jumlah nilai hilang: 27847\n",
            "  - Rasio True: 3.44%\n",
            "  - Strategi: majority_false\n",
            "  - Nilai imputasi: False\n",
            "\n",
            "Ringkasan imputasi boolean untuk HasCopyrightInfo:\n",
            "  - Jumlah nilai hilang: 37158\n",
            "  - Rasio True: 75.06%\n",
            "  - Strategi: majority_true\n",
            "  - Nilai imputasi: True\n",
            "\n",
            "Handling column: URLLength (Data Type: float64)\n",
            "\n",
            "Handling column: DomainLength (Data Type: float64)\n",
            "\n",
            "Handling column: CharContinuationRate (Data Type: float64)\n",
            "\n",
            "Handling column: TLDLegitimateProb (Data Type: float64)\n",
            "\n",
            "Handling column: URLCharProb (Data Type: float64)\n",
            "\n",
            "Handling column: TLDLength (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfSubDomain (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfObfuscatedChar (Data Type: float64)\n",
            "\n",
            "Handling column: ObfuscationRatio (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfLettersInURL (Data Type: float64)\n",
            "\n",
            "Handling column: LetterRatioInURL (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfDegitsInURL (Data Type: float64)\n",
            "\n",
            "Handling column: DegitRatioInURL (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfEqualsInURL (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfQMarkInURL (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfAmpersandInURL (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfOtherSpecialCharsInURL (Data Type: float64)\n",
            "\n",
            "Handling column: SpacialCharRatioInURL (Data Type: float64)\n",
            "\n",
            "Handling column: LineOfCode (Data Type: float64)\n",
            "\n",
            "Handling column: LargestLineLength (Data Type: float64)\n",
            "\n",
            "Handling column: DomainTitleMatchScore (Data Type: float64)\n",
            "\n",
            "Handling column: URLTitleMatchScore (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfURLRedirect (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfSelfRedirect (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfPopup (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfiFrame (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfImage (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfCSS (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfJS (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfSelfRef (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfEmptyRef (Data Type: float64)\n",
            "\n",
            "Handling column: NoOfExternalRef (Data Type: float64)\n",
            "\n",
            "Imputation process completed successfully.\n",
            "Successfully imputed columns:\n",
            "['LineOfCode', 'LargestLineLength', 'DomainTitleMatchScore', 'URLTitleMatchScore', 'NoOfURLRedirect', 'NoOfSelfRedirect', 'NoOfPopup', 'NoOfiFrame', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef', 'NoOfExternalRef']\n"
          ]
        }
      ],
      "source": [
        "df_completed = processor.impute_missing_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Imputation Validation:\n",
            "                       Missing Before  Missing After  Values Imputed\n",
            "LineOfCode                      38231              0           38231\n",
            "LargestLineLength               37542              0           37542\n",
            "HasTitle                        24673              0           24673\n",
            "DomainTitleMatchScore           27558              0           27558\n",
            "URLTitleMatchScore              28859              0           28859\n",
            "HasFavicon                      32397              0           32397\n",
            "Robots                          26135              0           26135\n",
            "IsResponsive                    23489              0           23489\n",
            "NoOfURLRedirect                 37141              0           37141\n",
            "NoOfSelfRedirect                36924              0           36924\n",
            "HasDescription                  30150              0           30150\n",
            "NoOfPopup                       23797              0           23797\n",
            "NoOfiFrame                      27473              0           27473\n",
            "HasExternalFormSubmit           30771              0           30771\n",
            "HasSocialNet                    37564              0           37564\n",
            "HasSubmitButton                 34078              0           34078\n",
            "HasHiddenFields                 24126              0           24126\n",
            "HasPasswordField                36691              0           36691\n",
            "Bank                            30337              0           30337\n",
            "Pay                             23932              0           23932\n",
            "Crypto                          27847              0           27847\n",
            "HasCopyrightInfo                37158              0           37158\n",
            "NoOfImage                       27811              0           27811\n",
            "NoOfCSS                         36991              0           36991\n",
            "NoOfJS                          33492              0           33492\n",
            "NoOfSelfRef                     26558              0           26558\n",
            "NoOfEmptyRef                    23616              0           23616\n",
            "NoOfExternalRef                 38400              0           38400\n"
          ]
        }
      ],
      "source": [
        "processor._validate_imputation(derived_url_data, df_completed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Hasil imputasi lengkap disimpan di 'completed_data.csv'\n"
          ]
        }
      ],
      "source": [
        "df_completed.to_csv(\"completed_data.csv\", index=False)\n",
        "print(\"\\nHasil imputasi lengkap disimpan di 'completed_data.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgrSMcK75VY_"
      },
      "source": [
        "### II. Dealing with Outliers\n",
        "\n",
        "Outliers are data points that significantly differ from the majority of the data. They can be unusually high or low values that do not fit the pattern of the rest of the dataset. Outliers can significantly impact model performance, so it is important to handle them properly.\n",
        "\n",
        "Some methods to handle outliers:\n",
        "1. **Imputation**: Replace with mean, median, or a boundary value.\n",
        "2. **Clipping**: Cap values to upper and lower limits.\n",
        "3. **Transformation**: Use log, square root, or power transformations to reduce their influence.\n",
        "4. **Model-Based**: Use algorithms robust to outliers (e.g., tree-based models, Huber regression)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "CgbZ6Lv17Uf0"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO0ZEZ-s6Lu-"
      },
      "source": [
        "### III. Remove Duplicates\n",
        "Handling duplicate values is crucial because they can compromise data integrity, leading to inaccurate analysis and insights. Duplicate entries can bias machine learning models, causing overfitting and reducing their ability to generalize to new data. They also inflate the dataset size unnecessarily, increasing computational costs and processing times. Additionally, duplicates can distort statistical measures and lead to inconsistencies, ultimately affecting the reliability of data-driven decisions and reporting. Ensuring data quality by removing duplicates is essential for accurate, efficient, and consistent analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "BHCkkZ-v7iF8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['http://test-mantenimiento-bancaweb.azurewebsites.net/'], dtype='object', name='URL')\n"
          ]
        }
      ],
      "source": [
        "duplicate_urls = df_completed['URL'].value_counts()\n",
        "duplicate_urls = duplicate_urls[duplicate_urls > 1].index\n",
        "print(duplicate_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           id                                                URL  URLLength  \\\n",
            "16778  122751  http://test-mantenimiento-bancaweb.azurewebsit...       53.0   \n",
            "21282  180616  http://test-mantenimiento-bancaweb.azurewebsit...       53.0   \n",
            "\n",
            "                                              Domain  DomainLength  \\\n",
            "16778  test-mantenimiento-bancaweb.azurewebsites.net          45.0   \n",
            "21282  test-mantenimiento-bancaweb.azurewebsites.net          45.0   \n",
            "\n",
            "       IsDomainIP  TLD  CharContinuationRate  TLDLegitimateProb  URLCharProb  \\\n",
            "16778       False  net              0.320755           0.029014     0.061098   \n",
            "21282       False  net              0.341463           0.029014     0.061098   \n",
            "\n",
            "       ...    Pay  Crypto  HasCopyrightInfo  NoOfImage  NoOfCSS     NoOfJS  \\\n",
            "16778  ...  False   False              True  41.483831      1.0  16.370174   \n",
            "21282  ...  False   False              True   0.000000      5.0   1.000000   \n",
            "\n",
            "       NoOfSelfRef  NoOfEmptyRef  NoOfExternalRef  label  \n",
            "16778          1.0           0.0             38.0  False  \n",
            "21282          1.0           0.0             38.0  False  \n",
            "\n",
            "[2 rows x 55 columns]\n"
          ]
        }
      ],
      "source": [
        "rows_with_duplicate_urls = df_completed[df_completed['URL'].isin(duplicate_urls)]\n",
        "print(rows_with_duplicate_urls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            id                                                URL  URLLength  \\\n",
            "73353   122751  http://test-mantenimiento-bancaweb.azurewebsit...       53.0   \n",
            "107606  180616  http://test-mantenimiento-bancaweb.azurewebsit...        NaN   \n",
            "\n",
            "                                               Domain  DomainLength  \\\n",
            "73353                                             NaN          45.0   \n",
            "107606  test-mantenimiento-bancaweb.azurewebsites.net          45.0   \n",
            "\n",
            "        IsDomainIP  TLD  CharContinuationRate  TLDLegitimateProb  URLCharProb  \\\n",
            "73353          0.0  net                   NaN                NaN     0.061098   \n",
            "107606         0.0  NaN              0.341463                NaN     0.061098   \n",
            "\n",
            "        ...  Pay  Crypto  HasCopyrightInfo  NoOfImage  NoOfCSS  NoOfJS  \\\n",
            "73353   ...  NaN     NaN               1.0        NaN      1.0     NaN   \n",
            "107606  ...  NaN     0.0               1.0        0.0      NaN     1.0   \n",
            "\n",
            "        NoOfSelfRef  NoOfEmptyRef  NoOfExternalRef  label  \n",
            "73353           1.0           0.0              NaN      0  \n",
            "107606          1.0           0.0              NaN      0  \n",
            "\n",
            "[2 rows x 55 columns]\n"
          ]
        }
      ],
      "source": [
        "rows_with_duplicate_urls = df[df['URL'].isin(duplicate_urls)]\n",
        "print(rows_with_duplicate_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           id                                                URL  URLLength  \\\n",
            "16778  122751  http://test-mantenimiento-bancaweb.azurewebsit...       53.0   \n",
            "21282  180616  http://test-mantenimiento-bancaweb.azurewebsit...       53.0   \n",
            "\n",
            "                                              Domain  DomainLength  \\\n",
            "16778  test-mantenimiento-bancaweb.azurewebsites.net          45.0   \n",
            "21282  test-mantenimiento-bancaweb.azurewebsites.net          45.0   \n",
            "\n",
            "       IsDomainIP  TLD  CharContinuationRate  TLDLegitimateProb  URLCharProb  \\\n",
            "16778       False  net              0.320755           0.029014     0.061098   \n",
            "21282       False  net              0.341463           0.029014     0.061098   \n",
            "\n",
            "       ...    Pay  Crypto  HasCopyrightInfo  NoOfImage  NoOfCSS     NoOfJS  \\\n",
            "16778  ...  False   False              True  41.483831      1.0  16.370174   \n",
            "21282  ...  False   False              True   0.000000      5.0   1.000000   \n",
            "\n",
            "       NoOfSelfRef  NoOfEmptyRef  NoOfExternalRef  label  \n",
            "16778          1.0           0.0             38.0  False  \n",
            "21282          1.0           0.0             38.0  False  \n",
            "\n",
            "[2 rows x 55 columns]\n"
          ]
        }
      ],
      "source": [
        "rows_with_duplicate_urls = df_completed[df_completed['URL'].isin(duplicate_urls)]\n",
        "print(rows_with_duplicate_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_duplicate_urls(df_completed: pd.DataFrame, df_original: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merge rows with duplicate URLs in `df_completed` by prioritizing data from `df_original`.\n",
        "\n",
        "    Args:\n",
        "        df_completed: The processed DataFrame with imputed values.\n",
        "        df_original: The original DataFrame with raw data.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with duplicate URLs merged, prioritizing data from the original DataFrame.\n",
        "    \"\"\"\n",
        "    # Identify duplicate URLs\n",
        "    duplicate_urls = df_completed[df_completed.duplicated(subset=['URL'], keep=False)]\n",
        "    unique_urls = duplicate_urls['URL'].unique()\n",
        "\n",
        "    # Initialize a list to store merged rows\n",
        "    merged_rows = []\n",
        "\n",
        "    for url in unique_urls:\n",
        "        # Extract rows with the same URL from both DataFrames\n",
        "        rows_original = df_original[df_original['URL'] == url]\n",
        "        rows_completed = df_completed[df_completed['URL'] == url]\n",
        "\n",
        "        # Start with the row from the original DataFrame if available\n",
        "        if not rows_original.empty:\n",
        "            merged_row = rows_original.iloc[0].copy()\n",
        "        else:\n",
        "            merged_row = rows_completed.iloc[0].copy()\n",
        "\n",
        "        # Fill missing values in the merged row with data from the completed DataFrame\n",
        "        for col in merged_row.index:\n",
        "            if pd.isna(merged_row[col]) and col in rows_completed.columns:\n",
        "                merged_row[col] = rows_completed.iloc[0][col]\n",
        "\n",
        "        # Append the merged row to the list\n",
        "        merged_rows.append(merged_row)\n",
        "\n",
        "    # Convert the list of merged rows into a DataFrame\n",
        "    merged_df = pd.DataFrame(merged_rows)\n",
        "\n",
        "    # Combine the merged rows with non-duplicate rows from df_completed\n",
        "    non_duplicate_rows = df_completed[~df_completed['URL'].isin(unique_urls)]\n",
        "    result_df = pd.concat([non_duplicate_rows, merged_df], ignore_index=True)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           id                                                URL  URLLength  \\\n",
            "0      226786                          https://www.tetleyusa.com       24.0   \n",
            "1      163910                 https://www.atlanticwhiteshark.org       33.0   \n",
            "2      113381  https://apinchek.protationsacc.eu.org/aprep%d0...       59.0   \n",
            "3      163038                          https://www.malankara.com       24.0   \n",
            "4      117472                               https://www.rapp.com       19.0   \n",
            "...       ...                                                ...        ...   \n",
            "77622  159262                    https://www.physicianonfire.com       31.0   \n",
            "77623  118490     https://www.golfe-saint-tropez-information.com       45.0   \n",
            "77624   77235                          https://www.zappallas.com       25.0   \n",
            "77625  128558                           https://www.swoonish.com       24.0   \n",
            "77626  122751  http://test-mantenimiento-bancaweb.azurewebsit...       53.0   \n",
            "\n",
            "                                              Domain  DomainLength  \\\n",
            "0                                  www.tetleyusa.com          17.0   \n",
            "1                         www.atlanticwhiteshark.org          26.0   \n",
            "2                      apinchek.protationsacc.eu.org          29.0   \n",
            "3                                  www.malankara.com          17.0   \n",
            "4                                       www.rapp.com          12.0   \n",
            "...                                              ...           ...   \n",
            "77622                        www.physicianonfire.com          23.0   \n",
            "77623         www.golfe-saint-tropez-information.com          38.0   \n",
            "77624                              www.zappallas.com          17.0   \n",
            "77625                               www.swoonish.com          16.0   \n",
            "77626  test-mantenimiento-bancaweb.azurewebsites.net          45.0   \n",
            "\n",
            "       IsDomainIP  TLD  CharContinuationRate  TLDLegitimateProb  URLCharProb  \\\n",
            "0           False  com              0.520000           0.522907     0.052648   \n",
            "1           False  org              1.000000           0.123977     0.057792   \n",
            "2           False  org              0.560000           0.079963     0.057663   \n",
            "3           False  com              0.520000           0.522907     0.067413   \n",
            "4           False  com              1.000000           0.510163     0.061459   \n",
            "...           ...  ...                   ...                ...          ...   \n",
            "77622       False  com              0.612903           0.522907     0.060076   \n",
            "77623       False  com              0.400000           0.522907     0.062988   \n",
            "77624       False  com              0.520000           0.522907     0.046241   \n",
            "77625       False  com              1.000000           0.522907     0.062526   \n",
            "77626       False  net              0.320755           0.029014     0.061098   \n",
            "\n",
            "       ...    Pay  Crypto  HasCopyrightInfo  NoOfImage  NoOfCSS     NoOfJS  \\\n",
            "0      ...  False   False              True  41.483831      5.0   7.000000   \n",
            "1      ...   True   False             False  41.483831      4.0  16.370174   \n",
            "2      ...  False   False             False   0.000000      0.0   0.000000   \n",
            "3      ...   True   False              True  41.483831      5.0  16.370174   \n",
            "4      ...   True   False              True  25.000000      1.0  16.370174   \n",
            "...    ...    ...     ...               ...        ...      ...        ...   \n",
            "77622  ...   True   False              True  48.000000      5.0  16.370174   \n",
            "77623  ...  False   False              True  15.000000      8.0  16.000000   \n",
            "77624  ...  False   False              True  41.483831      3.0  16.370174   \n",
            "77625  ...  False   False              True   0.000000     12.0  17.000000   \n",
            "77626  ...  False   False              True  41.483831      1.0  16.370174   \n",
            "\n",
            "       NoOfSelfRef  NoOfEmptyRef  NoOfExternalRef  label  \n",
            "0       103.590836      1.000000             38.0   True  \n",
            "1       105.000000      0.000000             27.0   True  \n",
            "2         0.000000      3.798785              1.0  False  \n",
            "3       232.000000      3.798785             35.0   True  \n",
            "4        33.000000      3.798785              4.0   True  \n",
            "...            ...           ...              ...    ...  \n",
            "77622   172.000000     13.000000             38.0   True  \n",
            "77623   103.590836      3.798785             38.0   True  \n",
            "77624   103.590836      0.000000             38.0   True  \n",
            "77625    74.000000      0.000000            112.0   True  \n",
            "77626     1.000000      0.000000             38.0  False  \n",
            "\n",
            "[77627 rows x 55 columns]\n"
          ]
        }
      ],
      "source": [
        "merged_df = merge_duplicate_urls(df_completed, derived_url_data)\n",
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           id                                                URL  URLLength  \\\n",
            "77626  122751  http://test-mantenimiento-bancaweb.azurewebsit...       53.0   \n",
            "\n",
            "                                              Domain  DomainLength  \\\n",
            "77626  test-mantenimiento-bancaweb.azurewebsites.net          45.0   \n",
            "\n",
            "       IsDomainIP  TLD  CharContinuationRate  TLDLegitimateProb  URLCharProb  \\\n",
            "77626       False  net              0.320755           0.029014     0.061098   \n",
            "\n",
            "       ...    Pay  Crypto  HasCopyrightInfo  NoOfImage  NoOfCSS     NoOfJS  \\\n",
            "77626  ...  False   False              True  41.483831      1.0  16.370174   \n",
            "\n",
            "       NoOfSelfRef  NoOfEmptyRef  NoOfExternalRef  label  \n",
            "77626          1.0           0.0             38.0  False  \n",
            "\n",
            "[1 rows x 55 columns]\n"
          ]
        }
      ],
      "source": [
        "rows_with_duplicate_urls = merged_df[merged_df['URL'].isin(duplicate_urls)]\n",
        "print(rows_with_duplicate_urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eycPASmMLfHa"
      },
      "source": [
        "### IV. Feature Engineering\n",
        "\n",
        "**Feature engineering** involves creating new features (input variables) or transforming existing ones to improve the performance of machine learning models. Feature engineering aims to enhance the model's ability to learn patterns and make accurate predictions from the data. It's often said that \"good features make good models.\"\n",
        "\n",
        "1. **Feature Selection:** Feature engineering can involve selecting the most relevant and informative features from the dataset. Removing irrelevant or redundant features not only simplifies the model but also reduces the risk of overfitting.\n",
        "\n",
        "2. **Creating New Features:** Sometimes, the existing features may not capture the underlying patterns effectively. In such cases, engineers create new features that provide additional information. For example:\n",
        "   \n",
        "   - **Polynomial Features:** Engineers may create new features by taking the square, cube, or other higher-order terms of existing numerical features. This can help capture nonlinear relationships.\n",
        "   \n",
        "   - **Interaction Features:** Interaction features are created by combining two or more existing features. For example, if you have features \"length\" and \"width,\" you can create an \"area\" feature by multiplying them.\n",
        "\n",
        "3. **Binning or Discretization:** Continuous numerical features can be divided into bins or categories. For instance, age values can be grouped into bins like \"child,\" \"adult,\" and \"senior.\"\n",
        "\n",
        "4. **Domain-Specific Feature Engineering:** Depending on the domain and problem, engineers may create domain-specific features. For example, in fraud detection, features related to transaction history and user behavior may be engineered to identify anomalies.\n",
        "\n",
        "Feature engineering is both a creative and iterative process. It requires a deep understanding of the data, domain knowledge, and experimentation to determine which features will enhance the model's predictive power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "UoXEV6wkLfHa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 77627 entries, 0 to 77626\n",
            "Data columns (total 55 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   id                          77627 non-null  int64  \n",
            " 1   URL                         77627 non-null  object \n",
            " 2   URLLength                   77627 non-null  float64\n",
            " 3   Domain                      77627 non-null  object \n",
            " 4   DomainLength                77627 non-null  float64\n",
            " 5   IsDomainIP                  77627 non-null  boolean\n",
            " 6   TLD                         77627 non-null  object \n",
            " 7   CharContinuationRate        77627 non-null  float64\n",
            " 8   TLDLegitimateProb           77627 non-null  float64\n",
            " 9   URLCharProb                 77627 non-null  float64\n",
            " 10  TLDLength                   77627 non-null  float64\n",
            " 11  NoOfSubDomain               77627 non-null  float64\n",
            " 12  HasObfuscation              77627 non-null  boolean\n",
            " 13  NoOfObfuscatedChar          77627 non-null  float64\n",
            " 14  ObfuscationRatio            77627 non-null  float64\n",
            " 15  NoOfLettersInURL            77627 non-null  float64\n",
            " 16  LetterRatioInURL            77627 non-null  float64\n",
            " 17  NoOfDegitsInURL             77627 non-null  float64\n",
            " 18  DegitRatioInURL             77627 non-null  float64\n",
            " 19  NoOfEqualsInURL             77627 non-null  float64\n",
            " 20  NoOfQMarkInURL              77627 non-null  float64\n",
            " 21  NoOfAmpersandInURL          77627 non-null  float64\n",
            " 22  NoOfOtherSpecialCharsInURL  77627 non-null  float64\n",
            " 23  SpacialCharRatioInURL       77627 non-null  float64\n",
            " 24  IsHTTPS                     77627 non-null  boolean\n",
            " 25  LineOfCode                  77627 non-null  float64\n",
            " 26  LargestLineLength           77627 non-null  float64\n",
            " 27  HasTitle                    77627 non-null  boolean\n",
            " 28  Title                       45369 non-null  object \n",
            " 29  DomainTitleMatchScore       77627 non-null  float64\n",
            " 30  URLTitleMatchScore          77627 non-null  float64\n",
            " 31  HasFavicon                  77627 non-null  boolean\n",
            " 32  Robots                      77627 non-null  boolean\n",
            " 33  IsResponsive                77627 non-null  boolean\n",
            " 34  NoOfURLRedirect             77627 non-null  float64\n",
            " 35  NoOfSelfRedirect            77627 non-null  float64\n",
            " 36  HasDescription              77627 non-null  boolean\n",
            " 37  NoOfPopup                   77627 non-null  float64\n",
            " 38  NoOfiFrame                  77627 non-null  float64\n",
            " 39  HasExternalFormSubmit       77627 non-null  boolean\n",
            " 40  HasSocialNet                77627 non-null  boolean\n",
            " 41  HasSubmitButton             77627 non-null  boolean\n",
            " 42  HasHiddenFields             77627 non-null  boolean\n",
            " 43  HasPasswordField            77627 non-null  boolean\n",
            " 44  Bank                        77627 non-null  boolean\n",
            " 45  Pay                         77627 non-null  boolean\n",
            " 46  Crypto                      77627 non-null  boolean\n",
            " 47  HasCopyrightInfo            77627 non-null  boolean\n",
            " 48  NoOfImage                   77627 non-null  float64\n",
            " 49  NoOfCSS                     77627 non-null  float64\n",
            " 50  NoOfJS                      77627 non-null  float64\n",
            " 51  NoOfSelfRef                 77627 non-null  float64\n",
            " 52  NoOfEmptyRef                77627 non-null  float64\n",
            " 53  NoOfExternalRef             77627 non-null  float64\n",
            " 54  label                       77627 non-null  boolean\n",
            "dtypes: boolean(18), float64(32), int64(1), object(4)\n",
            "memory usage: 24.6+ MB\n"
          ]
        }
      ],
      "source": [
        "merged_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def feature_selection(df, target_column, variance_threshold=0.1, correlation_threshold=0.75, max_categories=50):\n",
        "    \"\"\"\n",
        "    Performs robust feature selection with detailed logging.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe\n",
        "        target_column (str): Name of the target column\n",
        "        variance_threshold (float): Minimum variance threshold for numeric features\n",
        "        correlation_threshold (float): Maximum correlation threshold for numeric features\n",
        "        max_categories (int): Maximum number of unique categories to process\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing selected features and details of dropped features with reasons\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid modifying original data\n",
        "    df = df.copy()\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_features = X.select_dtypes(include=['object', 'category', 'bool', 'boolean']).columns\n",
        "\n",
        "    selected_features = []\n",
        "    dropped_features = []\n",
        "\n",
        "    # Process numeric features\n",
        "    if len(numeric_features) > 0:\n",
        "        # Variance threshold\n",
        "        selector = VarianceThreshold(threshold=variance_threshold)\n",
        "        selector.fit(X[numeric_features])\n",
        "        variance_selected = numeric_features[selector.get_support()]\n",
        "\n",
        "        dropped_due_to_variance = list(set(numeric_features) - set(variance_selected))\n",
        "        dropped_features.extend([(col, 'Low Variance') for col in dropped_due_to_variance])\n",
        "\n",
        "        if len(variance_selected) > 0:\n",
        "            # Correlation with target\n",
        "            target_corr = X[variance_selected].corrwith(y).abs()\n",
        "            high_corr_with_target = target_corr[target_corr >= correlation_threshold].index.tolist()\n",
        "\n",
        "            dropped_due_to_low_target_corr = list(set(variance_selected) - set(high_corr_with_target))\n",
        "            dropped_features.extend([(col, 'Low Correlation with Target') for col in dropped_due_to_low_target_corr])\n",
        "\n",
        "            # Correlation analysis among selected features\n",
        "            X_high_corr = X[high_corr_with_target]\n",
        "            corr_matrix = X_high_corr.corr()\n",
        "\n",
        "            drop_features = set()\n",
        "            for i in range(len(corr_matrix.columns)):\n",
        "                if corr_matrix.columns[i] not in drop_features:\n",
        "                    for j in range(i + 1, len(corr_matrix.columns)):\n",
        "                        if abs(corr_matrix.iloc[i, j]) > correlation_threshold:\n",
        "                            drop_features.add(corr_matrix.columns[j])\n",
        "\n",
        "            correlation_selected = [f for f in high_corr_with_target if f not in drop_features]\n",
        "\n",
        "            dropped_due_to_feature_corr = list(set(high_corr_with_target) - set(correlation_selected))\n",
        "            dropped_features.extend([(col, 'High Correlation with Other Features') for col in dropped_due_to_feature_corr])\n",
        "\n",
        "            if len(correlation_selected) > 0:\n",
        "                # Mutual information\n",
        "                mi_scores = mutual_info_classif(X[correlation_selected], y, random_state=42)\n",
        "                mi_selected = [feature for score, feature in \n",
        "                               zip(mi_scores, correlation_selected) if score > 0.01]\n",
        "\n",
        "                dropped_due_to_low_mi = list(set(correlation_selected) - set(mi_selected))\n",
        "                dropped_features.extend([(col, 'Low Mutual Information') for col in dropped_due_to_low_mi])\n",
        "\n",
        "                selected_features.extend(mi_selected)\n",
        "\n",
        "    # Process categorical features\n",
        "    if len(categorical_features) > 0:\n",
        "        for feature in categorical_features:\n",
        "            # Skip if too many unique categories\n",
        "            if X[feature].nunique() > max_categories:\n",
        "                dropped_features.append((feature, 'Too Many Unique Categories'))\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Convert boolean to int for contingency table\n",
        "                if X[feature].dtype in ['bool', 'boolean']:\n",
        "                    contingency = pd.crosstab(X[feature].astype(int), y)\n",
        "                else:\n",
        "                    contingency = pd.crosstab(X[feature], y)\n",
        "\n",
        "                chi2, p_value, _, _ = chi2_contingency(contingency)\n",
        "\n",
        "                # Select feature if statistically significant\n",
        "                if p_value < 0.05:\n",
        "                    selected_features.append(feature)\n",
        "                else:\n",
        "                    dropped_features.append((feature, 'High p-value in Chi-Square Test'))\n",
        "            except (ValueError, MemoryError):\n",
        "                dropped_features.append((feature, 'Error in Processing'))\n",
        "                continue\n",
        "\n",
        "    return {\n",
        "        'selected_features': selected_features,\n",
        "        'dropped_features': dropped_features\n",
        "    }\n",
        "\n",
        "def print_feature_importance(df, selected_features, target_column):\n",
        "    \"\"\"\n",
        "    Prints importance metrics for selected features.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe\n",
        "        selected_features (list): List of selected feature names\n",
        "        target_column (str): Name of the target column\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    print(\"\\nFeature Importance Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    feature_importance = []\n",
        "    for feature in selected_features:\n",
        "        if df[feature].dtype in ['int64', 'float64']:\n",
        "            mi_score = mutual_info_classif(df[[feature]], df[target_column], random_state=42)[0]\n",
        "            feature_importance.append((feature, mi_score, 'MI'))\n",
        "            print(f\"{feature:30} MI Score: {mi_score:.4f}\")\n",
        "        else:\n",
        "            if df[feature].dtype in ['bool', 'boolean']:\n",
        "                contingency = pd.crosstab(df[feature].astype(int), df[target_column])\n",
        "            else:\n",
        "                contingency = pd.crosstab(df[feature], df[target_column])\n",
        "            chi2, p_value, _, _ = chi2_contingency(contingency)\n",
        "            feature_importance.append((feature, chi2, 'Chi2'))\n",
        "            print(f\"{feature:30} Chi2 Score: {chi2:.4f} (p={p_value:.4e})\")\n",
        "\n",
        "    return sorted(feature_importance, key=lambda x: x[1], reverse=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_polynomial_features(df, selected_features, degree=2):\n",
        "    \"\"\"\n",
        "    Generate polynomial features for selected numerical features, with fixes for NaN and non-numeric data.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame\n",
        "        selected_features (list): List of selected numerical features\n",
        "        degree (int): The degree of polynomial features to generate\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with added polynomial features\n",
        "    \"\"\"\n",
        "    from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "    # Filter numerical features\n",
        "    numerical_features = df[selected_features].select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "    # Handle missing values by imputing them with the mean\n",
        "    numerical_features = numerical_features.fillna(numerical_features.mean())\n",
        "\n",
        "    # Generate polynomial features\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    poly_features = poly.fit_transform(numerical_features)\n",
        "\n",
        "    # Get the names of polynomial features\n",
        "    poly_feature_names = poly.get_feature_names_out(numerical_features.columns)\n",
        "\n",
        "    # Create a DataFrame with new features\n",
        "    poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df.index)\n",
        "\n",
        "    # Combine the original DataFrame with the polynomial features\n",
        "    return pd.concat([df, poly_df], axis=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_interaction_features(df, selected_features):\n",
        "    \"\"\"\n",
        "    Generate interaction features between selected numerical features.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame\n",
        "        selected_features (list): List of selected numerical features\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with added interaction features\n",
        "    \"\"\"\n",
        "    interaction_features = {}\n",
        "    for i, feature1 in enumerate(selected_features):\n",
        "        for feature2 in selected_features[i+1:]:\n",
        "            interaction_name = f\"{feature1}_x_{feature2}\"\n",
        "            interaction_features[interaction_name] = df[feature1] * df[feature2]\n",
        "    \n",
        "    interaction_df = pd.DataFrame(interaction_features)\n",
        "    return pd.concat([df, interaction_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleksi fitur\n",
        "target_column = 'label'\n",
        "feature_selection_result = feature_selection(merged_df, target_column)\n",
        "selected_features = feature_selection_result['selected_features']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id                                0\n",
            "URL                               0\n",
            "URLLength                         0\n",
            "Domain                            0\n",
            "DomainLength                      0\n",
            "IsDomainIP                        0\n",
            "TLD                               0\n",
            "CharContinuationRate              0\n",
            "TLDLegitimateProb                 0\n",
            "URLCharProb                       0\n",
            "TLDLength                         0\n",
            "NoOfSubDomain                     0\n",
            "HasObfuscation                    0\n",
            "NoOfObfuscatedChar                0\n",
            "ObfuscationRatio                  0\n",
            "NoOfLettersInURL                  0\n",
            "LetterRatioInURL                  0\n",
            "NoOfDegitsInURL                   0\n",
            "DegitRatioInURL                   0\n",
            "NoOfEqualsInURL                   0\n",
            "NoOfQMarkInURL                    0\n",
            "NoOfAmpersandInURL                0\n",
            "NoOfOtherSpecialCharsInURL        0\n",
            "SpacialCharRatioInURL             0\n",
            "IsHTTPS                           0\n",
            "LineOfCode                        0\n",
            "LargestLineLength                 0\n",
            "HasTitle                          0\n",
            "Title                         32258\n",
            "DomainTitleMatchScore             0\n",
            "URLTitleMatchScore                0\n",
            "HasFavicon                        0\n",
            "Robots                            0\n",
            "IsResponsive                      0\n",
            "NoOfURLRedirect                   0\n",
            "NoOfSelfRedirect                  0\n",
            "HasDescription                    0\n",
            "NoOfPopup                         0\n",
            "NoOfiFrame                        0\n",
            "HasExternalFormSubmit             0\n",
            "HasSocialNet                      0\n",
            "HasSubmitButton                   0\n",
            "HasHiddenFields                   0\n",
            "HasPasswordField                  0\n",
            "Bank                              0\n",
            "Pay                               0\n",
            "Crypto                            0\n",
            "HasCopyrightInfo                  0\n",
            "NoOfImage                         0\n",
            "NoOfCSS                           0\n",
            "NoOfJS                            0\n",
            "NoOfSelfRef                       0\n",
            "NoOfEmptyRef                      0\n",
            "NoOfExternalRef                   0\n",
            "label                             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(merged_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "at least one array or dtype is required",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20276\\383147363.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_with_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_polynomial_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20276\\368275081.py\u001b[0m in \u001b[0;36mgenerate_polynomial_features\u001b[1;34m(df, selected_features, degree)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Generate polynomial features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mpoly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPolynomialFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mpoly_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumerical_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Get the names of polynomial features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 852\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    853\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \"\"\"\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 665\u001b[1;33m             \u001b[0mdtype_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: at least one array or dtype is required"
          ]
        }
      ],
      "source": [
        "df_with_poly = generate_polynomial_features(merged_df, selected_features, degree=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_with_interactions = generate_interaction_features(df_with_poly, selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Dataset after adding features: {df_with_interactions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw11_49xLfHb"
      },
      "source": [
        "## B. Data Preprocessing\n",
        "\n",
        "**Data preprocessing** is a broader step that encompasses both data cleaning and additional transformations to make the data suitable for machine learning algorithms. Its primary goals are:\n",
        "\n",
        "1. **Feature Scaling:** Ensure that numerical features have similar scales. Common techniques include Min-Max scaling (scaling to a specific range) or standardization (mean-centered, unit variance).\n",
        "\n",
        "2. **Encoding Categorical Variables:** Machine learning models typically work with numerical data, so categorical variables need to be encoded. This can be done using one-hot encoding, label encoding, or more advanced methods like target encoding.\n",
        "\n",
        "3. **Handling Imbalanced Classes:** If dealing with imbalanced classes in a binary classification task, apply techniques such as oversampling, undersampling, or using different evaluation metrics to address class imbalance.\n",
        "\n",
        "4. **Dimensionality Reduction:** Reduce the number of features using techniques like Principal Component Analysis (PCA) or feature selection to simplify the model and potentially improve its performance.\n",
        "\n",
        "5. **Normalization:** Normalize data to achieve a standard distribution. This is particularly important for algorithms that assume normally distributed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVyVnA1hLfHd"
      },
      "source": [
        "### Notes on Preprocessing processes\n",
        "\n",
        "It is advised to create functions or classes that have the same/similar type of inputs and outputs, so you can add, remove, or swap the order of the processes easily. You can implement the functions or classes by yourself\n",
        "\n",
        "or\n",
        "\n",
        "use `sklearn` library. To create a new preprocessing component in `sklearn`, implement a corresponding class that includes:\n",
        "1. Inheritance to `BaseEstimator` and `TransformerMixin`\n",
        "2. The method `fit`\n",
        "3. The method `transform`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WbxHt-5eKz_I"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "\n",
        "# from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# class FeatureEncoder(BaseEstimator, TransformerMixin):\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "\n",
        "#         # Fit the encoder here\n",
        "\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         X_encoded = X.copy()\n",
        "\n",
        "#         # Encode the categorical variables here\n",
        "\n",
        "#         return X_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhCgOl4xLfHb"
      },
      "source": [
        "### I. Feature Scaling\n",
        "\n",
        "**Feature scaling** is a preprocessing technique used in machine learning to standardize the range of independent variables or features of data. The primary goal of feature scaling is to ensure that all features contribute equally to the training process and that machine learning algorithms can work effectively with the data.\n",
        "\n",
        "Here are the main reasons why feature scaling is important:\n",
        "\n",
        "1. **Algorithm Sensitivity:** Many machine learning algorithms are sensitive to the scale of input features. If the scales of features are significantly different, some algorithms may perform poorly or take much longer to converge.\n",
        "\n",
        "2. **Distance-Based Algorithms:** Algorithms that rely on distances or similarities between data points, such as k-nearest neighbors (KNN) and support vector machines (SVM), can be influenced by feature scales. Features with larger scales may dominate the distance calculations.\n",
        "\n",
        "3. **Regularization:** Regularization techniques, like L1 (Lasso) and L2 (Ridge) regularization, add penalty terms based on feature coefficients. Scaling ensures that all features are treated equally in the regularization process.\n",
        "\n",
        "Common methods for feature scaling include:\n",
        "\n",
        "1. **Min-Max Scaling (Normalization):** This method scales features to a specific range, typically [0, 1]. It's done using the following formula:\n",
        "\n",
        "   $$X' = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
        "\n",
        "   - Here, $X$ is the original feature value, $X_{min}$ is the minimum value of the feature, and $X_{max}$ is the maximum value of the feature.  \n",
        "<br />\n",
        "<br />\n",
        "2. **Standardization (Z-score Scaling):** This method scales features to have a mean (average) of 0 and a standard deviation of 1. It's done using the following formula:\n",
        "\n",
        "   $$X' = \\frac{X - \\mu}{\\sigma}$$\n",
        "\n",
        "   - $X$ is the original feature value, $\\mu$ is the mean of the feature, and $\\sigma$ is the standard deviation of the feature.  \n",
        "<br />\n",
        "<br />\n",
        "3. **Robust Scaling:** Robust scaling is a method that scales features to the interquartile range (IQR) and is less affected by outliers. It's calculated as:\n",
        "\n",
        "   $$X' = \\frac{X - Q1}{Q3 - Q1}$$\n",
        "\n",
        "   - $X$ is the original feature value, $Q1$ is the first quartile (25th percentile), and $Q3$ is the third quartile (75th percentile) of the feature.  \n",
        "<br />\n",
        "<br />\n",
        "4. **Log Transformation:** In cases where data is highly skewed or has a heavy-tailed distribution, taking the logarithm of the feature values can help stabilize the variance and improve scaling.\n",
        "\n",
        "The choice of scaling method depends on the characteristics of your data and the requirements of your machine learning algorithm. **Min-max scaling and standardization are the most commonly used techniques and work well for many datasets.**\n",
        "\n",
        "Scaling should be applied separately to each training and test set to prevent data leakage from the test set into the training set. Additionally, **some algorithms may not require feature scaling, particularly tree-based models.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "COef9EbCLfHb"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Lh-4JwLfHc"
      },
      "source": [
        "### II. Feature Encoding\n",
        "\n",
        "**Feature encoding**, also known as **categorical encoding**, is the process of converting categorical data (non-numeric data) into a numerical format so that it can be used as input for machine learning algorithms. Most machine learning models require numerical data for training and prediction, so feature encoding is a critical step in data preprocessing.\n",
        "\n",
        "Categorical data can take various forms, including:\n",
        "\n",
        "1. **Nominal Data:** Categories with no intrinsic order, like colors or country names.  \n",
        "\n",
        "2. **Ordinal Data:** Categories with a meaningful order but not necessarily equidistant, like education levels (e.g., \"high school,\" \"bachelor's,\" \"master's\").\n",
        "\n",
        "There are several common methods for encoding categorical data:\n",
        "\n",
        "1. **Label Encoding:**\n",
        "\n",
        "   - Label encoding assigns a unique integer to each category in a feature.\n",
        "   - It's suitable for ordinal data where there's a clear order among categories.\n",
        "   - For example, if you have an \"education\" feature with values \"high school,\" \"bachelor's,\" and \"master's,\" you can encode them as 0, 1, and 2, respectively.\n",
        "<br />\n",
        "<br />\n",
        "2. **One-Hot Encoding:**\n",
        "\n",
        "   - One-hot encoding creates a binary (0 or 1) column for each category in a nominal feature.\n",
        "   - It's suitable for nominal data where there's no inherent order among categories.\n",
        "   - Each category becomes a new feature, and the presence (1) or absence (0) of a category is indicated for each row.\n",
        "<br />\n",
        "<br />\n",
        "3. **Target Encoding (Mean Encoding):**\n",
        "\n",
        "   - Target encoding replaces each category with the mean of the target variable for that category.\n",
        "   - It's often used for classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "psElSUugLfHc"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKQO9wtB8Pc0"
      },
      "source": [
        "### III. Handling Imbalanced Dataset\n",
        "\n",
        "**Handling imbalanced datasets** is important because imbalanced data can lead to several issues that negatively impact the performance and reliability of machine learning models. Here are some key reasons:\n",
        "\n",
        "1. **Biased Model Performance**:\n",
        "\n",
        " - Models trained on imbalanced data tend to be biased towards the majority class, leading to poor performance on the minority class. This can result in misleading accuracy metrics.\n",
        "\n",
        "2. **Misleading Accuracy**:\n",
        "\n",
        " - High overall accuracy can be misleading in imbalanced datasets. For example, if 95% of the data belongs to one class, a model that always predicts the majority class will have 95% accuracy but will fail to identify the minority class.\n",
        "\n",
        "3. **Poor Generalization**:\n",
        "\n",
        " - Models trained on imbalanced data may not generalize well to new, unseen data, especially if the minority class is underrepresented.\n",
        "\n",
        "\n",
        "Some methods to handle imbalanced datasets:\n",
        "1. **Resampling Methods**:\n",
        "\n",
        " - Oversampling: Increase the number of instances in the minority class by duplicating or generating synthetic samples (e.g., SMOTE).\n",
        " - Undersampling: Reduce the number of instances in the majority class to balance the dataset.\n",
        "\n",
        "2. **Evaluation Metrics**:\n",
        "\n",
        " - Use appropriate evaluation metrics such as precision, recall, F1-score, ROC-AUC, and confusion matrix instead of accuracy to better assess model performance on imbalanced data.\n",
        "\n",
        "3. **Algorithmic Approaches**:\n",
        "\n",
        " - Use algorithms that are designed to handle imbalanced data, such as decision trees, random forests, or ensemble methods.\n",
        " - Adjust class weights in algorithms to give more importance to the minority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "u2BQd2XJ9W1i"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ctVzt5DLfHd"
      },
      "source": [
        "# 3. Compile Preprocessing Pipeline\n",
        "\n",
        "All of the preprocessing classes or functions defined earlier will be compiled in this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_ZlncSVjJG6"
      },
      "source": [
        "If you use sklearn to create preprocessing classes, you can list your preprocessing classes in the Pipeline object sequentially, and then fit and transform your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jHraoW_7LfHd"
      },
      "outputs": [],
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "\n",
        "# # Note: You can add or delete preprocessing components from this pipeline\n",
        "\n",
        "# pipe = Pipeline([(\"imputer\", FeatureImputer()),\n",
        "#                  (\"featurecreator\", FeatureCreator()),\n",
        "#                  (\"scaler\", FeatureScaler()),\n",
        "#                  (\"encoder\", FeatureEncoder())])\n",
        "\n",
        "# train_set = pipe.fit_transform(train_set)\n",
        "# val_set = pipe.transform(val_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9s56aFFxLfHd"
      },
      "outputs": [],
      "source": [
        "# # Your code should work up until this point\n",
        "# train_set = pipe.fit_transform(train_set)\n",
        "# val_set = pipe.transform(val_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXoCqMztjhr-"
      },
      "source": [
        "or create your own here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7OoZ3oXEj2CW"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A3adbZXLfHe"
      },
      "source": [
        "# 4. Modeling and Validation\n",
        "\n",
        "Modelling is the process of building your own machine learning models to solve specific problems, or in this assignment context, predicting the target feature `label`. Validation is the process of evaluating your trained model using the validation set or cross-validation method and providing some metrics that can help you decide what to do in the next iteration of development."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnhMNbBILfHf"
      },
      "source": [
        "## A. KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KV6ICmFmlqjk"
      },
      "outputs": [],
      "source": [
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW0bMzkDLfHf"
      },
      "source": [
        "## B. Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "C_XwsN_-LfHg"
      },
      "outputs": [],
      "source": [
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoH2u6fOLfHh"
      },
      "source": [
        "## C. Improvements (Optional)\n",
        "\n",
        "- **Visualize the model evaluation result**\n",
        "\n",
        "This will help you to understand the details more clearly about your model's performance. From the visualization, you can see clearly if your model is leaning towards a class than the others. (Hint: confusion matrix, ROC-AUC curve, etc.)\n",
        "\n",
        "- **Explore the hyperparameters of your models**\n",
        "\n",
        "Each models have their own hyperparameters. And each of the hyperparameter have different effects on the model behaviour. You can optimize the model performance by finding the good set of hyperparameters through a process called **hyperparameter tuning**. (Hint: Grid search, random search, bayesian optimization)\n",
        "\n",
        "- **Cross-validation**\n",
        "\n",
        "Cross-validation is a critical technique in machine learning and data science for evaluating and validating the performance of predictive models. It provides a more **robust** and **reliable** evaluation method compared to a hold-out (single train-test set) validation. Though, it requires more time and computing power because of how cross-validation works. (Hint: k-fold cross-validation, stratified k-fold cross-validation, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pg-A54yELfHh"
      },
      "outputs": [],
      "source": [
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li4l53DjLfHh"
      },
      "source": [
        "## D. Submission\n",
        "To predict the test set target feature and submit the results to the kaggle competition platform, do the following:\n",
        "1. Create a new pipeline instance identical to the first in Data Preprocessing\n",
        "2. With the pipeline, apply `fit_transform` to the original training set before splitting, then only apply `transform` to the test set.\n",
        "3. Retrain the model on the preprocessed training set\n",
        "4. Predict the test set\n",
        "5. Make sure the submission contains the `id` and `label` column.\n",
        "\n",
        "Note: Adjust step 1 and 2 to your implementation of the preprocessing step if you don't use pipeline API from `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LeqnfWc-LfHi"
      },
      "outputs": [],
      "source": [
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-jXvKOpLfHi"
      },
      "source": [
        "# 6. Error Analysis\n",
        "\n",
        "Based on all the process you have done until the modeling and evaluation step, write an analysis to support each steps you have taken to solve this problem. Write the analysis using the markdown block. Some questions that may help you in writing the analysis:\n",
        "\n",
        "- Does my model perform better in predicting one class than the other? If so, why is that?\n",
        "- To each models I have tried, which performs the best and what could be the reason?\n",
        "- Is it better for me to impute or drop the missing data? Why?\n",
        "- Does feature scaling help improve my model performance?\n",
        "- etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWL3nEAELfHj"
      },
      "source": [
        "`Provide your analysis here`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
