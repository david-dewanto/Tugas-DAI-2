{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR1JW69eLfG_"
   },
   "source": [
    "# IF3070 Foundations of Artificial Intelligence | Tugas Besar 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucbaI5rBLtjJ"
   },
   "source": [
    "Group Number: 18\n",
    "\n",
    "Group Members:\n",
    "- Arvyno Pranata Limahardja (18222007)\n",
    "- David Dewanto (18222027)\n",
    "- Bastian Natanael Sibarani (18222053)\n",
    "- Dedy Hofmanindo Saragih (18222085)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwzsfETHLfHA"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFm0BQYbQEgo",
    "outputId": "892ca042-e93d-4e3a-927d-b2de1f0bb4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\david\\anaconda3\\lib\\site-packages (2.6.4)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from category_encoders) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from category_encoders) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from category_encoders) (1.13.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from category_encoders) (0.14.2)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\david\\anaconda3\\lib\\site-packages (from category_encoders) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from category_encoders) (0.5.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\david\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\david\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\david\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (2.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\david\\anaconda3\\lib\\site-packages (from statsmodels>=0.9.0->category_encoders) (23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jZJU5W_4LfHB"
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union, Set\n",
    "from urllib.parse import urlparse, unquote, parse_qs\n",
    "import pickle\n",
    "\n",
    "# Core Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Network Analysis\n",
    "import ipaddress\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import (\n",
    "    chi2_contingency,\n",
    "    median_abs_deviation,\n",
    "    f_oneway,\n",
    "    spearmanr\n",
    ")\n",
    "\n",
    "# Scikit-learn Core\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    GridSearchCV\n",
    ")\n",
    "\n",
    "# Scikit-learn Preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    PowerTransformer\n",
    ")\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_classif,\n",
    "    VarianceThreshold\n",
    ")\n",
    "\n",
    "# Scikit-learn Models\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Scikit-learn Metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Imbalanced Learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Additional ML Libraries\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Time Utilities\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbjLIdYLfHC"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvx-gT3bLfHM"
   },
   "source": [
    "# 1. Split Training Set and Validation Set\n",
    "\n",
    "Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split.\n",
    "\n",
    "Note: For training, you should use the data contained in the `train` folder given by the TA. The `test` data is only used for kaggle submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC14lmo_LfHN"
   },
   "source": [
    "# 2. Data Cleaning and Preprocessing\n",
    "\n",
    "This step is the first thing to be done once a Data Scientist have grasped a general knowledge of the data. Raw data is **seldom ready for training**, therefore steps need to be taken to clean and format the data for the Machine Learning model to interpret.\n",
    "\n",
    "By performing data cleaning and preprocessing, you ensure that your dataset is ready for model training, leading to more accurate and reliable machine learning results. These steps are essential for transforming raw data into a format that machine learning algorithms can effectively learn from and make predictions.\n",
    "\n",
    "We will give some common methods for you to try, but you only have to **at least implement one method for each process**. For each step that you will do, **please explain the reason why did you do that process. Write it in a markdown cell under the code cell you wrote.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p95_A8hSLfHY"
   },
   "source": [
    "## A. Data Cleaning\n",
    "\n",
    "**Data cleaning** is the crucial first step in preparing your dataset for machine learning. Raw data collected from various sources is often messy and may contain errors, missing values, and inconsistencies. Data cleaning involves the following steps:\n",
    "\n",
    "1. **Handling Missing Data:** Identify and address missing values in the dataset. This can include imputing missing values, removing rows or columns with excessive missing data, or using more advanced techniques like interpolation.\n",
    "\n",
    "2. **Dealing with Outliers:** Identify and handle outliers, which are data points significantly different from the rest of the dataset. Outliers can be removed or transformed to improve model performance.\n",
    "\n",
    "3. **Data Validation:** Check for data integrity and consistency. Ensure that data types are correct, categorical variables have consistent labels, and numerical values fall within expected ranges.\n",
    "\n",
    "4. **Removing Duplicates:** Identify and remove duplicate rows, as they can skew the model's training process and evaluation metrics.\n",
    "\n",
    "5. **Feature Engineering**: Create new features or modify existing ones to extract relevant information. This step can involve scaling, normalizing, or encoding features for better model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wZmOgAVvPXP9"
   },
   "outputs": [],
   "source": [
    "class DatasetSplitter:\n",
    "    \"\"\"\n",
    "    A class to handle dataset splitting with stratification and validation.\n",
    "\n",
    "    Attributes:\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        test_size (float): Proportion of dataset to include in validation split\n",
    "        target_col (str): Name of the target column\n",
    "        verbose (bool): Whether to print detailed information\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        test_size: float = 0.2,\n",
    "        target_col: str = 'label',\n",
    "        random_state: int = 42,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DatasetSplitter.\n",
    "\n",
    "        Args:\n",
    "            test_size: Proportion of dataset for validation (default: 0.2)\n",
    "            target_col: Name of target column (default: 'label')\n",
    "            random_state: Random seed (default: 42)\n",
    "            verbose: Whether to print details (default: True)\n",
    "        \"\"\"\n",
    "        self.test_size = test_size\n",
    "        self.target_col = target_col\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialize split statistics\n",
    "        self.split_stats = {}\n",
    "\n",
    "    def validate_input(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Validate input DataFrame before splitting.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to validate\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        if self.target_col not in df.columns:\n",
    "            raise ValueError(f\"Target column '{self.target_col}' not found in DataFrame\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"DataFrame cannot be empty\")\n",
    "\n",
    "        if df[self.target_col].isnull().any():\n",
    "            raise ValueError(\"Target column contains missing values\")\n",
    "\n",
    "    def split_dataset(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Creates training and validation splits while preserving class distributions.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing the complete dataset\n",
    "\n",
    "        Returns:\n",
    "            tuple: (train_data, validation_data)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            self.validate_input(df)\n",
    "\n",
    "            # Store initial state\n",
    "            self.split_stats['initial_shape'] = df.shape\n",
    "            self.split_stats['initial_class_dist'] = df[self.target_col].value_counts(normalize=True).to_dict()\n",
    "\n",
    "            # Drop missing URL rows\n",
    "            df = df.dropna(subset=['URL'])\n",
    "\n",
    "            # Separate features and target\n",
    "            X = df.drop(self.target_col, axis=1)\n",
    "            y = df[self.target_col]\n",
    "\n",
    "            # Create stratified split\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y,\n",
    "                test_size=self.test_size,\n",
    "                random_state=self.random_state,\n",
    "                stratify=y\n",
    "            )\n",
    "\n",
    "            # Reconstruct complete dataframes\n",
    "            train_data = X_train.copy()\n",
    "            train_data[self.target_col] = y_train\n",
    "\n",
    "            validation_data = X_val.copy()\n",
    "            validation_data[self.target_col] = y_val\n",
    "\n",
    "            # Store split statistics\n",
    "            self._calculate_split_statistics(train_data, validation_data)\n",
    "\n",
    "            # Print statistics if verbose\n",
    "            if self.verbose:\n",
    "                self._print_split_summary()\n",
    "\n",
    "            return train_data, validation_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in split_dataset: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calculate_split_statistics(\n",
    "        self,\n",
    "        train_data: pd.DataFrame,\n",
    "        validation_data: pd.DataFrame\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Calculate detailed statistics about the split.\n",
    "\n",
    "        Args:\n",
    "            train_data: Training DataFrame\n",
    "            validation_data: Validation DataFrame\n",
    "        \"\"\"\n",
    "        self.split_stats.update({\n",
    "            'train_shape': train_data.shape,\n",
    "            'validation_shape': validation_data.shape,\n",
    "            'train_class_dist': train_data[self.target_col].value_counts(normalize=True).to_dict(),\n",
    "            'validation_class_dist': validation_data[self.target_col].value_counts(normalize=True).to_dict(),\n",
    "            'train_nulls': train_data.isnull().sum().to_dict(),\n",
    "            'validation_nulls': validation_data.isnull().sum().to_dict()\n",
    "        })\n",
    "\n",
    "    def _print_split_summary(self) -> None:\n",
    "        \"\"\"Print formatted summary of the split results.\"\"\"\n",
    "        self.logger.info(\"\\nDataset Split Summary\")\n",
    "        self.logger.info(\"-\" * 50)\n",
    "\n",
    "        # Size information\n",
    "        self.logger.info(f\"\\nTotal samples: {self.split_stats['initial_shape'][0]}\")\n",
    "        self.logger.info(\n",
    "            f\"Training set size: {self.split_stats['train_shape'][0]} \"\n",
    "            f\"({self.split_stats['train_shape'][0]/self.split_stats['initial_shape'][0]*100:.2f}%)\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"Validation set size: {self.split_stats['validation_shape'][0]} \"\n",
    "            f\"({self.split_stats['validation_shape'][0]/self.split_stats['initial_shape'][0]*100:.2f}%)\"\n",
    "        )\n",
    "\n",
    "        # Class distribution\n",
    "        self.logger.info(\"\\nClass Distribution:\")\n",
    "        self.logger.info(\"Original:\")\n",
    "        for cls, prop in self.split_stats['initial_class_dist'].items():\n",
    "            self.logger.info(f\"Class {cls}: {prop*100:.2f}%\")\n",
    "\n",
    "        self.logger.info(\"\\nTraining:\")\n",
    "        for cls, prop in self.split_stats['train_class_dist'].items():\n",
    "            self.logger.info(f\"Class {cls}: {prop*100:.2f}%\")\n",
    "\n",
    "        self.logger.info(\"\\nValidation:\")\n",
    "        for cls, prop in self.split_stats['validation_class_dist'].items():\n",
    "            self.logger.info(f\"Class {cls}: {prop*100:.2f}%\")\n",
    "\n",
    "    def get_split_statistics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get dictionary of split statistics.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing split statistics\n",
    "        \"\"\"\n",
    "        return self.split_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wruGao9LfHZ"
   },
   "source": [
    "### I. Handling Missing Data\n",
    "\n",
    "Missing data can adversely affect the performance and accuracy of machine learning models. There are several strategies to handle missing data in machine learning:\n",
    "\n",
    "1. **Data Imputation:**\n",
    "\n",
    "    a. **Mean, Median, or Mode Imputation:** For numerical features, you can replace missing values with the mean, median, or mode of the non-missing values in the same feature. This method is simple and often effective when data is missing at random.\n",
    "\n",
    "    b. **Constant Value Imputation:** You can replace missing values with a predefined constant value (e.g., 0) if it makes sense for your dataset and problem.\n",
    "\n",
    "    c. **Imputation Using Predictive Models:** More advanced techniques involve using predictive models to estimate missing values. For example, you can train a regression model to predict missing numerical values or a classification model to predict missing categorical values.\n",
    "\n",
    "2. **Deletion of Missing Data:**\n",
    "\n",
    "    a. **Listwise Deletion:** In cases where the amount of missing data is relatively small, you can simply remove rows with missing values from your dataset. However, this approach can lead to a loss of valuable information.\n",
    "\n",
    "    b. **Column (Feature) Deletion:** If a feature has a large number of missing values and is not critical for your analysis, you can consider removing that feature altogether.\n",
    "\n",
    "3. **Domain-Specific Strategies:**\n",
    "\n",
    "    a. **Domain Knowledge:** In some cases, domain knowledge can guide the imputation process. For example, if you know that missing values are related to a specific condition, you can impute them accordingly.\n",
    "\n",
    "4. **Imputation Libraries:**\n",
    "\n",
    "    a. **Scikit-Learn:** Scikit-Learn provides a `SimpleImputer` class that can handle basic imputation strategies like mean, median, and mode imputation.\n",
    "\n",
    "    b. **Fancyimpute:** Fancyimpute is a Python library that offers more advanced imputation techniques, including matrix factorization, k-nearest neighbors, and deep learning-based methods.\n",
    "\n",
    "The choice of imputation method should be guided by the nature of your data, the amount of missing data, the problem you are trying to solve, and the assumptions you are willing to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ucZNfCkiLfHZ"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FeatureStats:\n",
    "    \"\"\"Data class to store feature statistics\"\"\"\n",
    "    data_type: str\n",
    "    distribution_type: Optional[str] = None\n",
    "    imputation_strategy: str = \"\"\n",
    "\n",
    "class MissingValueHandler:\n",
    "    \"\"\"\n",
    "    A class to handle missing values in the dataset with specialized handling for URL-derived features.\n",
    "\n",
    "    Attributes:\n",
    "        metadata (dict): Additional feature metadata\n",
    "        verbose (bool): Whether to print detailed information\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata: Optional[Dict] = None,\n",
    "        verbose: bool = True,\n",
    "        random_state: int = 42\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the MissingValueHandler.\n",
    "\n",
    "        Args:\n",
    "            metadata: Dictionary containing feature metadata\n",
    "            verbose: Whether to print details\n",
    "            random_state: Random seed\n",
    "        \"\"\"\n",
    "        self.metadata = metadata or {}\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.strategy_summary = {}\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialize URL feature extractor components\n",
    "        self.ip_pattern = r'^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$'\n",
    "\n",
    "    def _validate_input(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Validate input DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to validate\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"DataFrame cannot be empty\")\n",
    "\n",
    "    def _analyze_distributions(self, df: pd.DataFrame) -> Dict[str, FeatureStats]:\n",
    "        \"\"\"\n",
    "        Analyze distributions and determine imputation strategies.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            dict: Feature statistics and recommended strategies\n",
    "        \"\"\"\n",
    "        numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "        for col in numerical_columns:\n",
    "            clean_data = df[col].dropna()\n",
    "            if len(clean_data) > 0:\n",
    "                stats = {\n",
    "                    'mean': clean_data.mean(),\n",
    "                    'median': clean_data.median(),\n",
    "                    'std': clean_data.std(),\n",
    "                    'skew': clean_data.skew()\n",
    "                }\n",
    "\n",
    "                # Determine distribution type\n",
    "                mean_median_diff = abs(stats['mean'] - stats['median']) / stats['std']\n",
    "                if mean_median_diff < 0.2 and abs(stats['skew']) < 0.5:\n",
    "                    distribution = \"Normal\"\n",
    "                    strategy = \"Mean Imputation\"\n",
    "                else:\n",
    "                    distribution = \"Skewed\"\n",
    "                    strategy = \"Median Imputation\"\n",
    "\n",
    "                self.strategy_summary[col] = FeatureStats(\n",
    "                    data_type=str(df[col].dtype),\n",
    "                    distribution_type=distribution,\n",
    "                    imputation_strategy=strategy\n",
    "                )\n",
    "\n",
    "        # Handle categorical features\n",
    "        for col in categorical_columns:\n",
    "            if df[col].isnull().any():\n",
    "                self.strategy_summary[col] = FeatureStats(\n",
    "                    data_type=str(df[col].dtype),\n",
    "                    imputation_strategy=\"Mode Imputation\"\n",
    "                )\n",
    "\n",
    "        return self.strategy_summary\n",
    "\n",
    "    def _extract_url_features(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract features from a URL.\n",
    "\n",
    "        Args:\n",
    "            url: URL string\n",
    "\n",
    "        Returns:\n",
    "            dict: Extracted features\n",
    "        \"\"\"\n",
    "        if pd.isna(url):\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            # Parse URL\n",
    "            parsed = urlparse(url)\n",
    "            domain = parsed.netloc\n",
    "\n",
    "            features = {\n",
    "                'URLLength': len(url),\n",
    "                'Domain': domain,\n",
    "                'DomainLength': len(domain),\n",
    "                'TLD': domain.split('.')[-1] if domain else '',\n",
    "                'TLDLength': len(domain.split('.')[-1]) if domain else 0,\n",
    "                'NoOfSubDomain': domain.count('.') if domain else 0,\n",
    "                'IsHTTPS': url.startswith('https://'),\n",
    "                'IsDomainIP': bool(re.match(self.ip_pattern, domain))\n",
    "            }\n",
    "\n",
    "            # Character analysis\n",
    "            letters = sum(c.isalpha() for c in url)\n",
    "            digits = sum(c.isdigit() for c in url)\n",
    "            total_len = len(url)\n",
    "\n",
    "            features.update({\n",
    "                'NoOfLettersInURL': letters,\n",
    "                'LetterRatioInURL': letters / total_len if total_len > 0 else 0,\n",
    "                'NoOfDegitsInURL': digits,\n",
    "                'DegitRatioInURL': digits / total_len if total_len > 0 else 0,\n",
    "                'NoOfSpecialCharsInURL': total_len - letters - digits\n",
    "            })\n",
    "\n",
    "            return features\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error extracting URL features: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _impute_url_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Impute URL-derived features.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with imputed URL features\n",
    "        \"\"\"\n",
    "        df_imputed = df.copy()\n",
    "\n",
    "        # Process each URL\n",
    "        for idx, row in df.iterrows():\n",
    "            if pd.notna(row['URL']):\n",
    "                features = self._extract_url_features(row['URL'])\n",
    "                for feat, value in features.items():\n",
    "                    if feat in df.columns and pd.isna(df_imputed.at[idx, feat]):\n",
    "                        df_imputed.at[idx, feat] = value\n",
    "\n",
    "        return df_imputed\n",
    "\n",
    "    def _impute_standard_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Impute non-URL features using appropriate strategies.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with imputed standard features\n",
    "        \"\"\"\n",
    "        df_imputed = df.copy()\n",
    "\n",
    "        for col, stats in self.strategy_summary.items():\n",
    "            if col in df.columns and df[col].isnull().any():\n",
    "                try:\n",
    "                    if stats.imputation_strategy == \"Mean Imputation\":\n",
    "                        imputer = SimpleImputer(strategy='mean')\n",
    "                    elif stats.imputation_strategy == \"Median Imputation\":\n",
    "                        imputer = SimpleImputer(strategy='median')\n",
    "                    elif stats.imputation_strategy == \"Mode Imputation\":\n",
    "                        imputer = SimpleImputer(strategy='most_frequent')\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    df_imputed[col] = imputer.fit_transform(df[[col]])\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error imputing column {col}: {str(e)}\")\n",
    "\n",
    "        return df_imputed\n",
    "\n",
    "    def impute_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values in the dataset.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with imputed values\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            self._validate_input(df)\n",
    "\n",
    "            # Store initial state\n",
    "            initial_missing = df.isnull().sum()\n",
    "\n",
    "            # Analyze distributions and determine strategies\n",
    "            self._analyze_distributions(df)\n",
    "\n",
    "            # Handle URL-derived features first\n",
    "            df_imputed = self._impute_url_features(df)\n",
    "\n",
    "            # Handle remaining features\n",
    "            df_imputed = self._impute_standard_features(df_imputed)\n",
    "\n",
    "            # Calculate and print statistics if verbose\n",
    "            if self.verbose:\n",
    "                final_missing = df_imputed.isnull().sum()\n",
    "                self.logger.info(\"\\nMissing Value Imputation Summary:\")\n",
    "                self.logger.info(\"-\" * 50)\n",
    "\n",
    "                for column in df.columns:\n",
    "                    initial = initial_missing[column]\n",
    "                    final = final_missing[column]\n",
    "                    if initial > 0:\n",
    "                        self.logger.info(f\"\\nColumn: {column}\")\n",
    "                        self.logger.info(f\"Initial missing: {initial}\")\n",
    "                        self.logger.info(f\"Final missing: {final}\")\n",
    "                        self.logger.info(f\"Values imputed: {initial - final}\")\n",
    "                        if column in self.strategy_summary:\n",
    "                            self.logger.info(f\"Strategy used: {self.strategy_summary[column].imputation_strategy}\")\n",
    "\n",
    "            return df_imputed\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in impute_missing_values: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_imputation_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get imputation statistics and strategies.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing imputation statistics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'strategy_summary': self.strategy_summary,\n",
    "            'feature_metadata': self.metadata\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgrSMcK75VY_"
   },
   "source": [
    "### II. Dealing with Outliers\n",
    "\n",
    "Outliers are data points that significantly differ from the majority of the data. They can be unusually high or low values that do not fit the pattern of the rest of the dataset. Outliers can significantly impact model performance, so it is important to handle them properly.\n",
    "\n",
    "Some methods to handle outliers:\n",
    "1. **Imputation**: Replace with mean, median, or a boundary value.\n",
    "2. **Clipping**: Cap values to upper and lower limits.\n",
    "3. **Transformation**: Use log, square root, or power transformations to reduce their influence.\n",
    "4. **Model-Based**: Use algorithms robust to outliers (e.g., tree-based models, Huber regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CgbZ6Lv17Uf0"
   },
   "outputs": [],
   "source": [
    "class OutlierProcessor:\n",
    "    \"\"\"\n",
    "    A class to handle outlier detection and processing with specialized handling\n",
    "    for URL-based features and standard numerical features.\n",
    "\n",
    "    Attributes:\n",
    "        variance_threshold (float): Threshold for variance filter\n",
    "        correlation_threshold (float): Threshold for correlation analysis\n",
    "        verbose (bool): Whether to print detailed information\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        variance_threshold: float = 0.1,\n",
    "        correlation_threshold: float = 0.8,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the OutlierProcessor.\n",
    "\n",
    "        Args:\n",
    "            variance_threshold: Threshold for variance filter\n",
    "            correlation_threshold: Threshold for correlation analysis\n",
    "            verbose: Whether to print details\n",
    "        \"\"\"\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Store outlier statistics\n",
    "        self.outlier_stats = {}\n",
    "\n",
    "        # Initialize URL pattern detection\n",
    "        self.url_patterns = {\n",
    "            'url_encoding': r'%[0-9A-Fa-f]{2}',\n",
    "            'double_encoding': r'%25[0-9A-Fa-f]{2}',\n",
    "            'punycode': r'xn--',\n",
    "            'ip_pattern': r'^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$'\n",
    "        }\n",
    "\n",
    "        # Character substitutions for detection\n",
    "        self.char_substitutions = {\n",
    "            '0': 'o', '1': 'l', '3': 'e', '4': 'a', '5': 's',\n",
    "            '6': 'b', '7': 't', '8': 'b', '9': 'g'\n",
    "        }\n",
    "\n",
    "    def _validate_input(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Validate input DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to validate\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"DataFrame cannot be empty\")\n",
    "\n",
    "    def _determine_distribution(self, data: pd.Series) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Determine distribution type and appropriate outlier detection method.\n",
    "\n",
    "        Args:\n",
    "            data: Series of values to analyze\n",
    "\n",
    "        Returns:\n",
    "            tuple: (distribution_type, recommended_method)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mean = data.mean()\n",
    "            median = data.median()\n",
    "            std = data.std()\n",
    "            skewness = data.skew()\n",
    "\n",
    "            # Normalized mean-median difference\n",
    "            mean_median_diff = abs(mean - median) / std if std != 0 else float('inf')\n",
    "\n",
    "            if mean_median_diff < 0.2 and abs(skewness) < 0.5:\n",
    "                return \"Normal\", \"zscore\"\n",
    "            elif abs(skewness) < 1:\n",
    "                return \"Moderately-Skewed\", \"iqr\"\n",
    "            else:\n",
    "                return \"Highly-Skewed\", \"modified_zscore\"\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error determining distribution: {str(e)}\")\n",
    "            return \"Unknown\", \"iqr\"\n",
    "\n",
    "    def _detect_numerical_outliers(self, data: pd.Series, method: str) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Detect outliers in numerical data using specified method.\n",
    "\n",
    "        Args:\n",
    "            data: Series of values\n",
    "            method: Detection method ('zscore', 'iqr', or 'modified_zscore')\n",
    "\n",
    "        Returns:\n",
    "            Series: Boolean mask indicating outlier positions\n",
    "        \"\"\"\n",
    "        if method == \"zscore\":\n",
    "            z_scores = np.abs((data - data.mean()) / data.std())\n",
    "            return z_scores > 3\n",
    "\n",
    "        elif method == \"iqr\":\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            return (data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))\n",
    "\n",
    "        else:  # modified_zscore\n",
    "            median = data.median()\n",
    "            mad = median_abs_deviation(data)\n",
    "            modified_z_scores = 0.6745 * (data - median) / mad\n",
    "            return np.abs(modified_z_scores) > 3.5\n",
    "\n",
    "    def _detect_url_outliers(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Detect URL-based outliers and suspicious patterns.\n",
    "\n",
    "        Args:\n",
    "            url: URL string to analyze\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of detected patterns and metrics\n",
    "        \"\"\"\n",
    "        if pd.isna(url):\n",
    "            return {\n",
    "                'has_outliers': False,\n",
    "                'outlier_count': 0,\n",
    "                'outlier_ratio': 0.0,\n",
    "                'patterns': []\n",
    "            }\n",
    "\n",
    "        patterns = []\n",
    "        outlier_count = 0\n",
    "\n",
    "        # Check URL encoding\n",
    "        if re.search(self.url_patterns['url_encoding'], url):\n",
    "            patterns.append('url_encoding')\n",
    "            outlier_count += len(re.findall(self.url_patterns['url_encoding'], url))\n",
    "\n",
    "        # Check double encoding\n",
    "        if re.search(self.url_patterns['double_encoding'], url):\n",
    "            patterns.append('double_encoding')\n",
    "            outlier_count += len(re.findall(self.url_patterns['double_encoding'], url))\n",
    "\n",
    "        # Check punycode\n",
    "        if re.search(self.url_patterns['punycode'], url):\n",
    "            patterns.append('punycode')\n",
    "            outlier_count += 1\n",
    "\n",
    "        # Check character substitutions\n",
    "        domain = urlparse(url).netloc\n",
    "        for digit, letter in self.char_substitutions.items():\n",
    "            if digit in domain:\n",
    "                patterns.append('char_substitution')\n",
    "                outlier_count += domain.count(digit)\n",
    "\n",
    "        return {\n",
    "            'has_outliers': len(patterns) > 0,\n",
    "            'outlier_count': outlier_count,\n",
    "            'outlier_ratio': outlier_count / len(url) if len(url) > 0 else 0,\n",
    "            'patterns': patterns\n",
    "        }\n",
    "\n",
    "    def process_outliers(self, df: pd.DataFrame, handling_method: str = 'clip') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process outliers in the dataset.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            handling_method: Method to handle outliers ('clip' or 'remove')\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with processed outliers\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            self._validate_input(df)\n",
    "            df_processed = df.copy()\n",
    "\n",
    "            # Initialize statistics\n",
    "            self.outlier_stats = {\n",
    "                'numerical': {},\n",
    "                'url': {\n",
    "                    'pattern_counts': {},\n",
    "                    'outlier_ratios': []\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Process numerical features\n",
    "            numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "            for column in numerical_features:\n",
    "                data = df[column].dropna()\n",
    "                dist_type, method = self._determine_distribution(data)\n",
    "                outliers = self._detect_numerical_outliers(data, method)\n",
    "\n",
    "                # Store statistics\n",
    "                self.outlier_stats['numerical'][column] = {\n",
    "                    'distribution': dist_type,\n",
    "                    'method': method,\n",
    "                    'outlier_count': outliers.sum()\n",
    "                }\n",
    "\n",
    "                # Handle outliers\n",
    "                if handling_method == 'clip':\n",
    "                    if method == 'zscore':\n",
    "                        mean, std = data.mean(), data.std()\n",
    "                        df_processed[column] = data.clip(mean - 3*std, mean + 3*std)\n",
    "                    elif method == 'iqr':\n",
    "                        Q1, Q3 = data.quantile([0.25, 0.75])\n",
    "                        IQR = Q3 - Q1\n",
    "                        df_processed[column] = data.clip(Q1 - 1.5*IQR, Q3 + 1.5*IQR)\n",
    "                    else:\n",
    "                        median = data.median()\n",
    "                        mad = median_abs_deviation(data)\n",
    "                        df_processed[column] = data.clip(\n",
    "                            median - 3.5*mad/0.6745,\n",
    "                            median + 3.5*mad/0.6745\n",
    "                        )\n",
    "                elif handling_method == 'remove':\n",
    "                    df_processed = df_processed[~outliers]\n",
    "\n",
    "            # Process URL features\n",
    "            if 'URL' in df.columns:\n",
    "                for idx, url in df['URL'].items():\n",
    "                    if pd.notna(url):\n",
    "                        url_stats = self._detect_url_outliers(url)\n",
    "\n",
    "                        # Update pattern counts\n",
    "                        for pattern in url_stats['patterns']:\n",
    "                            self.outlier_stats['url']['pattern_counts'][pattern] = \\\n",
    "                                self.outlier_stats['url']['pattern_counts'].get(pattern, 0) + 1\n",
    "\n",
    "                        # Store outlier ratio\n",
    "                        if url_stats['outlier_ratio'] > 0:\n",
    "                            self.outlier_stats['url']['outlier_ratios'].append(url_stats['outlier_ratio'])\n",
    "\n",
    "            # Print summary if verbose\n",
    "            if self.verbose:\n",
    "                self._print_summary()\n",
    "\n",
    "            return df_processed\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in process_outliers: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _print_summary(self) -> None:\n",
    "        \"\"\"Print formatted summary of outlier processing results.\"\"\"\n",
    "        self.logger.info(\"\\nOutlier Processing Summary\")\n",
    "        self.logger.info(\"-\" * 50)\n",
    "\n",
    "        # Numerical features summary\n",
    "        self.logger.info(\"\\nNumerical Features:\")\n",
    "        for column, stats in self.outlier_stats['numerical'].items():\n",
    "            self.logger.info(f\"\\nColumn: {column}\")\n",
    "            self.logger.info(f\"Distribution: {stats['distribution']}\")\n",
    "            self.logger.info(f\"Method used: {stats['method']}\")\n",
    "            self.logger.info(f\"Outliers detected: {stats['outlier_count']}\")\n",
    "\n",
    "        # URL patterns summary\n",
    "        if self.outlier_stats['url']['pattern_counts']:\n",
    "            self.logger.info(\"\\nURL Patterns:\")\n",
    "            for pattern, count in self.outlier_stats['url']['pattern_counts'].items():\n",
    "                self.logger.info(f\"{pattern}: {count} occurrences\")\n",
    "\n",
    "            if self.outlier_stats['url']['outlier_ratios']:\n",
    "                avg_ratio = np.mean(self.outlier_stats['url']['outlier_ratios'])\n",
    "                self.logger.info(f\"\\nAverage URL outlier ratio: {avg_ratio:.4f}\")\n",
    "\n",
    "    def get_outlier_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get outlier processing statistics.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing outlier statistics\n",
    "        \"\"\"\n",
    "        return self.outlier_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO0ZEZ-s6Lu-"
   },
   "source": [
    "### III. Remove Duplicates\n",
    "Handling duplicate values is crucial because they can compromise data integrity, leading to inaccurate analysis and insights. Duplicate entries can bias machine learning models, causing overfitting and reducing their ability to generalize to new data. They also inflate the dataset size unnecessarily, increasing computational costs and processing times. Additionally, duplicates can distort statistical measures and lead to inconsistencies, ultimately affecting the reliability of data-driven decisions and reporting. Ensuring data quality by removing duplicates is essential for accurate, efficient, and consistent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BHCkkZ-v7iF8"
   },
   "outputs": [],
   "source": [
    "class DuplicateHandler:\n",
    "    \"\"\"\n",
    "    A class to handle duplicate detection and processing with specialized handling\n",
    "    for URL-based duplicates and similar records.\n",
    "\n",
    "    Attributes:\n",
    "        verbose (bool): Whether to print detailed information\n",
    "        min_similarity (float): Minimum similarity threshold for URL matching\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        verbose: bool = True,\n",
    "        min_similarity: float = 0.9\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DuplicateHandler.\n",
    "\n",
    "        Args:\n",
    "            verbose: Whether to print details\n",
    "            min_similarity: Minimum similarity threshold for URL matching\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.min_similarity = min_similarity\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Store duplicate statistics\n",
    "        self.duplicate_stats = {\n",
    "            'initial_rows': 0,\n",
    "            'final_rows': 0,\n",
    "            'exact_duplicates': 0,\n",
    "            'url_duplicates': 0,\n",
    "            'domain_duplicates': 0,\n",
    "            'total_removed': 0,\n",
    "            'percentage_retained': 0\n",
    "        }\n",
    "\n",
    "    def _validate_input(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Validate input DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to validate\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"DataFrame cannot be empty\")\n",
    "\n",
    "    def _find_exact_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Find and remove exact duplicates.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with exact duplicates removed\n",
    "        \"\"\"\n",
    "        # Count exact duplicates\n",
    "        self.duplicate_stats['exact_duplicates'] = df.duplicated().sum()\n",
    "\n",
    "        # Remove exact duplicates\n",
    "        return df.drop_duplicates()\n",
    "\n",
    "    def _merge_url_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Find and merge URL-based duplicates.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with merged URL duplicates\n",
    "        \"\"\"\n",
    "        if 'URL' not in df.columns:\n",
    "            return df\n",
    "\n",
    "        # Count URL duplicates\n",
    "        url_duplicates = df[df['URL'].notna()].duplicated(subset=['URL']).sum()\n",
    "        self.duplicate_stats['url_duplicates'] = url_duplicates\n",
    "\n",
    "        if url_duplicates == 0:\n",
    "            return df\n",
    "\n",
    "        # Get duplicate URLs for merging\n",
    "        duplicate_urls = df[df.duplicated(subset=['URL'], keep=False)]\n",
    "        unique_urls = duplicate_urls['URL'].unique()\n",
    "\n",
    "        # Process each set of URL duplicates\n",
    "        merged_rows = []\n",
    "        non_duplicate_mask = ~df['URL'].isin(unique_urls)\n",
    "\n",
    "        for url in unique_urls:\n",
    "            # Get all rows for this URL\n",
    "            url_rows = df[df['URL'] == url]\n",
    "\n",
    "            # Create merged row\n",
    "            merged_row = {}\n",
    "            for column in df.columns:\n",
    "                values = url_rows[column].dropna()\n",
    "                if len(values) > 0:\n",
    "                    if df[column].dtype == 'boolean':\n",
    "                        # For boolean columns, use most common value\n",
    "                        merged_row[column] = values.mode().iloc[0]\n",
    "                    elif pd.api.types.is_numeric_dtype(df[column].dtype):\n",
    "                        # For numeric columns, use mean\n",
    "                        merged_row[column] = values.mean()\n",
    "                    else:\n",
    "                        # For other types, use first non-null value\n",
    "                        merged_row[column] = values.iloc[0]\n",
    "                else:\n",
    "                    merged_row[column] = np.nan\n",
    "\n",
    "            merged_rows.append(merged_row)\n",
    "\n",
    "        # Combine non-duplicates with merged rows\n",
    "        return pd.concat([\n",
    "            df[non_duplicate_mask],\n",
    "            pd.DataFrame(merged_rows)\n",
    "        ], ignore_index=True)\n",
    "\n",
    "    def _analyze_domain_duplicates(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Analyze domain-based duplicates for reporting.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "        \"\"\"\n",
    "        if 'Domain' in df.columns:\n",
    "            self.duplicate_stats['domain_duplicates'] = \\\n",
    "                df[df['Domain'].notna()].duplicated(subset=['Domain']).sum()\n",
    "\n",
    "    def process_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process duplicates in the dataset.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with duplicates handled\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            self._validate_input(df)\n",
    "\n",
    "            # Store initial state\n",
    "            self.duplicate_stats['initial_rows'] = len(df)\n",
    "\n",
    "            # Remove exact duplicates\n",
    "            df_cleaned = self._find_exact_duplicates(df)\n",
    "\n",
    "            # Merge URL duplicates\n",
    "            df_cleaned = self._merge_url_duplicates(df_cleaned)\n",
    "\n",
    "            # Analyze domain duplicates for reporting\n",
    "            self._analyze_domain_duplicates(df_cleaned)\n",
    "\n",
    "            # Update final statistics\n",
    "            self.duplicate_stats['final_rows'] = len(df_cleaned)\n",
    "            self.duplicate_stats['total_removed'] = \\\n",
    "                self.duplicate_stats['initial_rows'] - self.duplicate_stats['final_rows']\n",
    "            self.duplicate_stats['percentage_retained'] = \\\n",
    "                (self.duplicate_stats['final_rows'] / self.duplicate_stats['initial_rows'] * 100)\n",
    "\n",
    "            # Print summary if verbose\n",
    "            if self.verbose:\n",
    "                self._print_summary()\n",
    "\n",
    "            return df_cleaned\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in process_duplicates: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _print_summary(self) -> None:\n",
    "        \"\"\"Print formatted summary of duplicate processing results.\"\"\"\n",
    "        self.logger.info(\"\\nDuplicate Processing Summary\")\n",
    "        self.logger.info(\"-\" * 30)\n",
    "\n",
    "        self.logger.info(f\"Initial number of rows: {self.duplicate_stats['initial_rows']:,}\")\n",
    "        self.logger.info(f\"Exact duplicates removed: {self.duplicate_stats['exact_duplicates']:,}\")\n",
    "        self.logger.info(f\"URL-based duplicates handled: {self.duplicate_stats['url_duplicates']:,}\")\n",
    "        self.logger.info(f\"Domain-based duplicates found: {self.duplicate_stats['domain_duplicates']:,}\")\n",
    "        self.logger.info(f\"Total rows removed: {self.duplicate_stats['total_removed']:,}\")\n",
    "        self.logger.info(f\"Final number of rows: {self.duplicate_stats['final_rows']:,}\")\n",
    "        self.logger.info(\n",
    "            f\"Percentage of data retained: {self.duplicate_stats['percentage_retained']:.2f}%\"\n",
    "        )\n",
    "\n",
    "    def get_duplicate_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get duplicate processing statistics.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing duplicate statistics\n",
    "        \"\"\"\n",
    "        return self.duplicate_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eycPASmMLfHa"
   },
   "source": [
    "### IV. Feature Engineering\n",
    "\n",
    "**Feature engineering** involves creating new features (input variables) or transforming existing ones to improve the performance of machine learning models. Feature engineering aims to enhance the model's ability to learn patterns and make accurate predictions from the data. It's often said that \"good features make good models.\"\n",
    "\n",
    "1. **Feature Selection:** Feature engineering can involve selecting the most relevant and informative features from the dataset. Removing irrelevant or redundant features not only simplifies the model but also reduces the risk of overfitting.\n",
    "\n",
    "2. **Creating New Features:** Sometimes, the existing features may not capture the underlying patterns effectively. In such cases, engineers create new features that provide additional information. For example:\n",
    "   \n",
    "   - **Polynomial Features:** Engineers may create new features by taking the square, cube, or other higher-order terms of existing numerical features. This can help capture nonlinear relationships.\n",
    "   \n",
    "   - **Interaction Features:** Interaction features are created by combining two or more existing features. For example, if you have features \"length\" and \"width,\" you can create an \"area\" feature by multiplying them.\n",
    "\n",
    "3. **Binning or Discretization:** Continuous numerical features can be divided into bins or categories. For instance, age values can be grouped into bins like \"child,\" \"adult,\" and \"senior.\"\n",
    "\n",
    "4. **Domain-Specific Feature Engineering:** Depending on the domain and problem, engineers may create domain-specific features. For example, in fraud detection, features related to transaction history and user behavior may be engineered to identify anomalies.\n",
    "\n",
    "Feature engineering is both a creative and iterative process. It requires a deep understanding of the data, domain knowledge, and experimentation to determine which features will enhance the model's predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UoXEV6wkLfHa"
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    A class to handle feature selection, creation, and engineering with specialized focus\n",
    "    on URL-based features and feature importance analysis.\n",
    "\n",
    "    Attributes:\n",
    "        variance_threshold (float): Threshold for variance filter\n",
    "        max_categories (int): Maximum categories for categorical features\n",
    "        p_value_threshold (float): Threshold for statistical tests\n",
    "        mi_threshold (float): Threshold for mutual information\n",
    "        correlation_threshold (float): Threshold for correlation analysis\n",
    "        verbose (bool): Whether to print detailed information\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        variance_threshold: float = 0.1,\n",
    "        max_categories: int = 50,\n",
    "        p_value_threshold: float = 0.05,\n",
    "        correlation_threshold: float = 0.8,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the FeatureEngineer.\n",
    "\n",
    "        Args:\n",
    "            variance_threshold: Threshold for variance filter\n",
    "            max_categories: Maximum allowed categories for categorical features\n",
    "            p_value_threshold: Threshold for statistical significance\n",
    "            mi_threshold: Threshold for mutual information\n",
    "            correlation_threshold: Threshold for correlation analysis\n",
    "            verbose: Whether to print details\n",
    "        \"\"\"\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.max_categories = max_categories\n",
    "        self.p_value_threshold = p_value_threshold\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Store feature statistics\n",
    "        self.feature_stats = {}\n",
    "\n",
    "    def _validate_input(self, df: pd.DataFrame, target_column: str) -> None:\n",
    "        \"\"\"\n",
    "        Validate input DataFrame and target column.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to validate\n",
    "            target_column: Name of target column\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"DataFrame cannot be empty\")\n",
    "\n",
    "        if target_column not in df.columns:\n",
    "            raise ValueError(f\"Target column '{target_column}' not found in DataFrame\")\n",
    "\n",
    "    def _analyze_numeric_features(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series\n",
    "    ) -> Tuple[List[str], List[Tuple[str, str]]]:\n",
    "        \"\"\"\n",
    "        Analyze numeric features using statistical tests.\n",
    "\n",
    "        Args:\n",
    "            X: Feature DataFrame\n",
    "            y: Target series\n",
    "\n",
    "        Returns:\n",
    "            tuple: (selected_features, dropped_features)\n",
    "        \"\"\"\n",
    "        selected_features = []\n",
    "        dropped_features = []\n",
    "\n",
    "        numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        for feature in numeric_features:\n",
    "            # Initialize feature statistics\n",
    "            self.feature_stats[feature] = {}\n",
    "\n",
    "            # Calculate variance\n",
    "            variance = X[feature].var()\n",
    "            self.feature_stats[feature]['variance'] = variance\n",
    "\n",
    "            if variance < self.variance_threshold:\n",
    "                dropped_features.append((feature, f'Low Variance ({variance:.4f})'))\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # ANOVA test\n",
    "                groups = [X[feature][y == val].dropna() for val in y.unique()]\n",
    "                f_stat, p_value = f_oneway(*groups)\n",
    "                self.feature_stats[feature].update({\n",
    "                    'f_statistic': f_stat,\n",
    "                    'p_value': p_value\n",
    "                })\n",
    "\n",
    "                if p_value < self.p_value_threshold:\n",
    "                    selected_features.append(feature)\n",
    "                else:\n",
    "                    reason = f'High p-value ({p_value:.4f})'\n",
    "                    dropped_features.append((feature, reason))\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error processing numeric feature {feature}: {e}\")\n",
    "                dropped_features.append((feature, f'Error: {str(e)}'))\n",
    "\n",
    "        return selected_features, dropped_features\n",
    "\n",
    "    def _analyze_categorical_features(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series\n",
    "    ) -> Tuple[List[str], List[Tuple[str, str]]]:\n",
    "        \"\"\"\n",
    "        Analyze categorical features using statistical tests.\n",
    "\n",
    "        Args:\n",
    "            X: Feature DataFrame\n",
    "            y: Target series\n",
    "\n",
    "        Returns:\n",
    "            tuple: (selected_features, dropped_features)\n",
    "        \"\"\"\n",
    "        selected_features = []\n",
    "        dropped_features = []\n",
    "\n",
    "        categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "        for feature in categorical_features:\n",
    "            # Initialize feature statistics\n",
    "            self.feature_stats[feature] = {}\n",
    "\n",
    "            try:\n",
    "                unique_count = X[feature].nunique()\n",
    "                if unique_count > self.max_categories:\n",
    "                    dropped_features.append((feature, f'Too Many Categories ({unique_count})'))\n",
    "                    continue\n",
    "\n",
    "                # Chi-square test\n",
    "                contingency = pd.crosstab(X[feature], y)\n",
    "                chi2, p_value, _, _ = chi2_contingency(contingency)\n",
    "                self.feature_stats[feature].update({\n",
    "                    'chi_square': chi2,\n",
    "                    'p_value': p_value\n",
    "                })\n",
    "\n",
    "                if p_value < self.p_value_threshold:\n",
    "                    selected_features.append(feature)\n",
    "                else:\n",
    "                    reason = f'High p-value ({p_value:.4f})'\n",
    "                    dropped_features.append((feature, reason))\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error processing categorical feature {feature}: {e}\")\n",
    "                dropped_features.append((feature, f'Error: {str(e)}'))\n",
    "\n",
    "        return selected_features, dropped_features\n",
    "\n",
    "    def _check_correlations(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        features: List[str]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Check for correlations between features.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            features: List of features to check\n",
    "\n",
    "        Returns:\n",
    "            list: Final selected features after correlation check\n",
    "        \"\"\"\n",
    "        final_features = set(features)\n",
    "\n",
    "        for i, feat_a in enumerate(features):\n",
    "            if feat_a not in final_features:\n",
    "                continue\n",
    "\n",
    "            for feat_b in features[i + 1:]:\n",
    "                if feat_b not in final_features:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Calculate correlation based on data type\n",
    "                    if df[feat_a].dtype in ['float64', 'int64'] and \\\n",
    "                       df[feat_b].dtype in ['float64', 'int64']:\n",
    "                        corr, _ = spearmanr(df[feat_a].fillna(0), df[feat_b].fillna(0))\n",
    "                        corr = abs(corr)\n",
    "                    else:\n",
    "                        # Use Cramer's V for categorical features\n",
    "                        contingency = pd.crosstab(\n",
    "                            df[feat_a].fillna('missing'),\n",
    "                            df[feat_b].fillna('missing')\n",
    "                        )\n",
    "                        chi2, _, _, _ = chi2_contingency(contingency)\n",
    "                        n = contingency.sum().sum()\n",
    "                        min_dim = min(contingency.shape) - 1\n",
    "                        corr = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0\n",
    "\n",
    "                    if corr >= self.correlation_threshold:\n",
    "                        # Compare mutual information scores\n",
    "                        score_a = self.feature_stats[feat_a].get('mutual_info', 0)\n",
    "                        score_b = self.feature_stats[feat_b].get('mutual_info', 0)\n",
    "\n",
    "                        if score_a >= score_b:\n",
    "                            final_features.discard(feat_b)\n",
    "                            self.logger.info(\n",
    "                                f\"Removed {feat_b} due to high correlation with {feat_a}\"\n",
    "                            )\n",
    "                        else:\n",
    "                            final_features.discard(feat_a)\n",
    "                            self.logger.info(\n",
    "                                f\"Removed {feat_a} due to high correlation with {feat_b}\"\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(\n",
    "                        f\"Error calculating correlation between {feat_a} and {feat_b}: {e}\"\n",
    "                    )\n",
    "\n",
    "        return list(final_features)\n",
    "\n",
    "    def select_features(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        target_column: str\n",
    "    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Select important features from the dataset.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            target_column: Name of target column\n",
    "\n",
    "        Returns:\n",
    "            tuple: (DataFrame with selected features, Selection results)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            self._validate_input(df, target_column)\n",
    "\n",
    "            # Separate features and target\n",
    "            X = df.drop(columns=[target_column])\n",
    "            y = df[target_column]\n",
    "\n",
    "            # Process features by type\n",
    "            numeric_selected, numeric_dropped = self._analyze_numeric_features(X, y)\n",
    "            categorical_selected, categorical_dropped = self._analyze_categorical_features(X, y)\n",
    "\n",
    "            # Combine and check correlations\n",
    "            all_selected = numeric_selected + categorical_selected\n",
    "            final_features = self._check_correlations(df, all_selected)\n",
    "\n",
    "            # Create results dictionary\n",
    "            results = {\n",
    "                'selected_features': final_features,\n",
    "                'dropped_features': numeric_dropped + categorical_dropped,\n",
    "                'feature_stats': self.feature_stats\n",
    "            }\n",
    "\n",
    "            # Return DataFrame with selected features AND target column\n",
    "            return df[final_features + [target_column]], results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in select_features: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def transform(self, df: pd.DataFrame, selected_features) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform new data using the feature selection rules learned during fitting.\n",
    "        This method should be used on validation or test data.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to transform\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with selected features\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If transform is called before select_features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if selected_features:\n",
    "                df_selected = df[selected_features]\n",
    "\n",
    "            # Return DataFrame with only the selected features\n",
    "            return df_selected\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in transform: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _print_summary(self, results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Print formatted summary of feature selection results.\"\"\"\n",
    "        self.logger.info(\"\\nFeature Selection Summary\")\n",
    "        self.logger.info(\"-\" * 50)\n",
    "\n",
    "        self.logger.info(f\"\\nSelected Features ({len(results['selected_features'])}):\")\n",
    "        for feature in results['selected_features']:\n",
    "            stats = self.feature_stats[feature]\n",
    "            self.logger.info(f\"\\n{feature}:\")\n",
    "            for stat, value in stats.items():\n",
    "                self.logger.info(f\"- {stat}: {value:.4f}\")\n",
    "\n",
    "        self.logger.info(f\"\\nDropped Features ({len(results['dropped_features'])}):\")\n",
    "        for feature, reason in results['dropped_features']:\n",
    "            self.logger.info(f\"{feature}: {reason}\")\n",
    "\n",
    "    def get_feature_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get feature selection statistics.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing feature statistics\n",
    "        \"\"\"\n",
    "        return self.feature_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw11_49xLfHb"
   },
   "source": [
    "## B. Data Preprocessing\n",
    "\n",
    "**Data preprocessing** is a broader step that encompasses both data cleaning and additional transformations to make the data suitable for machine learning algorithms. Its primary goals are:\n",
    "\n",
    "1. **Feature Scaling:** Ensure that numerical features have similar scales. Common techniques include Min-Max scaling (scaling to a specific range) or standardization (mean-centered, unit variance).\n",
    "\n",
    "2. **Encoding Categorical Variables:** Machine learning models typically work with numerical data, so categorical variables need to be encoded. This can be done using one-hot encoding, label encoding, or more advanced methods like target encoding.\n",
    "\n",
    "3. **Handling Imbalanced Classes:** If dealing with imbalanced classes in a binary classification task, apply techniques such as oversampling, undersampling, or using different evaluation metrics to address class imbalance.\n",
    "\n",
    "4. **Dimensionality Reduction:** Reduce the number of features using techniques like Principal Component Analysis (PCA) or feature selection to simplify the model and potentially improve its performance.\n",
    "\n",
    "5. **Normalization:** Normalize data to achieve a standard distribution. This is particularly important for algorithms that assume normally distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVyVnA1hLfHd"
   },
   "source": [
    "### Notes on Preprocessing processes\n",
    "\n",
    "It is advised to create functions or classes that have the same/similar type of inputs and outputs, so you can add, remove, or swap the order of the processes easily. You can implement the functions or classes by yourself\n",
    "\n",
    "or\n",
    "\n",
    "use `sklearn` library. To create a new preprocessing component in `sklearn`, implement a corresponding class that includes:\n",
    "1. Inheritance to `BaseEstimator` and `TransformerMixin`\n",
    "2. The method `fit`\n",
    "3. The method `transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WbxHt-5eKz_I"
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# class FeatureEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "\n",
    "#         # Fit the encoder here\n",
    "\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         X_encoded = X.copy()\n",
    "\n",
    "#         # Encode the categorical variables here\n",
    "\n",
    "#         return X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhCgOl4xLfHb"
   },
   "source": [
    "### I. Feature Scaling\n",
    "\n",
    "**Feature scaling** is a preprocessing technique used in machine learning to standardize the range of independent variables or features of data. The primary goal of feature scaling is to ensure that all features contribute equally to the training process and that machine learning algorithms can work effectively with the data.\n",
    "\n",
    "Here are the main reasons why feature scaling is important:\n",
    "\n",
    "1. **Algorithm Sensitivity:** Many machine learning algorithms are sensitive to the scale of input features. If the scales of features are significantly different, some algorithms may perform poorly or take much longer to converge.\n",
    "\n",
    "2. **Distance-Based Algorithms:** Algorithms that rely on distances or similarities between data points, such as k-nearest neighbors (KNN) and support vector machines (SVM), can be influenced by feature scales. Features with larger scales may dominate the distance calculations.\n",
    "\n",
    "3. **Regularization:** Regularization techniques, like L1 (Lasso) and L2 (Ridge) regularization, add penalty terms based on feature coefficients. Scaling ensures that all features are treated equally in the regularization process.\n",
    "\n",
    "Common methods for feature scaling include:\n",
    "\n",
    "1. **Min-Max Scaling (Normalization):** This method scales features to a specific range, typically [0, 1]. It's done using the following formula:\n",
    "\n",
    "   $$X' = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "   - Here, $X$ is the original feature value, $X_{min}$ is the minimum value of the feature, and $X_{max}$ is the maximum value of the feature.  \n",
    "<br />\n",
    "<br />\n",
    "2. **Standardization (Z-score Scaling):** This method scales features to have a mean (average) of 0 and a standard deviation of 1. It's done using the following formula:\n",
    "\n",
    "   $$X' = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "   - $X$ is the original feature value, $\\mu$ is the mean of the feature, and $\\sigma$ is the standard deviation of the feature.  \n",
    "<br />\n",
    "<br />\n",
    "3. **Robust Scaling:** Robust scaling is a method that scales features to the interquartile range (IQR) and is less affected by outliers. It's calculated as:\n",
    "\n",
    "   $$X' = \\frac{X - Q1}{Q3 - Q1}$$\n",
    "\n",
    "   - $X$ is the original feature value, $Q1$ is the first quartile (25th percentile), and $Q3$ is the third quartile (75th percentile) of the feature.  \n",
    "<br />\n",
    "<br />\n",
    "4. **Log Transformation:** In cases where data is highly skewed or has a heavy-tailed distribution, taking the logarithm of the feature values can help stabilize the variance and improve scaling.\n",
    "\n",
    "The choice of scaling method depends on the characteristics of your data and the requirements of your machine learning algorithm. **Min-max scaling and standardization are the most commonly used techniques and work well for many datasets.**\n",
    "\n",
    "Scaling should be applied separately to each training and test set to prevent data leakage from the test set into the training set. Additionally, **some algorithms may not require feature scaling, particularly tree-based models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "COef9EbCLfHb"
   },
   "outputs": [],
   "source": [
    "class FeatureScaler:\n",
    "    \"\"\"\n",
    "    A class to handle feature scaling with specialized handling for different feature types.\n",
    "    Supports standard scaling, min-max scaling, and power transformation.\n",
    "\n",
    "    Attributes:\n",
    "        verbose (bool): Whether to print detailed information\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the FeatureScaler.\n",
    "\n",
    "        Args:\n",
    "            verbose: Whether to print details\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialize feature categories\n",
    "        self.feature_categories = {\n",
    "            'numerical': [],    # For general numeric features\n",
    "            'ratio': [],       # For features between 0 and 1\n",
    "            'skewed': [],      # For highly skewed features\n",
    "            'boolean': [],     # For binary features\n",
    "        }\n",
    "\n",
    "        # Initialize scalers\n",
    "        self.scalers = {\n",
    "            'standard': StandardScaler(),\n",
    "            'minmax': MinMaxScaler(),\n",
    "            'power': PowerTransformer(method='yeo-johnson')\n",
    "        }\n",
    "\n",
    "        # Track fitting status\n",
    "        self.is_fitted = False\n",
    "\n",
    "        # Store scaling statistics\n",
    "        self.scaling_stats = {}\n",
    "\n",
    "    def _validate_input(self, X: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Validate input DataFrame.\n",
    "\n",
    "        Args:\n",
    "            X: Input DataFrame to validate\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        if X.empty:\n",
    "            raise ValueError(\"DataFrame cannot be empty\")\n",
    "\n",
    "    def _categorize_features(self, X: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Categorize features based on their characteristics.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Reset feature categories\n",
    "            for category in ['numerical', 'ratio', 'skewed', 'boolean']:\n",
    "                self.feature_categories[category] = []\n",
    "\n",
    "            for column in X.columns:\n",
    "                # Categorize based on data characteristics\n",
    "                if X[column].dtype == 'boolean' or (\n",
    "                    X[column].nunique() == 2 and\n",
    "                    set(X[column].unique()).issubset({0, 1, True, False})\n",
    "                ):\n",
    "                    self.feature_categories['boolean'].append(column)\n",
    "                elif pd.api.types.is_numeric_dtype(X[column]):\n",
    "                    # For numeric features, check if it's a ratio or skewed\n",
    "                    if 0 <= X[column].min() and X[column].max() <= 1:\n",
    "                        self.feature_categories['ratio'].append(column)\n",
    "                    else:\n",
    "                        skewness = X[column].skew()\n",
    "                        if abs(skewness) > 1:\n",
    "                            self.feature_categories['skewed'].append(column)\n",
    "                        else:\n",
    "                            self.feature_categories['numerical'].append(column)\n",
    "                else:\n",
    "                    # For non-numeric features, try to convert to numeric if possible\n",
    "                    try:\n",
    "                        numeric_series = pd.to_numeric(X[column])\n",
    "                        self.feature_categories['numerical'].append(column)\n",
    "                        self.logger.warning(\n",
    "                            f\"Column {column} was converted to numeric for scaling\"\n",
    "                        )\n",
    "                    except:\n",
    "                        self.logger.warning(\n",
    "                            f\"Column {column} could not be scaled (non-numeric)\"\n",
    "                        )\n",
    "                        self.feature_categories['excluded'].append(column)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in feature categorization: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calculate_scaling_stats(self, X: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Calculate scaling statistics for each feature.\n",
    "\n",
    "        Args:\n",
    "            X: Input DataFrame\n",
    "        \"\"\"\n",
    "        for category, features in self.feature_categories.items():\n",
    "            if category != 'excluded':\n",
    "                for feature in features:\n",
    "                    self.scaling_stats[feature] = {\n",
    "                        'category': category,\n",
    "                        'original_stats': {\n",
    "                            'mean': X[feature].mean(),\n",
    "                            'std': X[feature].std(),\n",
    "                            'min': X[feature].min(),\n",
    "                            'max': X[feature].max(),\n",
    "                            'skew': X[feature].skew()\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fit scalers and transform data.\n",
    "        Use this only on training data.\n",
    "\n",
    "        Args:\n",
    "            X: Input DataFrame to fit and transform\n",
    "\n",
    "        Returns:\n",
    "            Scaled DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            self._validate_input(X)\n",
    "\n",
    "            # Categorize features\n",
    "            self._categorize_features(X)\n",
    "\n",
    "            # Calculate initial statistics\n",
    "            self._calculate_scaling_stats(X)\n",
    "\n",
    "            # Transform data\n",
    "            transformed_data = {}\n",
    "\n",
    "            # Transform skewed features\n",
    "            if self.feature_categories['skewed']:\n",
    "                transformed_data['power'] = self.scalers['power'].fit_transform(\n",
    "                    X[self.feature_categories['skewed']]\n",
    "                )\n",
    "\n",
    "            # Transform numerical features\n",
    "            if self.feature_categories['numerical']:\n",
    "                transformed_data['standard'] = self.scalers['standard'].fit_transform(\n",
    "                    X[self.feature_categories['numerical']]\n",
    "                )\n",
    "\n",
    "            # Transform ratio features\n",
    "            if self.feature_categories['ratio']:\n",
    "                transformed_data['minmax'] = self.scalers['minmax'].fit_transform(\n",
    "                    X[self.feature_categories['ratio']]\n",
    "                )\n",
    "\n",
    "            # Keep boolean features as is\n",
    "            if self.feature_categories['boolean']:\n",
    "                transformed_data['boolean'] = X[self.feature_categories['boolean']].astype(float).values\n",
    "\n",
    "            # Combine all transformed features\n",
    "            scaled_arrays = [arr for arr in transformed_data.values()]\n",
    "            if scaled_arrays:\n",
    "                scaled_array = np.hstack(scaled_arrays)\n",
    "\n",
    "                # Create column names\n",
    "                columns = (self.feature_categories['skewed'] +\n",
    "                         self.feature_categories['numerical'] +\n",
    "                         self.feature_categories['ratio'] +\n",
    "                         self.feature_categories['boolean'])\n",
    "\n",
    "                result = pd.DataFrame(scaled_array, columns=columns, index=X.index)\n",
    "            else:\n",
    "                result = pd.DataFrame(index=X.index)\n",
    "\n",
    "            self.is_fitted = True\n",
    "\n",
    "            # Calculate post-scaling statistics\n",
    "            self._update_scaling_stats(result)\n",
    "\n",
    "            # Print summary if verbose\n",
    "            if self.verbose:\n",
    "                self._print_summary()\n",
    "\n",
    "            # Memory cleanup\n",
    "            gc.collect()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in fit_transform: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform data using fitted scalers.\n",
    "        Use this on validation/test data.\n",
    "\n",
    "        Args:\n",
    "            X: Input DataFrame to transform\n",
    "\n",
    "        Returns:\n",
    "            Scaled DataFrame\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Scalers must be fitted before calling transform\")\n",
    "\n",
    "        try:\n",
    "            transformed_data = {}\n",
    "\n",
    "            # Check which features are available in input data\n",
    "            available_skewed = [f for f in self.feature_categories['skewed'] if f in X.columns]\n",
    "            available_numerical = [f for f in self.feature_categories['numerical'] if f in X.columns]\n",
    "            available_ratio = [f for f in self.feature_categories['ratio'] if f in X.columns]\n",
    "            available_boolean = [f for f in self.feature_categories['boolean'] if f in X.columns]\n",
    "\n",
    "            # Log any missing features\n",
    "            all_expected = (self.feature_categories['skewed'] +\n",
    "                        self.feature_categories['numerical'] +\n",
    "                        self.feature_categories['ratio'] +\n",
    "                        self.feature_categories['boolean'])\n",
    "            missing_features = set(all_expected) - set(X.columns)\n",
    "            if missing_features:\n",
    "                self.logger.warning(f\"Missing features in transform input: {missing_features}\")\n",
    "\n",
    "            # Transform skewed features\n",
    "            if available_skewed:\n",
    "                transformed_data['power'] = self.scalers['power'].transform(\n",
    "                    X[available_skewed]\n",
    "                )\n",
    "\n",
    "            # Transform numerical features\n",
    "            if available_numerical:\n",
    "                transformed_data['standard'] = self.scalers['standard'].transform(\n",
    "                    X[available_numerical]\n",
    "                )\n",
    "\n",
    "            # Transform ratio features\n",
    "            if available_ratio:\n",
    "                transformed_data['minmax'] = self.scalers['minmax'].transform(\n",
    "                    X[available_ratio]\n",
    "                )\n",
    "\n",
    "            # Keep boolean features as is\n",
    "            if available_boolean:\n",
    "                transformed_data['boolean'] = X[available_boolean].astype(float).values\n",
    "\n",
    "            # Combine all transformed features\n",
    "            scaled_arrays = [arr for arr in transformed_data.values()]\n",
    "            if scaled_arrays:\n",
    "                scaled_array = np.hstack(scaled_arrays)\n",
    "\n",
    "                # Create column names\n",
    "                columns = (available_skewed +\n",
    "                        available_numerical +\n",
    "                        available_ratio +\n",
    "                        available_boolean)\n",
    "\n",
    "                result = pd.DataFrame(scaled_array, columns=columns, index=X.index)\n",
    "            else:\n",
    "                result = pd.DataFrame(index=X.index)\n",
    "\n",
    "            # Memory cleanup\n",
    "            gc.collect()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in transform: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _update_scaling_stats(self, scaled_df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Update scaling statistics with post-scaling metrics.\n",
    "\n",
    "        Args:\n",
    "            scaled_df: Scaled DataFrame\n",
    "        \"\"\"\n",
    "        for feature in scaled_df.columns:\n",
    "            self.scaling_stats[feature]['scaled_stats'] = {\n",
    "                'mean': scaled_df[feature].mean(),\n",
    "                'std': scaled_df[feature].std(),\n",
    "                'min': scaled_df[feature].min(),\n",
    "                'max': scaled_df[feature].max(),\n",
    "                'skew': scaled_df[feature].skew()\n",
    "            }\n",
    "\n",
    "    def _print_summary(self) -> None:\n",
    "        \"\"\"Print formatted summary of scaling results.\"\"\"\n",
    "        self.logger.info(\"\\nFeature Scaling Summary\")\n",
    "        self.logger.info(\"-\" * 50)\n",
    "\n",
    "        for category, features in self.feature_categories.items():\n",
    "            if features and category != 'excluded':\n",
    "                self.logger.info(f\"\\n{category.capitalize()} Features:\")\n",
    "                for feature in features:\n",
    "                    stats = self.scaling_stats[feature]\n",
    "                    self.logger.info(f\"\\n{feature}:\")\n",
    "                    self.logger.info(\"Original statistics:\")\n",
    "                    for stat, value in stats['original_stats'].items():\n",
    "                        self.logger.info(f\"- {stat}: {value:.4f}\")\n",
    "                    if 'scaled_stats' in stats:\n",
    "                        self.logger.info(\"Scaled statistics:\")\n",
    "                        for stat, value in stats['scaled_stats'].items():\n",
    "                            self.logger.info(f\"- {stat}: {value:.4f}\")\n",
    "\n",
    "    def get_feature_categories(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Get the categorized features.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of feature categories\n",
    "        \"\"\"\n",
    "        return self.feature_categories\n",
    "\n",
    "    def get_scaling_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get scaling statistics.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing scaling statistics\n",
    "        \"\"\"\n",
    "        return self.scaling_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_Lh-4JwLfHc"
   },
   "source": [
    "### II. Feature Encoding\n",
    "\n",
    "**Feature encoding**, also known as **categorical encoding**, is the process of converting categorical data (non-numeric data) into a numerical format so that it can be used as input for machine learning algorithms. Most machine learning models require numerical data for training and prediction, so feature encoding is a critical step in data preprocessing.\n",
    "\n",
    "Categorical data can take various forms, including:\n",
    "\n",
    "1. **Nominal Data:** Categories with no intrinsic order, like colors or country names.  \n",
    "\n",
    "2. **Ordinal Data:** Categories with a meaningful order but not necessarily equidistant, like education levels (e.g., \"high school,\" \"bachelor's,\" \"master's\").\n",
    "\n",
    "There are several common methods for encoding categorical data:\n",
    "\n",
    "1. **Label Encoding:**\n",
    "\n",
    "   - Label encoding assigns a unique integer to each category in a feature.\n",
    "   - It's suitable for ordinal data where there's a clear order among categories.\n",
    "   - For example, if you have an \"education\" feature with values \"high school,\" \"bachelor's,\" and \"master's,\" you can encode them as 0, 1, and 2, respectively.\n",
    "<br />\n",
    "<br />\n",
    "2. **One-Hot Encoding:**\n",
    "\n",
    "   - One-hot encoding creates a binary (0 or 1) column for each category in a nominal feature.\n",
    "   - It's suitable for nominal data where there's no inherent order among categories.\n",
    "   - Each category becomes a new feature, and the presence (1) or absence (0) of a category is indicated for each row.\n",
    "<br />\n",
    "<br />\n",
    "3. **Target Encoding (Mean Encoding):**\n",
    "\n",
    "   - Target encoding replaces each category with the mean of the target variable for that category.\n",
    "   - It's often used for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "psElSUugLfHc"
   },
   "outputs": [],
   "source": [
    "class FeatureEncoder:\n",
    "    \"\"\"\n",
    "    A class to handle feature encoding with specialized handling for different feature types.\n",
    "    Supports label encoding, target encoding, and boolean conversion.\n",
    "\n",
    "    Attributes:\n",
    "        target_col (str): Name of target column\n",
    "        verbose (bool): Whether to print detailed information\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_col: str = 'label',\n",
    "        verbose: bool = True,\n",
    "        random_state: int = 42\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the FeatureEncoder.\n",
    "\n",
    "        Args:\n",
    "            target_col: Name of target column\n",
    "            verbose: Whether to print details\n",
    "            random_state: Random seed\n",
    "        \"\"\"\n",
    "        self.target_col = target_col\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialize encoders\n",
    "        self.label_encoders = {}\n",
    "        self.target_encoder = None\n",
    "\n",
    "        # Store encoding statistics\n",
    "        self.encoding_stats = {\n",
    "            'boolean_features': {},\n",
    "            'categorical_features': {},\n",
    "            'encoded_features': {}\n",
    "        }\n",
    "\n",
    "        # Track fitting status\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _validate_input(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Validate input DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to validate\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"DataFrame cannot be empty\")\n",
    "\n",
    "    def _encode_boolean_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert boolean features to integers.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with encoded boolean features\n",
    "        \"\"\"\n",
    "        bool_features = [\n",
    "            'IsDomainIP', 'HasObfuscation', 'IsHTTPS', 'HasTitle',\n",
    "            'HasFavicon', 'Robots', 'IsResponsive', 'HasDescription',\n",
    "            'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton',\n",
    "            'HasHiddenFields', 'HasPasswordField', 'Bank', 'Pay',\n",
    "            'Crypto', 'HasCopyrightInfo', 'label'\n",
    "        ]\n",
    "\n",
    "        df_encoded = df.copy()\n",
    "        for feature in bool_features:\n",
    "            if feature in df.columns:\n",
    "                try:\n",
    "                    original_count = len(df[feature])\n",
    "                    missing_count = df[feature].isnull().sum()\n",
    "\n",
    "                    if df[feature].dtype == 'boolean' or set(df[feature].dropna().unique()).issubset({0, 1, True, False}):\n",
    "                        # Store statistics before encoding\n",
    "                        if not pd.isnull(df[feature]).all():\n",
    "                            true_ratio = df[feature].mean()\n",
    "                            mode_value = df[feature].mode()[0]\n",
    "                        else:\n",
    "                            true_ratio = np.nan\n",
    "                            mode_value = False\n",
    "\n",
    "                        # Store feature statistics\n",
    "                        self.encoding_stats['boolean_features'][feature] = {\n",
    "                            'original_count': original_count,\n",
    "                            'missing_count': missing_count,\n",
    "                            'true_ratio': true_ratio,\n",
    "                            'mode_value': mode_value\n",
    "                        }\n",
    "\n",
    "                        # Convert to int\n",
    "                        df_encoded[feature] = df_encoded[feature].fillna(mode_value).astype(int)\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error encoding boolean feature {feature}: {str(e)}\")\n",
    "\n",
    "        return df_encoded\n",
    "\n",
    "    def _encode_categorical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encode categorical features using appropriate methods.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with encoded categorical features\n",
    "        \"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        categorical_features = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "        for feature in categorical_features:\n",
    "            try:\n",
    "                if feature == self.target_col:\n",
    "                    continue\n",
    "\n",
    "                # Store original statistics\n",
    "                self.encoding_stats['categorical_features'][feature] = {\n",
    "                    'unique_values': df[feature].nunique(),\n",
    "                    'missing_count': df[feature].isnull().sum(),\n",
    "                    'value_counts': df[feature].value_counts().to_dict()\n",
    "                }\n",
    "\n",
    "                # Create label encoder for the feature\n",
    "                le = LabelEncoder()\n",
    "                non_null_mask = df[feature].notna()\n",
    "\n",
    "                if non_null_mask.any():\n",
    "                    non_null_values = df.loc[non_null_mask, feature]\n",
    "                    encoded_values = le.fit_transform(non_null_values)\n",
    "                    df_encoded.loc[non_null_mask, feature] = encoded_values\n",
    "\n",
    "                    # Store encoder and mapping\n",
    "                    self.label_encoders[feature] = le\n",
    "                    self.encoding_stats['encoded_features'][feature] = {\n",
    "                        'unique_encoded_values': len(le.classes_),\n",
    "                        'mapping': dict(zip(le.classes_, range(len(le.classes_))))\n",
    "                    }\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error encoding categorical feature {feature}: {str(e)}\")\n",
    "\n",
    "        return df_encoded\n",
    "\n",
    "    def _encode_binary(self, df: pd.DataFrame) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform binning on numerical columns into binary values (0 and 1).\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            numerical_columns: List of numerical column names\n",
    "\n",
    "        Returns:\n",
    "            tuple: (Binned DataFrame, Dictionary of descriptions)\n",
    "        \"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "        for col in numerical_features:\n",
    "            col_data = df[col].dropna()\n",
    "            if len(col_data) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate statistics\n",
    "            mean = col_data.mean()\n",
    "            median = col_data.median()\n",
    "            std = col_data.std()\n",
    "            skewness = col_data.skew()\n",
    "\n",
    "            # Normalized mean-median difference\n",
    "            mean_median_diff = abs(mean - median) / std if std != 0 else float('inf')\n",
    "\n",
    "            if mean_median_diff < 0.2 and abs(skewness) < 0.5:\n",
    "                # Use the mean as the threshold for nearly normal distributions\n",
    "                threshold = mean\n",
    "            elif abs(skewness) < 1:\n",
    "                # Use the median as the threshold for moderately skewed distributions\n",
    "                threshold = median\n",
    "            else:\n",
    "                # Use a custom rule for highly skewed distributions (e.g., 75th percentile)\n",
    "                threshold = col_data.quantile(0.75)\n",
    "\n",
    "            # Apply binary binning\n",
    "            df_encoded[col] = (df[col] >= threshold).astype(int)\n",
    "\n",
    "        return df_encoded\n",
    "\n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fit encoders and transform data.\n",
    "        Use this only on training data.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to fit and transform\n",
    "\n",
    "        Returns:\n",
    "            Encoded DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            self._validate_input(df)\n",
    "\n",
    "            # Encode boolean features\n",
    "            df_encoded = self._encode_boolean_features(df)\n",
    "\n",
    "            # Encode categorical features\n",
    "            df_encoded = self._encode_categorical_features(df_encoded)\n",
    "\n",
    "            # Encode numerical features into binary values\n",
    "            df_encoded = self._encode_binary(df_encoded)\n",
    "\n",
    "            self.is_fitted = True\n",
    "\n",
    "            # Print summary if verbose\n",
    "            if self.verbose:\n",
    "                self._print_summary()\n",
    "\n",
    "            return df_encoded\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in fit_transform: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform data using fitted encoders.\n",
    "        Use this on validation/test data.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to transform\n",
    "\n",
    "        Returns:\n",
    "            Encoded DataFrame\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Encoders must be fitted before calling transform\")\n",
    "\n",
    "        try:\n",
    "            # Encode boolean features\n",
    "            df_encoded = self._encode_boolean_features(df)\n",
    "\n",
    "            # Encode numerical features\n",
    "            df_encoded = self._encode_binary(df_encoded)\n",
    "\n",
    "            # Encode categorical features using fitted encoders\n",
    "            for feature, encoder in self.label_encoders.items():\n",
    "                if feature in df.columns:\n",
    "                    non_null_mask = df[feature].notna()\n",
    "                    if non_null_mask.any():\n",
    "                        # Handle unseen categories\n",
    "                        unique_values = df.loc[non_null_mask, feature].unique()\n",
    "                        unknown_values = set(unique_values) - set(encoder.classes_)\n",
    "\n",
    "                        if unknown_values:\n",
    "                            self.logger.warning(\n",
    "                                f\"Found unknown categories in {feature}: {unknown_values}\"\n",
    "                            )\n",
    "                            # Replace unknown values with mode from training\n",
    "                            mode_value = encoder.classes_[0]  # Most frequent in training\n",
    "                            for val in unknown_values:\n",
    "                                df_encoded.loc[df[feature] == val, feature] = mode_value\n",
    "\n",
    "                        # Transform known values\n",
    "                        valid_mask = df_encoded[feature].isin(encoder.classes_)\n",
    "                        df_encoded.loc[valid_mask, feature] = encoder.transform(\n",
    "                            df_encoded.loc[valid_mask, feature]\n",
    "                        )\n",
    "\n",
    "            return df_encoded\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in transform: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _print_summary(self) -> None:\n",
    "        \"\"\"Print formatted summary of encoding results.\"\"\"\n",
    "        self.logger.info(\"\\nFeature Encoding Summary\")\n",
    "        self.logger.info(\"-\" * 50)\n",
    "\n",
    "        # Boolean features summary\n",
    "        if self.encoding_stats['boolean_features']:\n",
    "            self.logger.info(\"\\nBoolean Features:\")\n",
    "            for feature, stats in self.encoding_stats['boolean_features'].items():\n",
    "                self.logger.info(f\"\\n{feature}:\")\n",
    "                self.logger.info(f\"- Original count: {stats['original_count']}\")\n",
    "                self.logger.info(f\"- Missing values: {stats['missing_count']}\")\n",
    "                self.logger.info(f\"- True ratio: {stats['true_ratio']:.4f}\")\n",
    "                self.logger.info(f\"- Mode value: {stats['mode_value']}\")\n",
    "\n",
    "        # Categorical features summary\n",
    "        if self.encoding_stats['categorical_features']:\n",
    "            self.logger.info(\"\\nCategorical Features:\")\n",
    "            for feature, stats in self.encoding_stats['categorical_features'].items():\n",
    "                self.logger.info(f\"\\n{feature}:\")\n",
    "                self.logger.info(f\"- Unique values: {stats['unique_values']}\")\n",
    "                self.logger.info(f\"- Missing values: {stats['missing_count']}\")\n",
    "\n",
    "    def get_encoding_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get encoding statistics.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing encoding statistics\n",
    "        \"\"\"\n",
    "        return self.encoding_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKQO9wtB8Pc0"
   },
   "source": [
    "### III. Handling Imbalanced Dataset\n",
    "\n",
    "**Handling imbalanced datasets** is important because imbalanced data can lead to several issues that negatively impact the performance and reliability of machine learning models. Here are some key reasons:\n",
    "\n",
    "1. **Biased Model Performance**:\n",
    "\n",
    " - Models trained on imbalanced data tend to be biased towards the majority class, leading to poor performance on the minority class. This can result in misleading accuracy metrics.\n",
    "\n",
    "2. **Misleading Accuracy**:\n",
    "\n",
    " - High overall accuracy can be misleading in imbalanced datasets. For example, if 95% of the data belongs to one class, a model that always predicts the majority class will have 95% accuracy but will fail to identify the minority class.\n",
    "\n",
    "3. **Poor Generalization**:\n",
    "\n",
    " - Models trained on imbalanced data may not generalize well to new, unseen data, especially if the minority class is underrepresented.\n",
    "\n",
    "\n",
    "Some methods to handle imbalanced datasets:\n",
    "1. **Resampling Methods**:\n",
    "\n",
    " - Oversampling: Increase the number of instances in the minority class by duplicating or generating synthetic samples (e.g., SMOTE).\n",
    " - Undersampling: Reduce the number of instances in the majority class to balance the dataset.\n",
    "\n",
    "2. **Evaluation Metrics**:\n",
    "\n",
    " - Use appropriate evaluation metrics such as precision, recall, F1-score, ROC-AUC, and confusion matrix instead of accuracy to better assess model performance on imbalanced data.\n",
    "\n",
    "3. **Algorithmic Approaches**:\n",
    "\n",
    " - Use algorithms that are designed to handle imbalanced data, such as decision trees, random forests, or ensemble methods.\n",
    " - Adjust class weights in algorithms to give more importance to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "u2BQd2XJ9W1i"
   },
   "outputs": [],
   "source": [
    "class ImbalanceHandler:\n",
    "    \"\"\"\n",
    "    A class to handle dataset imbalance using various resampling techniques.\n",
    "    Supports SMOTE, random undersampling, SMOTEENN, and class weights.\n",
    "\n",
    "    Attributes:\n",
    "        target_col (str): Name of target column\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        verbose (bool): Whether to print detailed information\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_col: str = 'label',\n",
    "        random_state: int = 42,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ImbalanceHandler.\n",
    "\n",
    "        Args:\n",
    "            target_col: Name of target column\n",
    "            random_state: Random seed\n",
    "            verbose: Whether to print details\n",
    "        \"\"\"\n",
    "        self.target_col = target_col\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Store imbalance handling results\n",
    "        self.results = {\n",
    "            'original': {},\n",
    "            'smote': {},\n",
    "            'undersampling': {},\n",
    "            'smoteenn': {},\n",
    "            'class_weights': None\n",
    "        }\n",
    "\n",
    "        # Track processing status\n",
    "        self.is_processed = False\n",
    "\n",
    "    def _validate_input(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Validate input DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to validate\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"DataFrame cannot be empty\")\n",
    "\n",
    "        if self.target_col not in df.columns:\n",
    "            raise ValueError(f\"Target column '{self.target_col}' not found in DataFrame\")\n",
    "\n",
    "        # Check for appropriate number of classes\n",
    "        n_classes = len(np.unique(df[self.target_col]))\n",
    "        if n_classes < 2:\n",
    "            raise ValueError(\n",
    "                f\"Target variable must have at least 2 classes for imbalance handling. \"\n",
    "                f\"Currently has {n_classes} class(es).\"\n",
    "            )\n",
    "\n",
    "        # Check for missing values\n",
    "        if df.isnull().any().any():\n",
    "            raise ValueError(\"Dataset contains missing values. Please handle them before resampling.\")\n",
    "\n",
    "    def _calculate_class_distribution(self, y: pd.Series) -> Dict[Any, float]:\n",
    "        \"\"\"\n",
    "        Calculate class distribution.\n",
    "\n",
    "        Args:\n",
    "            y: Target series\n",
    "\n",
    "        Returns:\n",
    "            dict: Class distribution\n",
    "        \"\"\"\n",
    "        return dict(y.value_counts(normalize=True))\n",
    "\n",
    "    def _store_distribution_stats(self, name: str, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        \"\"\"\n",
    "        Store distribution statistics for a given resampling method.\n",
    "\n",
    "        Args:\n",
    "            name: Name of resampling method\n",
    "            X: Feature DataFrame\n",
    "            y: Target series\n",
    "        \"\"\"\n",
    "        self.results[name] = {\n",
    "            'shape': X.shape,\n",
    "            'class_distribution': self._calculate_class_distribution(y)\n",
    "        }\n",
    "\n",
    "    def handle_imbalance(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process class imbalance using multiple methods.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing resampled datasets and class weights\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            self._validate_input(df)\n",
    "\n",
    "            # Separate features and target\n",
    "            X = df.drop(self.target_col, axis=1)\n",
    "            y = df[self.target_col]\n",
    "\n",
    "            # Store original distribution\n",
    "            self._store_distribution_stats('original', X, y)\n",
    "\n",
    "            # Apply SMOTE\n",
    "            self.logger.info(\"\\nApplying SMOTE...\")\n",
    "            smote = SMOTE(random_state=self.random_state)\n",
    "            X_smote, y_smote = smote.fit_resample(X, y)\n",
    "            self._store_distribution_stats('smote', X_smote, y_smote)\n",
    "            self.results['smote']['resampled_data'] = {\n",
    "                'X': X_smote,\n",
    "                'y': y_smote\n",
    "            }\n",
    "\n",
    "            # Apply Random Undersampling\n",
    "            self.logger.info(\"Applying Random Undersampling...\")\n",
    "            rus = RandomUnderSampler(random_state=self.random_state)\n",
    "            X_under, y_under = rus.fit_resample(X, y)\n",
    "            self._store_distribution_stats('undersampling', X_under, y_under)\n",
    "            self.results['undersampling']['resampled_data'] = {\n",
    "                'X': X_under,\n",
    "                'y': y_under\n",
    "            }\n",
    "\n",
    "            # Apply SMOTEENN\n",
    "            self.logger.info(\"Applying SMOTEENN...\")\n",
    "            smoteenn = SMOTEENN(random_state=self.random_state)\n",
    "            X_smoteenn, y_smoteenn = smoteenn.fit_resample(X, y)\n",
    "            self._store_distribution_stats('smoteenn', X_smoteenn, y_smoteenn)\n",
    "            self.results['smoteenn']['resampled_data'] = {\n",
    "                'X': X_smoteenn,\n",
    "                'y': y_smoteenn\n",
    "            }\n",
    "\n",
    "            # Calculate class weights\n",
    "            self.logger.info(\"Calculating class weights...\")\n",
    "            classes = np.unique(y)\n",
    "            class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "            self.results['class_weights'] = dict(zip(classes, class_weights))\n",
    "\n",
    "            self.is_processed = True\n",
    "\n",
    "            # Print summary if verbose\n",
    "            if self.verbose:\n",
    "                self._print_summary()\n",
    "\n",
    "            return self.results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in handle_imbalance: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_resampled_data(self, method: str = 'smoteenn') -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Get resampled data for a specific method.\n",
    "\n",
    "        Args:\n",
    "            method: Resampling method ('smote', 'undersampling', or 'smoteenn')\n",
    "\n",
    "        Returns:\n",
    "            tuple: (X_resampled, y_resampled)\n",
    "        \"\"\"\n",
    "        if not self.is_processed:\n",
    "            raise ValueError(\"Must run handle_imbalance before accessing resampled data\")\n",
    "\n",
    "        if method not in ['smote', 'undersampling', 'smoteenn']:\n",
    "            raise ValueError(f\"Unknown resampling method: {method}\")\n",
    "\n",
    "        return (\n",
    "            self.results[method]['resampled_data']['X'],\n",
    "            self.results[method]['resampled_data']['y']\n",
    "        )\n",
    "\n",
    "    def get_class_weights(self) -> Dict[Any, float]:\n",
    "        \"\"\"\n",
    "        Get computed class weights.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of class weights\n",
    "        \"\"\"\n",
    "        if not self.is_processed:\n",
    "            raise ValueError(\"Must run handle_imbalance before accessing class weights\")\n",
    "\n",
    "        return self.results['class_weights']\n",
    "\n",
    "    def _print_summary(self) -> None:\n",
    "        \"\"\"Print formatted summary of imbalance handling results.\"\"\"\n",
    "        self.logger.info(\"\\nImbalance Handling Summary\")\n",
    "        self.logger.info(\"-\" * 50)\n",
    "\n",
    "        # Print results for each method\n",
    "        for method, stats in self.results.items():\n",
    "            if method != 'class_weights':\n",
    "                self.logger.info(f\"\\n{method.upper()}:\")\n",
    "                self.logger.info(f\"Shape: {stats['shape']}\")\n",
    "                self.logger.info(\"Class distribution:\")\n",
    "                for cls, prop in stats['class_distribution'].items():\n",
    "                    self.logger.info(f\"- Class {cls}: {prop:.4%}\")\n",
    "\n",
    "        # Print class weights\n",
    "        self.logger.info(\"\\nClass Weights:\")\n",
    "        for cls, weight in self.results['class_weights'].items():\n",
    "            self.logger.info(f\"- Class {cls}: {weight:.4f}\")\n",
    "\n",
    "    def get_imbalance_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get imbalance handling statistics.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing imbalance handling statistics\n",
    "        \"\"\"\n",
    "        if not self.is_processed:\n",
    "            raise ValueError(\"Must run handle_imbalance before accessing statistics\")\n",
    "\n",
    "        return {\n",
    "            method: {\n",
    "                'shape': stats['shape'],\n",
    "                'class_distribution': stats['class_distribution']\n",
    "            }\n",
    "            for method, stats in self.results.items()\n",
    "            if method != 'class_weights'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ctVzt5DLfHd"
   },
   "source": [
    "# 3. Compile Preprocessing Pipeline\n",
    "\n",
    "All of the preprocessing classes or functions defined earlier will be compiled in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_ZlncSVjJG6"
   },
   "source": [
    "If you use sklearn to create preprocessing classes, you can list your preprocessing classes in the Pipeline object sequentially, and then fit and transform your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "jHraoW_7LfHd"
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Note: You can add or delete preprocessing components from this pipeline\n",
    "\n",
    "# pipe = Pipeline([(\"imputer\", FeatureImputer()),\n",
    "#                  (\"featurecreator\", FeatureCreator()),\n",
    "#                  (\"scaler\", FeatureScaler()),\n",
    "#                  (\"encoder\", FeatureEncoder())])\n",
    "\n",
    "# train_set = pipe.fit_transform(train_set)\n",
    "# val_set = pipe.transform(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9s56aFFxLfHd"
   },
   "outputs": [],
   "source": [
    "# # Your code should work up until this point\n",
    "# train_set = pipe.fit_transform(train_set)\n",
    "# val_set = pipe.transform(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXoCqMztjhr-"
   },
   "source": [
    "or create your own here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "rFn5x8VyPXQH"
   },
   "outputs": [],
   "source": [
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    \"\"\"\n",
    "    Custom JSON Encoder that can handle:\n",
    "    - FeatureStats objects\n",
    "    - NumPy numeric types\n",
    "    - NumPy arrays\n",
    "    - Sets\n",
    "    - Any other custom objects that need serialization\n",
    "    \"\"\"\n",
    "    def default(self, obj):\n",
    "        # Handle FeatureStats objects\n",
    "        if isinstance(obj, FeatureStats):\n",
    "            return {\n",
    "                'data_type': obj.data_type,\n",
    "                'distribution_type': obj.distribution_type,\n",
    "                'imputation_strategy': obj.imputation_strategy\n",
    "            }\n",
    "\n",
    "        # Handle NumPy numeric types\n",
    "        if isinstance(obj, (np.integer, np.floating)):\n",
    "            return float(obj)\n",
    "\n",
    "        # Handle NumPy arrays\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "\n",
    "        # Handle sets\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "\n",
    "        # Handle other NumPy types\n",
    "        if isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "\n",
    "        try:\n",
    "            # Let the base class handle anything else\n",
    "            return super().default(obj)\n",
    "        except TypeError:\n",
    "            # If all else fails, try converting to string\n",
    "            return str(obj)\n",
    "\n",
    "class DataSaver:\n",
    "    \"\"\"\n",
    "    Utility class for saving preprocessing results consistently and reliably.\n",
    "    Handles both intermediate and final results with validation and versioning.\n",
    "\n",
    "    Attributes:\n",
    "        base_dir (str): Base directory for saving files\n",
    "        verbose (bool): Whether to print detailed information\n",
    "        create_dirs (bool): Whether to create directories if they don't exist\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_dir: str = 'preprocessing_output',\n",
    "        verbose: bool = True,\n",
    "        create_dirs: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        [Previous implementation remains the same]\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir\n",
    "        self.verbose = verbose\n",
    "        self.create_dirs = create_dirs\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Create base directory if needed\n",
    "        if create_dirs and not os.path.exists(base_dir):\n",
    "            os.makedirs(base_dir)\n",
    "\n",
    "        # Track saved files\n",
    "        self.saved_files = {\n",
    "            'datasets': [],\n",
    "            'metadata': [],\n",
    "            'statistics': []\n",
    "        }\n",
    "\n",
    "        # Initialize custom JSON encoder\n",
    "        self.json_encoder = CustomJSONEncoder\n",
    "\n",
    "    def _validate_dataframe(self, df: pd.DataFrame, purpose: str) -> None:\n",
    "        \"\"\"\n",
    "        Validate DataFrame before saving.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "            purpose: Purpose of the DataFrame (for error messages)\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(f\"{purpose} must be a pandas DataFrame\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"{purpose} cannot be empty\")\n",
    "\n",
    "    def _create_directory(self, directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Create directory if it doesn't exist and is allowed.\n",
    "\n",
    "        Args:\n",
    "            directory: Directory path to create\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If directory creation is needed but not allowed\n",
    "        \"\"\"\n",
    "        if not os.path.exists(directory):\n",
    "            if self.create_dirs:\n",
    "                os.makedirs(directory)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Directory {directory} does not exist and create_dirs=False\"\n",
    "                )\n",
    "\n",
    "    def _generate_filename(\n",
    "        self,\n",
    "        base_name: str,\n",
    "        step: str,\n",
    "        extension: str\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate filename with timestamp.\n",
    "\n",
    "        Args:\n",
    "            base_name: Base filename\n",
    "            step: Processing step name\n",
    "            extension: File extension\n",
    "\n",
    "        Returns:\n",
    "            str: Generated filename\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        return f\"{base_name}_{step}_{timestamp}.{extension}\"\n",
    "\n",
    "    def save_dataset(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        filename: str,\n",
    "        step: str,\n",
    "        subdirectory: Optional[str] = None,\n",
    "        metadata: Optional[Dict] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        [Previous docstring remains the same]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            self._validate_dataframe(df, \"Dataset\")\n",
    "\n",
    "            # Prepare directory\n",
    "            save_dir = os.path.join(self.base_dir, subdirectory) if subdirectory else self.base_dir\n",
    "            self._create_directory(save_dir)\n",
    "\n",
    "            # Generate full filename\n",
    "            csv_filename = self._generate_filename(filename, step, \"csv\")\n",
    "            full_path = os.path.join(save_dir, csv_filename)\n",
    "\n",
    "            # Save DataFrame\n",
    "            df.to_csv(full_path, index=False)\n",
    "\n",
    "            # Save metadata if provided\n",
    "            if metadata:\n",
    "                metadata_filename = self._generate_filename(filename, f\"{step}_metadata\", \"json\")\n",
    "                metadata_path = os.path.join(save_dir, metadata_filename)\n",
    "\n",
    "                with open(metadata_path, 'w') as f:\n",
    "                    json.dump(metadata, f, indent=4, cls=self.json_encoder)\n",
    "\n",
    "                self.saved_files['metadata'].append(metadata_path)\n",
    "\n",
    "            # Track saved file\n",
    "            self.saved_files['datasets'].append(full_path)\n",
    "\n",
    "            if self.verbose:\n",
    "                self.logger.info(f\"Saved dataset to: {full_path}\")\n",
    "                if metadata:\n",
    "                    self.logger.info(f\"Saved metadata to: {metadata_path}\")\n",
    "\n",
    "            return full_path\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving dataset: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_intermediate_result(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        step: str,\n",
    "        metadata: Optional[Dict] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save intermediate processing result.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            step: Processing step name\n",
    "            metadata: Optional metadata dictionary\n",
    "\n",
    "        Returns:\n",
    "            str: Path to saved file\n",
    "        \"\"\"\n",
    "        return self.save_dataset(\n",
    "            df=df,\n",
    "            filename=\"intermediate\",\n",
    "            step=step,\n",
    "            subdirectory=\"intermediate\",\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "    def save_final_result(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        purpose: str = 'train',\n",
    "        metadata: Optional[Dict] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save final processing result.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            purpose: Purpose of the dataset ('train' or 'validation')\n",
    "            metadata: Optional metadata dictionary\n",
    "\n",
    "        Returns:\n",
    "            str: Path to saved file\n",
    "        \"\"\"\n",
    "        return self.save_dataset(\n",
    "            df=df,\n",
    "            filename=f\"final_{purpose}\",\n",
    "            step=\"final\",\n",
    "            subdirectory=\"final\",\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "    def save_statistics(\n",
    "        self,\n",
    "        statistics: Dict[str, Any],\n",
    "        step: str,\n",
    "        subdirectory: str = \"statistics\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save processing statistics.\n",
    "\n",
    "        Args:\n",
    "            statistics: Statistics dictionary to save\n",
    "            step: Processing step name\n",
    "            subdirectory: Subdirectory for statistics\n",
    "\n",
    "        Returns:\n",
    "            str: Path to saved file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare directory\n",
    "            save_dir = os.path.join(self.base_dir, subdirectory)\n",
    "            self._create_directory(save_dir)\n",
    "\n",
    "            # Generate filename\n",
    "            filename = self._generate_filename(\"statistics\", step, \"json\")\n",
    "            full_path = os.path.join(save_dir, filename)\n",
    "\n",
    "            # Save statistics\n",
    "            with open(full_path, 'w') as f:\n",
    "                json.dump(statistics, f, indent=4)\n",
    "\n",
    "            # Track saved file\n",
    "            self.saved_files['statistics'].append(full_path)\n",
    "\n",
    "            if self.verbose:\n",
    "                self.logger.info(f\"Saved statistics to: {full_path}\")\n",
    "\n",
    "            return full_path\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving statistics: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_saved_files(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Get dictionary of all saved files.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing paths to all saved files\n",
    "        \"\"\"\n",
    "        return self.saved_files\n",
    "\n",
    "    def validate_saved_files(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Validate that all saved files exist.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of invalid file paths\n",
    "        \"\"\"\n",
    "        invalid_files = {\n",
    "            'datasets': [],\n",
    "            'metadata': [],\n",
    "            'statistics': []\n",
    "        }\n",
    "\n",
    "        for category, files in self.saved_files.items():\n",
    "            for file_path in files:\n",
    "                if not os.path.exists(file_path):\n",
    "                    invalid_files[category].append(file_path)\n",
    "\n",
    "        return invalid_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dRWxxtZhPXQI"
   },
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"\n",
    "    Utility class for comprehensive data validation throughout the preprocessing pipeline.\n",
    "    Provides methods for validating data types, ranges, completeness, and consistency.\n",
    "\n",
    "    Attributes:\n",
    "        verbose (bool): Whether to print detailed information\n",
    "        raise_warnings (bool): Whether to raise warnings as exceptions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        verbose: bool = True,\n",
    "        raise_warnings: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DataValidator.\n",
    "\n",
    "        Args:\n",
    "            verbose: Whether to print details\n",
    "            raise_warnings: Whether to raise warnings as exceptions\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.raise_warnings = raise_warnings\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Define expected data types and ranges\n",
    "        self.expected_types = {\n",
    "            'URL': 'string',\n",
    "            'URLLength': 'float64',\n",
    "            'Domain': 'string',\n",
    "            'DomainLength': 'float64',\n",
    "            'IsDomainIP': 'bool',\n",
    "            'TLD': 'object',\n",
    "            'CharContinuationRate': 'float64',\n",
    "            'TLDLegitimateProb': 'float64',\n",
    "            'URLCharProb': 'float64',\n",
    "            'TLDLength': 'float64',\n",
    "            'NoOfSubDomain': 'float64',\n",
    "            'HasObfuscation': 'bool',\n",
    "            'NoOfObfuscatedChar': 'float64',\n",
    "            'ObfuscationRatio': 'float64',\n",
    "            'NoOfLettersInURL': 'float64',\n",
    "            'LetterRatioInURL': 'float64',\n",
    "            'NoOfDegitsInURL': 'float64',\n",
    "            'DegitRatioInURL': 'float64',\n",
    "            'NoOfEqualsInURL': 'float64',\n",
    "            'NoOfQMarkInURL': 'float64',\n",
    "            'NoOfAmpersandInURL': 'float64',\n",
    "            'NoOfOtherSpecialCharsInURL': 'float64',\n",
    "            'SpacialCharRatioInURL': 'float64',\n",
    "            'IsHTTPS': 'bool',\n",
    "            'LineOfCode': 'float64',\n",
    "            'LargestLineLength': 'float64',\n",
    "            'HasTitle': 'bool',\n",
    "            'DomainTitleMatchScore': 'float64',\n",
    "            'URLTitleMatchScore': 'float64',\n",
    "            'HasFavicon': 'bool',\n",
    "            'Robots': 'bool',\n",
    "            'IsResponsive': 'bool',\n",
    "            'NoOfURLRedirect': 'float64',\n",
    "            'NoOfSelfRedirect': 'float64',\n",
    "            'HasDescription': 'bool',\n",
    "            'NoOfPopup': 'float64',\n",
    "            'NoOfiFrame': 'float64',\n",
    "            'HasExternalFormSubmit': 'bool',\n",
    "            'HasSocialNet': 'bool',\n",
    "            'HasSubmitButton': 'bool',\n",
    "            'HasHiddenFields': 'bool',\n",
    "            'HasPasswordField': 'bool',\n",
    "            'Bank': 'bool',\n",
    "            'Pay': 'bool',\n",
    "            'Crypto': 'bool',\n",
    "            'HasCopyrightInfo': 'bool',\n",
    "            'NoOfImage': 'float64',\n",
    "            'NoOfCSS': 'float64',\n",
    "            'NoOfJS': 'float64',\n",
    "            'NoOfSelfRef': 'float64',\n",
    "            'NoOfEmptyRef': 'float64',\n",
    "            'NoOfExternalRef': 'float64',\n",
    "            'label': 'bool'\n",
    "        }\n",
    "\n",
    "        # Define expected value ranges for numeric features\n",
    "        self.value_ranges = {\n",
    "            'URLLength': (0, float('inf')),\n",
    "            'DomainLength': (0, float('inf')),\n",
    "            'TLDLength': (0, float('inf')),\n",
    "            'NoOfSubDomain': (0, float('inf'))\n",
    "        }\n",
    "\n",
    "        # Store validation results\n",
    "        self.validation_results = {}\n",
    "\n",
    "    def validate_data_types(self, df: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Validate data types of DataFrame columns.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of type mismatches\n",
    "        \"\"\"\n",
    "        type_mismatches = {}\n",
    "\n",
    "        for column, expected_type in self.expected_types.items():\n",
    "            if column in df.columns:\n",
    "                actual_type = str(df[column].dtype)\n",
    "                if actual_type != expected_type:\n",
    "                    type_mismatches[column] = {\n",
    "                        'expected': expected_type,\n",
    "                        'actual': actual_type\n",
    "                    }\n",
    "\n",
    "        self.validation_results['type_validation'] = {\n",
    "            'valid': len(type_mismatches) == 0,\n",
    "            'mismatches': type_mismatches\n",
    "        }\n",
    "\n",
    "        return type_mismatches\n",
    "\n",
    "    def validate_value_ranges(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Validate value ranges of numeric features.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of range violations\n",
    "        \"\"\"\n",
    "        range_violations = {}\n",
    "\n",
    "        for column, (min_val, max_val) in self.value_ranges.items():\n",
    "            if column in df.columns:\n",
    "                violations = df[\n",
    "                    (df[column] < min_val) |\n",
    "                    (df[column] > max_val)\n",
    "                ][column]\n",
    "\n",
    "                if len(violations) > 0:\n",
    "                    range_violations[column] = {\n",
    "                        'expected_range': (min_val, max_val),\n",
    "                        'violations': violations.to_dict()\n",
    "                    }\n",
    "\n",
    "        self.validation_results['range_validation'] = {\n",
    "            'valid': len(range_violations) == 0,\n",
    "            'violations': range_violations\n",
    "        }\n",
    "\n",
    "        return range_violations\n",
    "\n",
    "    def validate_completeness(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Validate data completeness.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of completeness metrics\n",
    "        \"\"\"\n",
    "        completeness_metrics = {}\n",
    "\n",
    "        for column in df.columns:\n",
    "            missing_count = df[column].isnull().sum()\n",
    "            missing_percentage = (missing_count / len(df)) * 100\n",
    "\n",
    "            completeness_metrics[column] = {\n",
    "                'missing_count': missing_count,\n",
    "                'missing_percentage': missing_percentage,\n",
    "                'complete': missing_count == 0\n",
    "            }\n",
    "\n",
    "        self.validation_results['completeness_validation'] = {\n",
    "            'valid': all(metrics['complete'] for metrics in completeness_metrics.values()),\n",
    "            'metrics': completeness_metrics\n",
    "        }\n",
    "\n",
    "        return completeness_metrics\n",
    "\n",
    "    def validate_consistency(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate data consistency.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of consistency checks\n",
    "        \"\"\"\n",
    "        consistency_checks = {}\n",
    "\n",
    "        # Check URL-related consistency\n",
    "        if all(col in df.columns for col in ['URL', 'Domain', 'TLD']):\n",
    "            consistency_checks['url_consistency'] = self._check_url_consistency(df)\n",
    "\n",
    "        # Check boolean features consistency\n",
    "        bool_features = [col for col, type_ in self.expected_types.items()\n",
    "                        if type_ == 'bool' and col in df.columns]\n",
    "        consistency_checks['boolean_consistency'] = self._check_boolean_consistency(df[bool_features])\n",
    "\n",
    "        self.validation_results['consistency_validation'] = {\n",
    "            'valid': all(check.get('valid', False) for check in consistency_checks.values()),\n",
    "            'checks': consistency_checks\n",
    "        }\n",
    "\n",
    "        return consistency_checks\n",
    "\n",
    "    def _check_url_consistency(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Check consistency of URL-related features.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to check\n",
    "\n",
    "        Returns:\n",
    "            dict: URL consistency check results\n",
    "        \"\"\"\n",
    "        inconsistencies = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            if pd.notna(row['URL']) and pd.notna(row['Domain']) and pd.notna(row['TLD']):\n",
    "                # Check if Domain is part of URL\n",
    "                if row['Domain'] not in row['URL']:\n",
    "                    inconsistencies.append({\n",
    "                        'index': idx,\n",
    "                        'issue': 'Domain not in URL',\n",
    "                        'url': row['URL'],\n",
    "                        'domain': row['Domain']\n",
    "                    })\n",
    "\n",
    "                # Check if TLD is part of Domain\n",
    "                if row['TLD'] not in row['Domain']:\n",
    "                    inconsistencies.append({\n",
    "                        'index': idx,\n",
    "                        'issue': 'TLD not in Domain',\n",
    "                        'domain': row['Domain'],\n",
    "                        'tld': row['TLD']\n",
    "                    })\n",
    "\n",
    "        return {\n",
    "            'valid': len(inconsistencies) == 0,\n",
    "            'inconsistencies': inconsistencies\n",
    "        }\n",
    "\n",
    "    def _check_boolean_consistency(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Check consistency of boolean features.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame of boolean features\n",
    "\n",
    "        Returns:\n",
    "            dict: Boolean consistency check results\n",
    "        \"\"\"\n",
    "        inconsistencies = {}\n",
    "\n",
    "        for column in df.columns:\n",
    "            unique_values = df[column].unique()\n",
    "            valid_values = {True, False, 0, 1, np.nan}\n",
    "            invalid_values = set(unique_values) - valid_values\n",
    "\n",
    "            if len(invalid_values) > 0:\n",
    "                inconsistencies[column] = {\n",
    "                    'invalid_values': list(invalid_values),\n",
    "                    'count': len(df[df[column].isin(invalid_values)])\n",
    "                }\n",
    "\n",
    "        return {\n",
    "            'valid': len(inconsistencies) == 0,\n",
    "            'inconsistencies': inconsistencies\n",
    "        }\n",
    "\n",
    "    def validate_all(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run all validation checks.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "\n",
    "        Returns:\n",
    "            dict: Complete validation results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Run all validations\n",
    "            type_mismatches = self.validate_data_types(df)\n",
    "            range_violations = self.validate_value_ranges(df)\n",
    "            completeness_metrics = self.validate_completeness(df)\n",
    "            consistency_checks = self.validate_consistency(df)\n",
    "\n",
    "            # Determine overall validation status\n",
    "            is_valid = all([\n",
    "                self.validation_results['type_validation']['valid'],\n",
    "                self.validation_results['range_validation']['valid'],\n",
    "                self.validation_results['completeness_validation']['valid'],\n",
    "                self.validation_results['consistency_validation']['valid']\n",
    "            ])\n",
    "\n",
    "            # Store timestamp\n",
    "            self.validation_results['timestamp'] = datetime.now().isoformat()\n",
    "            self.validation_results['overall_valid'] = is_valid\n",
    "\n",
    "            # Print summary if verbose\n",
    "            if self.verbose:\n",
    "                self._print_summary()\n",
    "\n",
    "            return self.validation_results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in validate_all: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _print_summary(self) -> None:\n",
    "        \"\"\"Print formatted summary of validation results.\"\"\"\n",
    "        self.logger.info(\"\\nData Validation Summary\")\n",
    "        self.logger.info(\"-\" * 50)\n",
    "\n",
    "        # Overall status\n",
    "        self.logger.info(f\"\\nOverall Status: {'Valid' if self.validation_results['overall_valid'] else 'Invalid'}\")\n",
    "\n",
    "        # Type validation\n",
    "        self.logger.info(\"\\nData Type Validation:\")\n",
    "        if self.validation_results['type_validation']['mismatches']:\n",
    "            for col, mismatch in self.validation_results['type_validation']['mismatches'].items():\n",
    "                self.logger.info(\n",
    "                    f\"- {col}: Expected {mismatch['expected']}, got {mismatch['actual']}\"\n",
    "                )\n",
    "        else:\n",
    "            self.logger.info(\"All data types valid\")\n",
    "\n",
    "        # Range validation\n",
    "        self.logger.info(\"\\nValue Range Validation:\")\n",
    "        if self.validation_results['range_validation']['violations']:\n",
    "            for col, violation in self.validation_results['range_validation']['violations'].items():\n",
    "                self.logger.info(\n",
    "                    f\"- {col}: {len(violation['violations'])} violations\"\n",
    "                )\n",
    "        else:\n",
    "            self.logger.info(\"All values within expected ranges\")\n",
    "\n",
    "        # Completeness validation\n",
    "        self.logger.info(\"\\nCompleteness Validation:\")\n",
    "        metrics = self.validation_results['completeness_validation']['metrics']\n",
    "        for col, metric in metrics.items():\n",
    "            if metric['missing_count'] > 0:\n",
    "                self.logger.info(\n",
    "                    f\"- {col}: {metric['missing_count']} missing \"\n",
    "                    f\"({metric['missing_percentage']:.2f}%)\"\n",
    "                )\n",
    "\n",
    "        # Consistency validation\n",
    "        self.logger.info(\"\\nConsistency Validation:\")\n",
    "        checks = self.validation_results['consistency_validation']['checks']\n",
    "        for check_name, results in checks.items():\n",
    "            if not results['valid']:\n",
    "                self.logger.info(f\"- {check_name}: Failed\")\n",
    "                if 'inconsistencies' in results:\n",
    "                    self.logger.info(f\"  {len(results['inconsistencies'])} inconsistencies found\")\n",
    "\n",
    "    def get_validation_results(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get complete validation results.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing all validation results\n",
    "        \"\"\"\n",
    "        return self.validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "u4XmLqPcPXQI"
   },
   "outputs": [],
   "source": [
    "class DataPreprocessingPipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline class to orchestrate the entire preprocessing workflow.\n",
    "    Coordinates all preprocessing components and manages the overall flow.\n",
    "\n",
    "    Attributes:\n",
    "        config (Dict): Configuration parameters for preprocessing steps\n",
    "        base_dir (str): Base directory for saving pipeline state\n",
    "        verbose (bool): Whether to print detailed information\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[Dict] = None,\n",
    "        base_dir: str = 'preprocessing_output',\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration dictionary for preprocessing parameters\n",
    "            base_dir: Base directory for saving pipeline state\n",
    "            verbose: Whether to print details\n",
    "        \"\"\"\n",
    "        self.config = config or {}\n",
    "        self.base_dir = base_dir\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO if verbose else logging.WARNING)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialize pipeline components\n",
    "        self.selected_features = []\n",
    "        self._initialize_components()\n",
    "\n",
    "        # Track pipeline state\n",
    "        self.is_fitted = False\n",
    "        self.pipeline_state = {\n",
    "            'fitted_on': None,\n",
    "            'steps_completed': [],\n",
    "            'current_shape': None,\n",
    "            'preprocessing_stats': {}\n",
    "        }\n",
    "\n",
    "    def _initialize_components(self) -> None:\n",
    "        \"\"\"Initialize all preprocessing components with configuration.\"\"\"\n",
    "        try:\n",
    "            # Initialize data handling components\n",
    "            self.splitter = DatasetSplitter(\n",
    "                test_size=self.config.get('test_size', 0.2),\n",
    "                target_col=self.config.get('target_col', 'label'),\n",
    "                random_state=self.config.get('random_state', 42),\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "\n",
    "            self.missing_handler = MissingValueHandler(\n",
    "                verbose=self.verbose,\n",
    "                random_state=self.config.get('random_state', 42)\n",
    "            )\n",
    "\n",
    "            self.outlier_processor = OutlierProcessor(\n",
    "                variance_threshold=self.config.get('variance_threshold', 0.1),\n",
    "                correlation_threshold=self.config.get('correlation_threshold', 0.8),\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "\n",
    "            self.duplicate_handler = DuplicateHandler(\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "\n",
    "            self.feature_engineer = FeatureEngineer(\n",
    "                variance_threshold=self.config.get('variance_threshold', 0.1),\n",
    "                max_categories=self.config.get('max_categories', 50),\n",
    "                p_value_threshold=self.config.get('p_value_threshold', 0.05),\n",
    "                correlation_threshold=self.config.get('correlation_threshold', 0.8),\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "\n",
    "            self.feature_scaler = FeatureScaler(\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "\n",
    "            self.feature_encoder = FeatureEncoder(\n",
    "                target_col=self.config.get('target_col', 'label'),\n",
    "                verbose=self.verbose,\n",
    "                random_state=self.config.get('random_state', 42)\n",
    "            )\n",
    "\n",
    "            self.imbalance_handler = ImbalanceHandler(\n",
    "                target_col=self.config.get('target_col', 'label'),\n",
    "                random_state=self.config.get('random_state', 42),\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "\n",
    "            # Initialize utility components\n",
    "            self.data_saver = DataSaver(\n",
    "                base_dir=self.base_dir,\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "\n",
    "            self.data_validator = DataValidator(\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing pipeline components: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_training_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Process training data through the complete pipeline with intermediate saves.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame for training\n",
    "\n",
    "        Returns:\n",
    "            tuple: (Processed training data, Processed validation data)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Store original columns for consistency checking\n",
    "            original_columns = set(df.columns)\n",
    "\n",
    "            # Save initial state\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                df, \"initial\",\n",
    "                metadata={\"original_columns\": list(original_columns)}\n",
    "            )\n",
    "\n",
    "            # Validate input data\n",
    "            self.logger.info(\"Validating input data...\")\n",
    "            validation_results = self.data_validator.validate_all(df)\n",
    "            if not validation_results['overall_valid']:\n",
    "                self.logger.warning(\"Input data validation revealed issues\")\n",
    "\n",
    "            # Split dataset\n",
    "            self.logger.info(\"Splitting dataset...\")\n",
    "            train_data, val_data = self.splitter.split_dataset(df)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                train_data, \"split_train\",\n",
    "                metadata=self.splitter.get_split_statistics()\n",
    "            )\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                val_data, \"split_val\",\n",
    "                metadata=self.splitter.get_split_statistics()\n",
    "            )\n",
    "\n",
    "            # Handle missing values\n",
    "            self.logger.info(\"Handling missing values...\")\n",
    "            train_data = self.missing_handler.impute_missing_values(train_data)\n",
    "            val_data = self.missing_handler.impute_missing_values(val_data)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                train_data, \"missing_handled_train\",\n",
    "                metadata=self.missing_handler.get_imputation_statistics()\n",
    "            )\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                val_data, \"missing_handled_val\"\n",
    "            )\n",
    "\n",
    "            # Process outliers\n",
    "            self.logger.info(\"Processing outliers...\")\n",
    "            train_data = self.outlier_processor.process_outliers(train_data)\n",
    "            val_data = self.outlier_processor.process_outliers(val_data)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                train_data, \"outliers_processed_train\",\n",
    "                metadata=self.outlier_processor.get_outlier_statistics()\n",
    "            )\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                val_data, \"outliers_processed_val\"\n",
    "            )\n",
    "\n",
    "            # Handle duplicates\n",
    "            self.logger.info(\"Handling duplicates...\")\n",
    "            train_data = self.duplicate_handler.process_duplicates(train_data)\n",
    "            val_data = self.duplicate_handler.process_duplicates(val_data)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                train_data, \"duplicates_handled_train\",\n",
    "                metadata=self.duplicate_handler.get_duplicate_statistics()\n",
    "            )\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                val_data, \"duplicates_handled_val\"\n",
    "            )\n",
    "\n",
    "            # Engineer features\n",
    "            self.logger.info(\"Engineering features...\")\n",
    "            train_data, engineering_results = self.feature_engineer.select_features(\n",
    "                train_data, self.config.get('target_col', 'label')\n",
    "            )\n",
    "            selected_features = engineering_results.get('selected_features', [])\n",
    "            if selected_features:\n",
    "                train_data = train_data[selected_features + [self.config.get('target_col', 'label')]]\n",
    "                val_data = val_data[selected_features + [self.config.get('target_col', 'label')]]\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                train_data, \"features_engineered_train\",\n",
    "                metadata=engineering_results\n",
    "            )\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                val_data, \"features_engineered_val\"\n",
    "            )\n",
    "\n",
    "            # Scale features\n",
    "            self.logger.info(\"Scaling features...\")\n",
    "            train_data = self.feature_scaler.fit_transform(train_data)\n",
    "            val_data = self.feature_scaler.transform(val_data)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                train_data, \"features_scaled_train\",\n",
    "                metadata=self.feature_scaler.get_scaling_statistics()\n",
    "            )\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                val_data, \"features_scaled_val\"\n",
    "            )\n",
    "\n",
    "            # Encode features\n",
    "            self.logger.info(\"Encoding features...\")\n",
    "            train_data = self.feature_encoder.fit_transform(train_data)\n",
    "            val_data = self.feature_encoder.transform(val_data)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                train_data, \"features_encoded_train\",\n",
    "                metadata=self.feature_encoder.get_encoding_statistics()\n",
    "            )\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                val_data, \"features_encoded_val\"\n",
    "            )\n",
    "\n",
    "            # Handle imbalance\n",
    "            self.logger.info(\"Handling class imbalance...\")\n",
    "            imbalance_results = self.imbalance_handler.handle_imbalance(train_data)\n",
    "            train_features = imbalance_results['smoteenn']['resampled_data']['X']\n",
    "            train_target = imbalance_results['smoteenn']['resampled_data']['y']\n",
    "\n",
    "            # Combine features and target back into a DataFrame\n",
    "            train_data = pd.DataFrame(train_features, columns=train_features.columns)\n",
    "            train_data[self.config.get('target_col', 'label')] = train_target\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                train_data, \"imbalance_handled_train\",\n",
    "                metadata=self.imbalance_handler.get_imbalance_statistics()\n",
    "            )\n",
    "\n",
    "            # Final results\n",
    "            self.logger.info(\"Saving final results...\")\n",
    "            self.data_saver.save_final_result(\n",
    "                train_data, 'train',\n",
    "                metadata={\"final_columns\": list(train_data.columns)}\n",
    "            )\n",
    "            self.data_saver.save_final_result(\n",
    "                val_data, 'validation',\n",
    "                metadata={\"final_columns\": list(val_data.columns)}\n",
    "            )\n",
    "\n",
    "            # Update pipeline state\n",
    "            self.selected_features = selected_features\n",
    "            self.is_fitted = True\n",
    "            self.pipeline_state.update({\n",
    "                'fitted_on': datetime.now().isoformat(),\n",
    "                'steps_completed': [\n",
    "                    'split', 'missing', 'outliers', 'duplicates',\n",
    "                    'engineering', 'scaling', 'encoding', 'imbalance'\n",
    "                ],\n",
    "                'current_shape': train_data.shape,\n",
    "                'final_columns': list(train_data.columns)\n",
    "            })\n",
    "\n",
    "            return train_data, val_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in process_training_data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_validation_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process validation/test data using fitted pipeline.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame for validation/test\n",
    "\n",
    "        Returns:\n",
    "            Processed DataFrame\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Pipeline must be fitted before processing validation data\")\n",
    "\n",
    "        try:\n",
    "            # Validate input data\n",
    "            self.logger.info(\"Validating input data...\")\n",
    "            validation_results = self.data_validator.validate_all(df)\n",
    "            if not validation_results['overall_valid']:\n",
    "                self.logger.warning(\"Input data validation revealed issues\")\n",
    "\n",
    "            # Process through pipeline components\n",
    "            processed_data = df.copy()\n",
    "\n",
    "            # Save initial state\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                processed_data, \"initial_test\"\n",
    "            )\n",
    "\n",
    "            # Handle missing values\n",
    "            processed_data = self.missing_handler.impute_missing_values(processed_data)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                processed_data, \"missing_handled_test\"\n",
    "            )\n",
    "\n",
    "            # Process outliers\n",
    "            processed_data = self.outlier_processor.process_outliers(processed_data)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                processed_data, \"outliers_processed_test\"\n",
    "            )\n",
    "\n",
    "            # Handle duplicates\n",
    "            processed_data = self.duplicate_handler.process_duplicates(processed_data)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                processed_data, \"duplicates_handled_test\"\n",
    "            )\n",
    "\n",
    "            # Engineer features\n",
    "            processed_data = self.feature_engineer.transform(processed_data, self.selected_features)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                processed_data, \"features_engineered_test\"\n",
    "            )\n",
    "\n",
    "            # Scale features\n",
    "            processed_data = self.feature_scaler.transform(processed_data)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                processed_data, \"features_scaled_test\"\n",
    "            )\n",
    "\n",
    "            # Encode features\n",
    "            processed_data = self.feature_encoder.transform(processed_data)\n",
    "            self.data_saver.save_intermediate_result(\n",
    "                processed_data, \"features_encoded_test\"\n",
    "            )\n",
    "\n",
    "            # Save results\n",
    "            self.data_saver.save_final_result(\n",
    "                processed_data,\n",
    "                'validation_external'\n",
    "            )\n",
    "\n",
    "            return processed_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in process_validation_data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_pipeline_state(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Save pipeline state to disk.\n",
    "\n",
    "        Args:\n",
    "            filepath: Path to save pipeline state\n",
    "        \"\"\"\n",
    "        try:\n",
    "            state = {\n",
    "                'pipeline_state': self.pipeline_state,\n",
    "                'selected_features': self.selected_features,\n",
    "                'components': {\n",
    "                    'missing_handler': self.missing_handler,\n",
    "                    'outlier_processor': self.outlier_processor,\n",
    "                    'duplicate_handler': self.duplicate_handler,\n",
    "                    'feature_engineer': self.feature_engineer,\n",
    "                    'feature_scaler': self.feature_scaler,\n",
    "                    'feature_encoder': self.feature_encoder,\n",
    "                    'imbalance_handler': self.imbalance_handler\n",
    "                },\n",
    "                'config': self.config\n",
    "            }\n",
    "\n",
    "            joblib.dump(state, filepath)\n",
    "            self.logger.info(f\"Pipeline state saved to: {filepath}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving pipeline state: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_pipeline_state(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Load pipeline state from disk.\n",
    "\n",
    "        Args:\n",
    "            filepath: Path to load pipeline state from\n",
    "        \"\"\"\n",
    "        try:\n",
    "            state = joblib.load(filepath)\n",
    "\n",
    "            self.pipeline_state = state['pipeline_state']\n",
    "            self.config = state['config']\n",
    "            self.selected_features = state['selected_features']\n",
    "\n",
    "            # Load component states\n",
    "            self.missing_handler = state['components']['missing_handler']\n",
    "            self.outlier_processor = state['components']['outlier_processor']\n",
    "            self.duplicate_handler = state['components']['duplicate_handler']\n",
    "            self.feature_engineer = state['components']['feature_engineer']\n",
    "            self.feature_scaler = state['components']['feature_scaler']\n",
    "            self.feature_encoder = state['components']['feature_encoder']\n",
    "            self.imbalance_handler = state['components']['imbalance_handler']\n",
    "\n",
    "            self.is_fitted = True\n",
    "            self.logger.info(f\"Pipeline state loaded from: {filepath}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading pipeline state: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_pipeline_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get complete pipeline statistics.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing all pipeline statistics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'pipeline_state': self.pipeline_state,\n",
    "            'component_statistics': {\n",
    "                'splitting': self.splitter.get_split_statistics(),\n",
    "                'missing': self.missing_handler.get_imputation_statistics(),\n",
    "                'outliers': self.outlier_processor.get_outlier_statistics(),\n",
    "                'duplicates': self.duplicate_handler.get_duplicate_statistics(),\n",
    "                'engineering': self.feature_engineer.get_feature_statistics(),\n",
    "                'scaling': self.feature_scaler.get_scaling_statistics(),\n",
    "                'encoding': self.feature_encoder.get_encoding_statistics(),\n",
    "                'imbalance': self.imbalance_handler.get_imbalance_statistics()\n",
    "            },\n",
    "            'saved_files': self.data_saver.get_saved_files(),\n",
    "            'validation_results': self.data_validator.get_validation_results()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_XIHXnWVPXQJ"
   },
   "outputs": [],
   "source": [
    "# Setup configuration\n",
    "config = {\n",
    "    'test_size': 0.2,\n",
    "    'target_col': 'label',\n",
    "    'random_state': 42,\n",
    "    'variance_threshold': 0.1,\n",
    "    'correlation_threshold': 0.8,\n",
    "    'max_categories': 50,\n",
    "    'p_value_threshold': 0.05,\n",
    "    'base_dir': 'preprocessing_output'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "4oCf2PtNPXQJ"
   },
   "outputs": [],
   "source": [
    "def correct_data_types(df, is_training):\n",
    "    \"\"\"Unified function to correct data types for both training and testing data\"\"\"\n",
    "    df_correct = df.copy()\n",
    "\n",
    "    # Base binary features\n",
    "    binary_features = [\n",
    "        'HasTitle', 'HasFavicon', 'HasDescription', 'HasSocialNet',\n",
    "        'HasSubmitButton', 'HasHiddenFields', 'HasPasswordField',\n",
    "        'HasCopyrightInfo', 'HasExternalFormSubmit', 'IsDomainIP',\n",
    "        'IsHTTPS', 'IsResponsive', 'HasObfuscation', 'Bank', 'Pay',\n",
    "        'Crypto', 'Robots'\n",
    "    ]\n",
    "\n",
    "    # Add label for training data\n",
    "    if is_training:\n",
    "        binary_features.append('label')\n",
    "\n",
    "    # Convert binary features to boolean\n",
    "    for col in binary_features:\n",
    "        if col in df_correct.columns:  # Check if column exists\n",
    "            df_correct[col] = df_correct[col].astype(\"bool\")\n",
    "\n",
    "    # Convert text features to string\n",
    "    text_features = ['URL', 'Domain']\n",
    "    for col in text_features:\n",
    "        if col in df_correct.columns:\n",
    "            df_correct[col] = df_correct[col].astype(\"string\")\n",
    "\n",
    "    return df_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eT4lJClQPXQK",
    "outputId": "691e14b2-4ab7-4caf-c207-b7895100d691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: \n",
      "(140404, 53)\n",
      "\n",
      "Column info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140404 entries, 0 to 140403\n",
      "Data columns (total 53 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   URL                         96917 non-null   string \n",
      " 1   URLLength                   79765 non-null   float64\n",
      " 2   Domain                      70207 non-null   string \n",
      " 3   DomainLength                94085 non-null   float64\n",
      " 4   IsDomainIP                  140404 non-null  bool   \n",
      " 5   TLD                         95005 non-null   object \n",
      " 6   CharContinuationRate        92362 non-null   float64\n",
      " 7   TLDLegitimateProb           87531 non-null   float64\n",
      " 8   URLCharProb                 88333 non-null   float64\n",
      " 9   TLDLength                   92673 non-null   float64\n",
      " 10  NoOfSubDomain               96344 non-null   float64\n",
      " 11  HasObfuscation              140404 non-null  bool   \n",
      " 12  NoOfObfuscatedChar          73606 non-null   float64\n",
      " 13  ObfuscationRatio            75806 non-null   float64\n",
      " 14  NoOfLettersInURL            77066 non-null   float64\n",
      " 15  LetterRatioInURL            74658 non-null   float64\n",
      " 16  NoOfDegitsInURL             81594 non-null   float64\n",
      " 17  DegitRatioInURL             86896 non-null   float64\n",
      " 18  NoOfEqualsInURL             78826 non-null   float64\n",
      " 19  NoOfQMarkInURL              96303 non-null   float64\n",
      " 20  NoOfAmpersandInURL          95017 non-null   float64\n",
      " 21  NoOfOtherSpecialCharsInURL  92775 non-null   float64\n",
      " 22  SpacialCharRatioInURL       77570 non-null   float64\n",
      " 23  IsHTTPS                     140404 non-null  bool   \n",
      " 24  LineOfCode                  71251 non-null   float64\n",
      " 25  LargestLineLength           72476 non-null   float64\n",
      " 26  HasTitle                    140404 non-null  bool   \n",
      " 27  DomainTitleMatchScore       90407 non-null   float64\n",
      " 28  URLTitleMatchScore          88188 non-null   float64\n",
      " 29  HasFavicon                  140404 non-null  bool   \n",
      " 30  Robots                      140404 non-null  bool   \n",
      " 31  IsResponsive                140404 non-null  bool   \n",
      " 32  NoOfURLRedirect             73020 non-null   float64\n",
      " 33  NoOfSelfRedirect            73689 non-null   float64\n",
      " 34  HasDescription              140404 non-null  bool   \n",
      " 35  NoOfPopup                   97051 non-null   float64\n",
      " 36  NoOfiFrame                  90460 non-null   float64\n",
      " 37  HasExternalFormSubmit       140404 non-null  bool   \n",
      " 38  HasSocialNet                140404 non-null  bool   \n",
      " 39  HasSubmitButton             140404 non-null  bool   \n",
      " 40  HasHiddenFields             140404 non-null  bool   \n",
      " 41  HasPasswordField            140404 non-null  bool   \n",
      " 42  Bank                        140404 non-null  bool   \n",
      " 43  Pay                         140404 non-null  bool   \n",
      " 44  Crypto                      140404 non-null  bool   \n",
      " 45  HasCopyrightInfo            140404 non-null  bool   \n",
      " 46  NoOfImage                   89932 non-null   float64\n",
      " 47  NoOfCSS                     73270 non-null   float64\n",
      " 48  NoOfJS                      79603 non-null   float64\n",
      " 49  NoOfSelfRef                 92272 non-null   float64\n",
      " 50  NoOfEmptyRef                97718 non-null   float64\n",
      " 51  NoOfExternalRef             71025 non-null   float64\n",
      " 52  label                       140404 non-null  bool   \n",
      "dtypes: bool(18), float64(32), object(1), string(2)\n",
      "memory usage: 39.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = DataPreprocessingPipeline(\n",
    "    config=config,\n",
    "    base_dir='preprocessing_output',\n",
    "    verbose=True\n",
    ")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load training data\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Modify dataset\n",
    "df = df.drop(['id', 'FILENAME', 'Title'], axis=1)\n",
    "\n",
    "# Apply type-correcting function\n",
    "df = correct_data_types(df, is_training=True)\n",
    "\n",
    "# Number of rows and columns\n",
    "print(f\"Dataset shape: \\n{df.shape}\")\n",
    "\n",
    "# Column names and their data types\n",
    "print(\"\\nColumn info:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lp8E1kjVPXQK",
    "outputId": "d39673d5-8b83-4352-c111-8844774054a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_initial_20241229_121223.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\intermediate\\intermediate_initial_metadata_20241229_121227.json\n",
      "INFO:__main__:Validating input data...\n",
      "INFO:__main__:\n",
      "Data Validation Summary\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Overall Status: Invalid\n",
      "INFO:__main__:\n",
      "Data Type Validation:\n",
      "INFO:__main__:All data types valid\n",
      "INFO:__main__:\n",
      "Value Range Validation:\n",
      "INFO:__main__:All values within expected ranges\n",
      "INFO:__main__:\n",
      "Completeness Validation:\n",
      "INFO:__main__:- URL: 43487 missing (30.97%)\n",
      "INFO:__main__:- URLLength: 60639 missing (43.19%)\n",
      "INFO:__main__:- Domain: 70197 missing (50.00%)\n",
      "INFO:__main__:- DomainLength: 46319 missing (32.99%)\n",
      "INFO:__main__:- TLD: 45399 missing (32.33%)\n",
      "INFO:__main__:- CharContinuationRate: 48042 missing (34.22%)\n",
      "INFO:__main__:- TLDLegitimateProb: 52873 missing (37.66%)\n",
      "INFO:__main__:- URLCharProb: 52071 missing (37.09%)\n",
      "INFO:__main__:- TLDLength: 47731 missing (34.00%)\n",
      "INFO:__main__:- NoOfSubDomain: 44060 missing (31.38%)\n",
      "INFO:__main__:- NoOfObfuscatedChar: 66798 missing (47.58%)\n",
      "INFO:__main__:- ObfuscationRatio: 64598 missing (46.01%)\n",
      "INFO:__main__:- NoOfLettersInURL: 63338 missing (45.11%)\n",
      "INFO:__main__:- LetterRatioInURL: 65746 missing (46.83%)\n",
      "INFO:__main__:- NoOfDegitsInURL: 58810 missing (41.89%)\n",
      "INFO:__main__:- DegitRatioInURL: 53508 missing (38.11%)\n",
      "INFO:__main__:- NoOfEqualsInURL: 61578 missing (43.86%)\n",
      "INFO:__main__:- NoOfQMarkInURL: 44101 missing (31.41%)\n",
      "INFO:__main__:- NoOfAmpersandInURL: 45387 missing (32.33%)\n",
      "INFO:__main__:- NoOfOtherSpecialCharsInURL: 47629 missing (33.92%)\n",
      "INFO:__main__:- SpacialCharRatioInURL: 62834 missing (44.75%)\n",
      "INFO:__main__:- LineOfCode: 69153 missing (49.25%)\n",
      "INFO:__main__:- LargestLineLength: 67928 missing (48.38%)\n",
      "INFO:__main__:- DomainTitleMatchScore: 49997 missing (35.61%)\n",
      "INFO:__main__:- URLTitleMatchScore: 52216 missing (37.19%)\n",
      "INFO:__main__:- NoOfURLRedirect: 67384 missing (47.99%)\n",
      "INFO:__main__:- NoOfSelfRedirect: 66715 missing (47.52%)\n",
      "INFO:__main__:- NoOfPopup: 43353 missing (30.88%)\n",
      "INFO:__main__:- NoOfiFrame: 49944 missing (35.57%)\n",
      "INFO:__main__:- NoOfImage: 50472 missing (35.95%)\n",
      "INFO:__main__:- NoOfCSS: 67134 missing (47.81%)\n",
      "INFO:__main__:- NoOfJS: 60801 missing (43.30%)\n",
      "INFO:__main__:- NoOfSelfRef: 48132 missing (34.28%)\n",
      "INFO:__main__:- NoOfEmptyRef: 42686 missing (30.40%)\n",
      "INFO:__main__:- NoOfExternalRef: 69379 missing (49.41%)\n",
      "INFO:__main__:\n",
      "Consistency Validation:\n",
      "WARNING:__main__:Input data validation revealed issues\n",
      "INFO:__main__:Splitting dataset...\n",
      "INFO:__main__:\n",
      "Dataset Split Summary\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Total samples: 140404\n",
      "INFO:__main__:Training set size: 77533 (55.22%)\n",
      "INFO:__main__:Validation set size: 19384 (13.81%)\n",
      "INFO:__main__:\n",
      "Class Distribution:\n",
      "INFO:__main__:Original:\n",
      "INFO:__main__:Class True: 92.48%\n",
      "INFO:__main__:Class False: 7.52%\n",
      "INFO:__main__:\n",
      "Training:\n",
      "INFO:__main__:Class True: 92.52%\n",
      "INFO:__main__:Class False: 7.48%\n",
      "INFO:__main__:\n",
      "Validation:\n",
      "INFO:__main__:Class True: 92.52%\n",
      "INFO:__main__:Class False: 7.48%\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_split_train_20241229_121238.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\intermediate\\intermediate_split_train_metadata_20241229_121240.json\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_split_val_20241229_121240.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\intermediate\\intermediate_split_val_metadata_20241229_121241.json\n",
      "INFO:__main__:Handling missing values...\n",
      "INFO:__main__:\n",
      "Missing Value Imputation Summary:\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Column: URLLength\n",
      "INFO:__main__:Initial missing: 33364\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 33364\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: Domain\n",
      "INFO:__main__:Initial missing: 38774\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 38774\n",
      "INFO:__main__:\n",
      "Column: DomainLength\n",
      "INFO:__main__:Initial missing: 25693\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 25693\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: TLD\n",
      "INFO:__main__:Initial missing: 25009\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 25009\n",
      "INFO:__main__:Strategy used: Mode Imputation\n",
      "INFO:__main__:\n",
      "Column: CharContinuationRate\n",
      "INFO:__main__:Initial missing: 26611\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 26611\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: TLDLegitimateProb\n",
      "INFO:__main__:Initial missing: 29082\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 29082\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: URLCharProb\n",
      "INFO:__main__:Initial missing: 28845\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 28845\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: TLDLength\n",
      "INFO:__main__:Initial missing: 26439\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 26439\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfSubDomain\n",
      "INFO:__main__:Initial missing: 24394\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 24394\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfObfuscatedChar\n",
      "INFO:__main__:Initial missing: 36882\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 36882\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: ObfuscationRatio\n",
      "INFO:__main__:Initial missing: 35542\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 35542\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfLettersInURL\n",
      "INFO:__main__:Initial missing: 34985\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 34985\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: LetterRatioInURL\n",
      "INFO:__main__:Initial missing: 36182\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 36182\n",
      "INFO:__main__:Strategy used: Mean Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfDegitsInURL\n",
      "INFO:__main__:Initial missing: 32432\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 32432\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: DegitRatioInURL\n",
      "INFO:__main__:Initial missing: 29618\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 29618\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfEqualsInURL\n",
      "INFO:__main__:Initial missing: 34110\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 34110\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfQMarkInURL\n",
      "INFO:__main__:Initial missing: 24267\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 24267\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfAmpersandInURL\n",
      "INFO:__main__:Initial missing: 24907\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 24907\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfOtherSpecialCharsInURL\n",
      "INFO:__main__:Initial missing: 26237\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 26237\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: SpacialCharRatioInURL\n",
      "INFO:__main__:Initial missing: 34793\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 34793\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: LineOfCode\n",
      "INFO:__main__:Initial missing: 38148\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 38148\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: LargestLineLength\n",
      "INFO:__main__:Initial missing: 37523\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 37523\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: DomainTitleMatchScore\n",
      "INFO:__main__:Initial missing: 27419\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 27419\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: URLTitleMatchScore\n",
      "INFO:__main__:Initial missing: 28927\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 28927\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfURLRedirect\n",
      "INFO:__main__:Initial missing: 37190\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 37190\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRedirect\n",
      "INFO:__main__:Initial missing: 36788\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 36788\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfPopup\n",
      "INFO:__main__:Initial missing: 23765\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 23765\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfiFrame\n",
      "INFO:__main__:Initial missing: 27524\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 27524\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfImage\n",
      "INFO:__main__:Initial missing: 27883\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 27883\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfCSS\n",
      "INFO:__main__:Initial missing: 36968\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 36968\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfJS\n",
      "INFO:__main__:Initial missing: 33349\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 33349\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRef\n",
      "INFO:__main__:Initial missing: 26680\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 26680\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfEmptyRef\n",
      "INFO:__main__:Initial missing: 23514\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 23514\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfExternalRef\n",
      "INFO:__main__:Initial missing: 38343\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 38343\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Missing Value Imputation Summary:\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Column: URLLength\n",
      "INFO:__main__:Initial missing: 8490\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 8490\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: Domain\n",
      "INFO:__main__:Initial missing: 9663\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 9663\n",
      "INFO:__main__:\n",
      "Column: DomainLength\n",
      "INFO:__main__:Initial missing: 6303\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6303\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: TLD\n",
      "INFO:__main__:Initial missing: 6302\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6302\n",
      "INFO:__main__:Strategy used: Mode Imputation\n",
      "INFO:__main__:\n",
      "Column: CharContinuationRate\n",
      "INFO:__main__:Initial missing: 6617\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6617\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: TLDLegitimateProb\n",
      "INFO:__main__:Initial missing: 7244\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 7244\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: URLCharProb\n",
      "INFO:__main__:Initial missing: 7083\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 7083\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: TLDLength\n",
      "INFO:__main__:Initial missing: 6485\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6485\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfSubDomain\n",
      "INFO:__main__:Initial missing: 6054\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6054\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfObfuscatedChar\n",
      "INFO:__main__:Initial missing: 9273\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 9273\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: ObfuscationRatio\n",
      "INFO:__main__:Initial missing: 8919\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 8919\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfLettersInURL\n",
      "INFO:__main__:Initial missing: 8675\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 8675\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: LetterRatioInURL\n",
      "INFO:__main__:Initial missing: 9092\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 9092\n",
      "INFO:__main__:Strategy used: Mean Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfDegitsInURL\n",
      "INFO:__main__:Initial missing: 8100\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 8100\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: DegitRatioInURL\n",
      "INFO:__main__:Initial missing: 7312\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 7312\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfEqualsInURL\n",
      "INFO:__main__:Initial missing: 8433\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 8433\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfQMarkInURL\n",
      "INFO:__main__:Initial missing: 6170\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6170\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfAmpersandInURL\n",
      "INFO:__main__:Initial missing: 6297\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6297\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfOtherSpecialCharsInURL\n",
      "INFO:__main__:Initial missing: 6557\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6557\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: SpacialCharRatioInURL\n",
      "INFO:__main__:Initial missing: 8626\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 8626\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: LineOfCode\n",
      "INFO:__main__:Initial missing: 9524\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 9524\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: LargestLineLength\n",
      "INFO:__main__:Initial missing: 9393\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 9393\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: DomainTitleMatchScore\n",
      "INFO:__main__:Initial missing: 6907\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6907\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: URLTitleMatchScore\n",
      "INFO:__main__:Initial missing: 7148\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 7148\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfURLRedirect\n",
      "INFO:__main__:Initial missing: 9228\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 9228\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRedirect\n",
      "INFO:__main__:Initial missing: 9278\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 9278\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfPopup\n",
      "INFO:__main__:Initial missing: 5975\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 5975\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfiFrame\n",
      "INFO:__main__:Initial missing: 6847\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6847\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfImage\n",
      "INFO:__main__:Initial missing: 6921\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6921\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfCSS\n",
      "INFO:__main__:Initial missing: 9310\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 9310\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfJS\n",
      "INFO:__main__:Initial missing: 8488\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 8488\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRef\n",
      "INFO:__main__:Initial missing: 6519\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 6519\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfEmptyRef\n",
      "INFO:__main__:Initial missing: 5916\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 5916\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfExternalRef\n",
      "INFO:__main__:Initial missing: 9554\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 9554\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_missing_handled_train_20241229_121444.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\intermediate\\intermediate_missing_handled_train_metadata_20241229_121449.json\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_missing_handled_val_20241229_121449.csv\n",
      "INFO:__main__:Processing outliers...\n",
      "INFO:__main__:\n",
      "Outlier Processing Summary\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Numerical Features:\n",
      "INFO:__main__:\n",
      "Column: URLLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 2289\n",
      "INFO:__main__:\n",
      "Column: DomainLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1187\n",
      "INFO:__main__:\n",
      "Column: CharContinuationRate\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 12887\n",
      "INFO:__main__:\n",
      "Column: TLDLegitimateProb\n",
      "INFO:__main__:Distribution: Moderately-Skewed\n",
      "INFO:__main__:Method used: iqr\n",
      "INFO:__main__:Outliers detected: 0\n",
      "INFO:__main__:\n",
      "Column: URLCharProb\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 10650\n",
      "INFO:__main__:\n",
      "Column: TLDLength\n",
      "INFO:__main__:Distribution: Moderately-Skewed\n",
      "INFO:__main__:Method used: iqr\n",
      "INFO:__main__:Outliers detected: 345\n",
      "INFO:__main__:\n",
      "Column: NoOfSubDomain\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 32846\n",
      "INFO:__main__:\n",
      "Column: NoOfObfuscatedChar\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 9\n",
      "INFO:__main__:\n",
      "Column: ObfuscationRatio\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 17\n",
      "INFO:__main__:\n",
      "Column: NoOfLettersInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 983\n",
      "INFO:__main__:\n",
      "Column: LetterRatioInURL\n",
      "INFO:__main__:Distribution: Normal\n",
      "INFO:__main__:Method used: zscore\n",
      "INFO:__main__:Outliers detected: 13\n",
      "INFO:__main__:\n",
      "Column: NoOfDegitsInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 4705\n",
      "INFO:__main__:\n",
      "Column: DegitRatioInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 4705\n",
      "INFO:__main__:\n",
      "Column: NoOfEqualsInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 166\n",
      "INFO:__main__:\n",
      "Column: NoOfQMarkInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 231\n",
      "INFO:__main__:\n",
      "Column: NoOfAmpersandInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 37\n",
      "INFO:__main__:\n",
      "Column: NoOfOtherSpecialCharsInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 13012\n",
      "INFO:__main__:\n",
      "Column: SpacialCharRatioInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 28219\n",
      "INFO:__main__:\n",
      "Column: LineOfCode\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 36131\n",
      "INFO:__main__:\n",
      "Column: LargestLineLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 34456\n",
      "INFO:__main__:\n",
      "Column: DomainTitleMatchScore\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 15534\n",
      "INFO:__main__:\n",
      "Column: URLTitleMatchScore\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 15063\n",
      "INFO:__main__:\n",
      "Column: NoOfURLRedirect\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 4877\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRedirect\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1144\n",
      "INFO:__main__:\n",
      "Column: NoOfPopup\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 4837\n",
      "INFO:__main__:\n",
      "Column: NoOfiFrame\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 4673\n",
      "INFO:__main__:\n",
      "Column: NoOfImage\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 9421\n",
      "INFO:__main__:\n",
      "Column: NoOfCSS\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 38101\n",
      "INFO:__main__:\n",
      "Column: NoOfJS\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 14823\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRef\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 7638\n",
      "INFO:__main__:\n",
      "Column: NoOfEmptyRef\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 25952\n",
      "INFO:__main__:\n",
      "Column: NoOfExternalRef\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 36515\n",
      "INFO:__main__:\n",
      "URL Patterns:\n",
      "INFO:__main__:char_substitution: 9168 occurrences\n",
      "INFO:__main__:url_encoding: 56 occurrences\n",
      "INFO:__main__:punycode: 6 occurrences\n",
      "INFO:__main__:double_encoding: 1 occurrences\n",
      "INFO:__main__:\n",
      "Average URL outlier ratio: 0.0934\n",
      "INFO:__main__:\n",
      "Outlier Processing Summary\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Numerical Features:\n",
      "INFO:__main__:\n",
      "Column: URLLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 378\n",
      "INFO:__main__:\n",
      "Column: DomainLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 315\n",
      "INFO:__main__:\n",
      "Column: CharContinuationRate\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 3200\n",
      "INFO:__main__:\n",
      "Column: TLDLegitimateProb\n",
      "INFO:__main__:Distribution: Moderately-Skewed\n",
      "INFO:__main__:Method used: iqr\n",
      "INFO:__main__:Outliers detected: 0\n",
      "INFO:__main__:\n",
      "Column: URLCharProb\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 2379\n",
      "INFO:__main__:\n",
      "Column: TLDLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 6096\n",
      "INFO:__main__:\n",
      "Column: NoOfSubDomain\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 8239\n",
      "INFO:__main__:\n",
      "Column: NoOfObfuscatedChar\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 5\n",
      "INFO:__main__:\n",
      "Column: ObfuscationRatio\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 6\n",
      "INFO:__main__:\n",
      "Column: NoOfLettersInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 242\n",
      "INFO:__main__:\n",
      "Column: LetterRatioInURL\n",
      "INFO:__main__:Distribution: Normal\n",
      "INFO:__main__:Method used: zscore\n",
      "INFO:__main__:Outliers detected: 3\n",
      "INFO:__main__:\n",
      "Column: NoOfDegitsInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1102\n",
      "INFO:__main__:\n",
      "Column: DegitRatioInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1103\n",
      "INFO:__main__:\n",
      "Column: NoOfEqualsInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 40\n",
      "INFO:__main__:\n",
      "Column: NoOfQMarkInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 65\n",
      "INFO:__main__:\n",
      "Column: NoOfAmpersandInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 7\n",
      "INFO:__main__:\n",
      "Column: NoOfOtherSpecialCharsInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 3249\n",
      "INFO:__main__:\n",
      "Column: SpacialCharRatioInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 7071\n",
      "INFO:__main__:\n",
      "Column: LineOfCode\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 9019\n",
      "INFO:__main__:\n",
      "Column: LargestLineLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 8721\n",
      "INFO:__main__:\n",
      "Column: DomainTitleMatchScore\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 3976\n",
      "INFO:__main__:\n",
      "Column: URLTitleMatchScore\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 3854\n",
      "INFO:__main__:\n",
      "Column: NoOfURLRedirect\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1255\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRedirect\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 298\n",
      "INFO:__main__:\n",
      "Column: NoOfPopup\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1200\n",
      "INFO:__main__:\n",
      "Column: NoOfiFrame\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1203\n",
      "INFO:__main__:\n",
      "Column: NoOfImage\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 2048\n",
      "INFO:__main__:\n",
      "Column: NoOfCSS\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 9527\n",
      "INFO:__main__:\n",
      "Column: NoOfJS\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 6543\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRef\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1809\n",
      "INFO:__main__:\n",
      "Column: NoOfEmptyRef\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 6553\n",
      "INFO:__main__:\n",
      "Column: NoOfExternalRef\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 9190\n",
      "INFO:__main__:\n",
      "URL Patterns:\n",
      "INFO:__main__:char_substitution: 2167 occurrences\n",
      "INFO:__main__:url_encoding: 9 occurrences\n",
      "INFO:__main__:punycode: 3 occurrences\n",
      "INFO:__main__:\n",
      "Average URL outlier ratio: 0.0984\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_outliers_processed_train_20241229_121455.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\intermediate\\intermediate_outliers_processed_train_metadata_20241229_121500.json\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_outliers_processed_val_20241229_121500.csv\n",
      "INFO:__main__:Handling duplicates...\n",
      "INFO:__main__:\n",
      "Duplicate Processing Summary\n",
      "INFO:__main__:------------------------------\n",
      "INFO:__main__:Initial number of rows: 77,533\n",
      "INFO:__main__:Exact duplicates removed: 0\n",
      "INFO:__main__:URL-based duplicates handled: 2\n",
      "INFO:__main__:Domain-based duplicates found: 427\n",
      "INFO:__main__:Total rows removed: 2\n",
      "INFO:__main__:Final number of rows: 77,531\n",
      "INFO:__main__:Percentage of data retained: 100.00%\n",
      "INFO:__main__:\n",
      "Duplicate Processing Summary\n",
      "INFO:__main__:------------------------------\n",
      "INFO:__main__:Initial number of rows: 19,384\n",
      "INFO:__main__:Exact duplicates removed: 0\n",
      "INFO:__main__:URL-based duplicates handled: 0\n",
      "INFO:__main__:Domain-based duplicates found: 78\n",
      "INFO:__main__:Total rows removed: 0\n",
      "INFO:__main__:Final number of rows: 19,384\n",
      "INFO:__main__:Percentage of data retained: 100.00%\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_duplicates_handled_train_20241229_121501.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\intermediate\\intermediate_duplicates_handled_train_metadata_20241229_121504.json\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_duplicates_handled_val_20241229_121504.csv\n",
      "INFO:__main__:Engineering features...\n",
      "INFO:__main__:Removed DomainLength due to high correlation with URLLength\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_features_engineered_train_20241229_121519.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\intermediate\\intermediate_features_engineered_train_metadata_20241229_121522.json\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_features_engineered_val_20241229_121522.csv\n",
      "INFO:__main__:Scaling features...\n",
      "WARNING:__main__:Column HasObfuscation was converted to numeric for scaling\n",
      "WARNING:__main__:Column HasSocialNet was converted to numeric for scaling\n",
      "WARNING:__main__:Column Pay was converted to numeric for scaling\n",
      "WARNING:__main__:Column HasDescription was converted to numeric for scaling\n",
      "WARNING:__main__:Column HasHiddenFields was converted to numeric for scaling\n",
      "WARNING:__main__:Column Robots was converted to numeric for scaling\n",
      "WARNING:__main__:Column HasExternalFormSubmit was converted to numeric for scaling\n",
      "WARNING:__main__:Column Crypto was converted to numeric for scaling\n",
      "WARNING:__main__:Column HasFavicon was converted to numeric for scaling\n",
      "INFO:__main__:\n",
      "Feature Scaling Summary\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Numerical Features:\n",
      "INFO:__main__:\n",
      "NoOfJS:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 13.1141\n",
      "INFO:__main__:- std: 5.7343\n",
      "INFO:__main__:- min: 2.6219\n",
      "INFO:__main__:- max: 23.3781\n",
      "INFO:__main__:- skew: 0.1320\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.8297\n",
      "INFO:__main__:- max: 1.7899\n",
      "INFO:__main__:- skew: 0.1320\n",
      "INFO:__main__:\n",
      "HasObfuscation:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.4700\n",
      "INFO:__main__:- std: 0.4991\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: 0.1201\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -0.9418\n",
      "INFO:__main__:- max: 1.0619\n",
      "INFO:__main__:- skew: 0.1201\n",
      "INFO:__main__:\n",
      "NoOfExternalRef:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 38.0126\n",
      "INFO:__main__:- std: 3.6094\n",
      "INFO:__main__:- min: 32.8110\n",
      "INFO:__main__:- max: 43.1890\n",
      "INFO:__main__:- skew: -0.0011\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.4411\n",
      "INFO:__main__:- max: 1.4342\n",
      "INFO:__main__:- skew: -0.0011\n",
      "INFO:__main__:\n",
      "NoOfImage:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 25.1774\n",
      "INFO:__main__:- std: 15.1332\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 54.1342\n",
      "INFO:__main__:- skew: 0.5411\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.6637\n",
      "INFO:__main__:- max: 1.9135\n",
      "INFO:__main__:- skew: 0.5411\n",
      "INFO:__main__:\n",
      "HasSocialNet:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.8634\n",
      "INFO:__main__:- std: 0.3435\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -2.1159\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -2.5137\n",
      "INFO:__main__:- max: 0.3978\n",
      "INFO:__main__:- skew: -2.1159\n",
      "INFO:__main__:\n",
      "LargestLineLength:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 2692.5894\n",
      "INFO:__main__:- std: 630.3278\n",
      "INFO:__main__:- min: 1765.5419\n",
      "INFO:__main__:- max: 3602.4581\n",
      "INFO:__main__:- skew: 0.0042\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.4707\n",
      "INFO:__main__:- max: 1.4435\n",
      "INFO:__main__:- skew: 0.0042\n",
      "INFO:__main__:\n",
      "Pay:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.5489\n",
      "INFO:__main__:- std: 0.4976\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -0.1964\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.1030\n",
      "INFO:__main__:- max: 0.9066\n",
      "INFO:__main__:- skew: -0.1964\n",
      "INFO:__main__:\n",
      "TLDLength:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 2.7258\n",
      "INFO:__main__:- std: 0.4909\n",
      "INFO:__main__:- min: 2.0000\n",
      "INFO:__main__:- max: 4.5000\n",
      "INFO:__main__:- skew: -0.3979\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.4786\n",
      "INFO:__main__:- max: 3.6146\n",
      "INFO:__main__:- skew: -0.3979\n",
      "INFO:__main__:\n",
      "HasDescription:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.8077\n",
      "INFO:__main__:- std: 0.3941\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -1.5617\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -2.0496\n",
      "INFO:__main__:- max: 0.4879\n",
      "INFO:__main__:- skew: -1.5617\n",
      "INFO:__main__:\n",
      "URLLength:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 27.1297\n",
      "INFO:__main__:- std: 5.3552\n",
      "INFO:__main__:- min: 15.0000\n",
      "INFO:__main__:- max: 41.5671\n",
      "INFO:__main__:- skew: 0.6773\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -2.2650\n",
      "INFO:__main__:- max: 2.6960\n",
      "INFO:__main__:- skew: 0.6773\n",
      "INFO:__main__:\n",
      "HasHiddenFields:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.6924\n",
      "INFO:__main__:- std: 0.4615\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -0.8336\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.5002\n",
      "INFO:__main__:- max: 0.6666\n",
      "INFO:__main__:- skew: -0.8336\n",
      "INFO:__main__:\n",
      "Robots:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.5932\n",
      "INFO:__main__:- std: 0.4912\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -0.3793\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.2075\n",
      "INFO:__main__:- max: 0.8282\n",
      "INFO:__main__:- skew: -0.3793\n",
      "INFO:__main__:\n",
      "HasExternalFormSubmit:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.4383\n",
      "INFO:__main__:- std: 0.4962\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: 0.2487\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -0.8834\n",
      "INFO:__main__:- max: 1.1320\n",
      "INFO:__main__:- skew: 0.2487\n",
      "INFO:__main__:\n",
      "Crypto:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.3803\n",
      "INFO:__main__:- std: 0.4855\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: 0.4932\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -0.7834\n",
      "INFO:__main__:- max: 1.2766\n",
      "INFO:__main__:- skew: 0.4932\n",
      "INFO:__main__:\n",
      "LineOfCode:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 1015.0957\n",
      "INFO:__main__:- std: 71.8831\n",
      "INFO:__main__:- min: 911.2194\n",
      "INFO:__main__:- max: 1118.7806\n",
      "INFO:__main__:- skew: 0.0004\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.4451\n",
      "INFO:__main__:- max: 1.4424\n",
      "INFO:__main__:- skew: 0.0004\n",
      "INFO:__main__:\n",
      "NoOfSelfRef:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 76.4250\n",
      "INFO:__main__:- std: 51.7413\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 183.1586\n",
      "INFO:__main__:- skew: 0.6753\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.4771\n",
      "INFO:__main__:- max: 2.0628\n",
      "INFO:__main__:- skew: 0.6753\n",
      "INFO:__main__:\n",
      "NoOfLettersInURL:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 17.5804\n",
      "INFO:__main__:- std: 7.3039\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 42.9451\n",
      "INFO:__main__:- skew: 0.6875\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -2.4070\n",
      "INFO:__main__:- max: 3.4728\n",
      "INFO:__main__:- skew: 0.6875\n",
      "INFO:__main__:\n",
      "HasFavicon:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.7272\n",
      "INFO:__main__:- std: 0.4454\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -1.0205\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.6329\n",
      "INFO:__main__:- max: 0.6124\n",
      "INFO:__main__:- skew: -1.0205\n",
      "INFO:__main__:\n",
      "Skewed Features:\n",
      "INFO:__main__:\n",
      "NoOfiFrame:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 1.3449\n",
      "INFO:__main__:- std: 1.6604\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 6.1890\n",
      "INFO:__main__:- skew: 1.8325\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: -0.0000\n",
      "INFO:__main__:- std: 1.0000\n",
      "INFO:__main__:- min: -1.2798\n",
      "INFO:__main__:- max: 1.8762\n",
      "INFO:__main__:- skew: 0.0846\n",
      "INFO:__main__:\n",
      "Boolean Features:\n",
      "INFO:__main__:\n",
      "HasPasswordField:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.5425\n",
      "INFO:__main__:- std: 0.4982\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -0.1705\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.5425\n",
      "INFO:__main__:- std: 0.4982\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -0.1705\n",
      "INFO:__main__:\n",
      "HasTitle:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.9832\n",
      "INFO:__main__:- std: 0.1284\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -7.5302\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.9832\n",
      "INFO:__main__:- std: 0.1284\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -7.5302\n",
      "INFO:__main__:\n",
      "IsResponsive:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.8695\n",
      "INFO:__main__:- std: 0.3368\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -2.1945\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.8695\n",
      "INFO:__main__:- std: 0.3368\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -2.1945\n",
      "INFO:__main__:\n",
      "HasSubmitButton:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.7858\n",
      "INFO:__main__:- std: 0.4103\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -1.3929\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.7858\n",
      "INFO:__main__:- std: 0.4103\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -1.3929\n",
      "INFO:__main__:\n",
      "Bank:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.4962\n",
      "INFO:__main__:- std: 0.5000\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: 0.0151\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.4962\n",
      "INFO:__main__:- std: 0.5000\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: 0.0151\n",
      "INFO:__main__:\n",
      "HasCopyrightInfo:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.8716\n",
      "INFO:__main__:- std: 0.3345\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -2.2220\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.8716\n",
      "INFO:__main__:- std: 0.3345\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -2.2220\n",
      "INFO:__main__:\n",
      "IsHTTPS:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.9755\n",
      "INFO:__main__:- std: 0.1547\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -6.1473\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.9755\n",
      "INFO:__main__:- std: 0.1547\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -6.1473\n",
      "INFO:__main__:\n",
      "label:\n",
      "INFO:__main__:Original statistics:\n",
      "INFO:__main__:- mean: 0.9253\n",
      "INFO:__main__:- std: 0.2630\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -3.2346\n",
      "INFO:__main__:Scaled statistics:\n",
      "INFO:__main__:- mean: 0.9253\n",
      "INFO:__main__:- std: 0.2630\n",
      "INFO:__main__:- min: 0.0000\n",
      "INFO:__main__:- max: 1.0000\n",
      "INFO:__main__:- skew: -3.2346\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_features_scaled_train_20241229_121525.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\intermediate\\intermediate_features_scaled_train_metadata_20241229_121530.json\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_features_scaled_val_20241229_121530.csv\n",
      "INFO:__main__:Encoding features...\n",
      "INFO:__main__:\n",
      "Feature Encoding Summary\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Boolean Features:\n",
      "INFO:__main__:\n",
      "IsHTTPS:\n",
      "INFO:__main__:- Original count: 77531\n",
      "INFO:__main__:- Missing values: 0\n",
      "INFO:__main__:- True ratio: 0.9755\n",
      "INFO:__main__:- Mode value: 1.0\n",
      "INFO:__main__:\n",
      "HasTitle:\n",
      "INFO:__main__:- Original count: 77531\n",
      "INFO:__main__:- Missing values: 0\n",
      "INFO:__main__:- True ratio: 0.9832\n",
      "INFO:__main__:- Mode value: 1.0\n",
      "INFO:__main__:\n",
      "IsResponsive:\n",
      "INFO:__main__:- Original count: 77531\n",
      "INFO:__main__:- Missing values: 0\n",
      "INFO:__main__:- True ratio: 0.8695\n",
      "INFO:__main__:- Mode value: 1.0\n",
      "INFO:__main__:\n",
      "HasSubmitButton:\n",
      "INFO:__main__:- Original count: 77531\n",
      "INFO:__main__:- Missing values: 0\n",
      "INFO:__main__:- True ratio: 0.7858\n",
      "INFO:__main__:- Mode value: 1.0\n",
      "INFO:__main__:\n",
      "HasPasswordField:\n",
      "INFO:__main__:- Original count: 77531\n",
      "INFO:__main__:- Missing values: 0\n",
      "INFO:__main__:- True ratio: 0.5425\n",
      "INFO:__main__:- Mode value: 1.0\n",
      "INFO:__main__:\n",
      "Bank:\n",
      "INFO:__main__:- Original count: 77531\n",
      "INFO:__main__:- Missing values: 0\n",
      "INFO:__main__:- True ratio: 0.4962\n",
      "INFO:__main__:- Mode value: 0.0\n",
      "INFO:__main__:\n",
      "HasCopyrightInfo:\n",
      "INFO:__main__:- Original count: 77531\n",
      "INFO:__main__:- Missing values: 0\n",
      "INFO:__main__:- True ratio: 0.8716\n",
      "INFO:__main__:- Mode value: 1.0\n",
      "INFO:__main__:\n",
      "label:\n",
      "INFO:__main__:- Original count: 77531\n",
      "INFO:__main__:- Missing values: 0\n",
      "INFO:__main__:- True ratio: 0.9253\n",
      "INFO:__main__:- Mode value: 1.0\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_features_encoded_train_20241229_121532.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\intermediate\\intermediate_features_encoded_train_metadata_20241229_121532.json\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_features_encoded_val_20241229_121532.csv\n",
      "INFO:__main__:Handling class imbalance...\n",
      "INFO:__main__:\n",
      "Applying SMOTE...\n",
      "INFO:__main__:Applying Random Undersampling...\n",
      "INFO:__main__:Applying SMOTEENN...\n",
      "INFO:__main__:Calculating class weights...\n",
      "INFO:__main__:\n",
      "Imbalance Handling Summary\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "ORIGINAL:\n",
      "INFO:__main__:Shape: (77531, 26)\n",
      "INFO:__main__:Class distribution:\n",
      "INFO:__main__:- Class 1: 92.5269%\n",
      "INFO:__main__:- Class 0: 7.4731%\n",
      "INFO:__main__:\n",
      "SMOTE:\n",
      "INFO:__main__:Shape: (143474, 26)\n",
      "INFO:__main__:Class distribution:\n",
      "INFO:__main__:- Class 1: 50.0000%\n",
      "INFO:__main__:- Class 0: 50.0000%\n",
      "INFO:__main__:\n",
      "UNDERSAMPLING:\n",
      "INFO:__main__:Shape: (11588, 26)\n",
      "INFO:__main__:Class distribution:\n",
      "INFO:__main__:- Class 0: 50.0000%\n",
      "INFO:__main__:- Class 1: 50.0000%\n",
      "INFO:__main__:\n",
      "SMOTEENN:\n",
      "INFO:__main__:Shape: (132894, 26)\n",
      "INFO:__main__:Class distribution:\n",
      "INFO:__main__:- Class 0: 52.0212%\n",
      "INFO:__main__:- Class 1: 47.9788%\n",
      "INFO:__main__:\n",
      "Class Weights:\n",
      "INFO:__main__:- Class 0: 6.6906\n",
      "INFO:__main__:- Class 1: 0.5404\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_imbalance_handled_train_20241229_122441.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\intermediate\\intermediate_imbalance_handled_train_metadata_20241229_122442.json\n",
      "INFO:__main__:Saving final results...\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\final\\final_train_final_20241229_122442.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\final\\final_train_final_metadata_20241229_122443.json\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\final\\final_validation_final_20241229_122443.csv\n",
      "INFO:__main__:Saved metadata to: preprocessing_output\\final\\final_validation_final_metadata_20241229_122443.json\n",
      "INFO:__main__:Pipeline state saved to: pipeline_state.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline Statistics:\n",
      "Current data shape: (132894, 27)\n",
      "Steps completed: ['split', 'missing', 'outliers', 'duplicates', 'engineering', 'scaling', 'encoding', 'imbalance']\n"
     ]
    }
   ],
   "source": [
    "# Process training data\n",
    "# This will automatically save intermediate results and create train/validation split\n",
    "train_data, val_data = pipeline.process_training_data(df)\n",
    "\n",
    "# Save pipeline state for later use\n",
    "pipeline.save_pipeline_state('pipeline_state.pkl')\n",
    "\n",
    "# Get pipeline statistics\n",
    "stats = pipeline.get_pipeline_statistics()\n",
    "print(\"\\nPipeline Statistics:\")\n",
    "print(f\"Current data shape: {stats['pipeline_state']['current_shape']}\")\n",
    "print(f\"Steps completed: {stats['pipeline_state']['steps_completed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmQIh-wbPXQK",
    "outputId": "a494d91d-fb47-4df3-aa7c-4e9fc4f4116c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pipeline state loaded from: pipeline_state.pkl\n",
      "INFO:__main__:Validating input data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: \n",
      "(10000, 52)\n",
      "\n",
      "Column info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 52 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   URL                         7920 non-null   string \n",
      " 1   URLLength                   7574 non-null   float64\n",
      " 2   Domain                      7815 non-null   string \n",
      " 3   DomainLength                7937 non-null   float64\n",
      " 4   IsDomainIP                  10000 non-null  bool   \n",
      " 5   TLD                         6067 non-null   object \n",
      " 6   CharContinuationRate        6055 non-null   float64\n",
      " 7   TLDLegitimateProb           6311 non-null   float64\n",
      " 8   URLCharProb                 6499 non-null   float64\n",
      " 9   TLDLength                   7025 non-null   float64\n",
      " 10  NoOfSubDomain               6321 non-null   float64\n",
      " 11  HasObfuscation              10000 non-null  bool   \n",
      " 12  NoOfObfuscatedChar          7733 non-null   float64\n",
      " 13  ObfuscationRatio            6546 non-null   float64\n",
      " 14  NoOfLettersInURL            7914 non-null   float64\n",
      " 15  LetterRatioInURL            6557 non-null   float64\n",
      " 16  NoOfDegitsInURL             7089 non-null   float64\n",
      " 17  DegitRatioInURL             7760 non-null   float64\n",
      " 18  NoOfEqualsInURL             6301 non-null   float64\n",
      " 19  NoOfQMarkInURL              6249 non-null   float64\n",
      " 20  NoOfAmpersandInURL          7459 non-null   float64\n",
      " 21  NoOfOtherSpecialCharsInURL  7154 non-null   float64\n",
      " 22  SpacialCharRatioInURL       7092 non-null   float64\n",
      " 23  IsHTTPS                     10000 non-null  bool   \n",
      " 24  LineOfCode                  7698 non-null   float64\n",
      " 25  LargestLineLength           6730 non-null   float64\n",
      " 26  HasTitle                    10000 non-null  bool   \n",
      " 27  DomainTitleMatchScore       6248 non-null   float64\n",
      " 28  URLTitleMatchScore          6495 non-null   float64\n",
      " 29  HasFavicon                  10000 non-null  bool   \n",
      " 30  Robots                      10000 non-null  bool   \n",
      " 31  IsResponsive                10000 non-null  bool   \n",
      " 32  NoOfURLRedirect             6838 non-null   float64\n",
      " 33  NoOfSelfRedirect            7989 non-null   float64\n",
      " 34  HasDescription              10000 non-null  bool   \n",
      " 35  NoOfPopup                   6984 non-null   float64\n",
      " 36  NoOfiFrame                  6868 non-null   float64\n",
      " 37  HasExternalFormSubmit       10000 non-null  bool   \n",
      " 38  HasSocialNet                10000 non-null  bool   \n",
      " 39  HasSubmitButton             10000 non-null  bool   \n",
      " 40  HasHiddenFields             10000 non-null  bool   \n",
      " 41  HasPasswordField            10000 non-null  bool   \n",
      " 42  Bank                        10000 non-null  bool   \n",
      " 43  Pay                         10000 non-null  bool   \n",
      " 44  Crypto                      10000 non-null  bool   \n",
      " 45  HasCopyrightInfo            10000 non-null  bool   \n",
      " 46  NoOfImage                   6379 non-null   float64\n",
      " 47  NoOfCSS                     6751 non-null   float64\n",
      " 48  NoOfJS                      6045 non-null   float64\n",
      " 49  NoOfSelfRef                 7286 non-null   float64\n",
      " 50  NoOfEmptyRef                7651 non-null   float64\n",
      " 51  NoOfExternalRef             7888 non-null   float64\n",
      "dtypes: bool(17), float64(32), object(1), string(2)\n",
      "memory usage: 2.8+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Data Validation Summary\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Overall Status: Invalid\n",
      "INFO:__main__:\n",
      "Data Type Validation:\n",
      "INFO:__main__:All data types valid\n",
      "INFO:__main__:\n",
      "Value Range Validation:\n",
      "INFO:__main__:All values within expected ranges\n",
      "INFO:__main__:\n",
      "Completeness Validation:\n",
      "INFO:__main__:- URL: 2080 missing (20.80%)\n",
      "INFO:__main__:- URLLength: 2426 missing (24.26%)\n",
      "INFO:__main__:- Domain: 2185 missing (21.85%)\n",
      "INFO:__main__:- DomainLength: 2063 missing (20.63%)\n",
      "INFO:__main__:- TLD: 3933 missing (39.33%)\n",
      "INFO:__main__:- CharContinuationRate: 3945 missing (39.45%)\n",
      "INFO:__main__:- TLDLegitimateProb: 3689 missing (36.89%)\n",
      "INFO:__main__:- URLCharProb: 3501 missing (35.01%)\n",
      "INFO:__main__:- TLDLength: 2975 missing (29.75%)\n",
      "INFO:__main__:- NoOfSubDomain: 3679 missing (36.79%)\n",
      "INFO:__main__:- NoOfObfuscatedChar: 2267 missing (22.67%)\n",
      "INFO:__main__:- ObfuscationRatio: 3454 missing (34.54%)\n",
      "INFO:__main__:- NoOfLettersInURL: 2086 missing (20.86%)\n",
      "INFO:__main__:- LetterRatioInURL: 3443 missing (34.43%)\n",
      "INFO:__main__:- NoOfDegitsInURL: 2911 missing (29.11%)\n",
      "INFO:__main__:- DegitRatioInURL: 2240 missing (22.40%)\n",
      "INFO:__main__:- NoOfEqualsInURL: 3699 missing (36.99%)\n",
      "INFO:__main__:- NoOfQMarkInURL: 3751 missing (37.51%)\n",
      "INFO:__main__:- NoOfAmpersandInURL: 2541 missing (25.41%)\n",
      "INFO:__main__:- NoOfOtherSpecialCharsInURL: 2846 missing (28.46%)\n",
      "INFO:__main__:- SpacialCharRatioInURL: 2908 missing (29.08%)\n",
      "INFO:__main__:- LineOfCode: 2302 missing (23.02%)\n",
      "INFO:__main__:- LargestLineLength: 3270 missing (32.70%)\n",
      "INFO:__main__:- DomainTitleMatchScore: 3752 missing (37.52%)\n",
      "INFO:__main__:- URLTitleMatchScore: 3505 missing (35.05%)\n",
      "INFO:__main__:- NoOfURLRedirect: 3162 missing (31.62%)\n",
      "INFO:__main__:- NoOfSelfRedirect: 2011 missing (20.11%)\n",
      "INFO:__main__:- NoOfPopup: 3016 missing (30.16%)\n",
      "INFO:__main__:- NoOfiFrame: 3132 missing (31.32%)\n",
      "INFO:__main__:- NoOfImage: 3621 missing (36.21%)\n",
      "INFO:__main__:- NoOfCSS: 3249 missing (32.49%)\n",
      "INFO:__main__:- NoOfJS: 3955 missing (39.55%)\n",
      "INFO:__main__:- NoOfSelfRef: 2714 missing (27.14%)\n",
      "INFO:__main__:- NoOfEmptyRef: 2349 missing (23.49%)\n",
      "INFO:__main__:- NoOfExternalRef: 2112 missing (21.12%)\n",
      "INFO:__main__:\n",
      "Consistency Validation:\n",
      "WARNING:__main__:Input data validation revealed issues\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_initial_test_20241229_122444.csv\n",
      "WARNING:__main__:Error imputing column TLD: 2\n",
      "INFO:__main__:\n",
      "Missing Value Imputation Summary:\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Column: URL\n",
      "INFO:__main__:Initial missing: 2080\n",
      "INFO:__main__:Final missing: 2080\n",
      "INFO:__main__:Values imputed: 0\n",
      "INFO:__main__:\n",
      "Column: URLLength\n",
      "INFO:__main__:Initial missing: 2426\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2426\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: Domain\n",
      "INFO:__main__:Initial missing: 2185\n",
      "INFO:__main__:Final missing: 447\n",
      "INFO:__main__:Values imputed: 1738\n",
      "INFO:__main__:\n",
      "Column: DomainLength\n",
      "INFO:__main__:Initial missing: 2063\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2063\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: TLD\n",
      "INFO:__main__:Initial missing: 3933\n",
      "INFO:__main__:Final missing: 805\n",
      "INFO:__main__:Values imputed: 3128\n",
      "INFO:__main__:Strategy used: Mode Imputation\n",
      "INFO:__main__:\n",
      "Column: CharContinuationRate\n",
      "INFO:__main__:Initial missing: 3945\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3945\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: TLDLegitimateProb\n",
      "INFO:__main__:Initial missing: 3689\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3689\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: URLCharProb\n",
      "INFO:__main__:Initial missing: 3501\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3501\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: TLDLength\n",
      "INFO:__main__:Initial missing: 2975\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2975\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfSubDomain\n",
      "INFO:__main__:Initial missing: 3679\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3679\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfObfuscatedChar\n",
      "INFO:__main__:Initial missing: 2267\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2267\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: ObfuscationRatio\n",
      "INFO:__main__:Initial missing: 3454\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3454\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfLettersInURL\n",
      "INFO:__main__:Initial missing: 2086\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2086\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: LetterRatioInURL\n",
      "INFO:__main__:Initial missing: 3443\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3443\n",
      "INFO:__main__:Strategy used: Mean Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfDegitsInURL\n",
      "INFO:__main__:Initial missing: 2911\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2911\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: DegitRatioInURL\n",
      "INFO:__main__:Initial missing: 2240\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2240\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfEqualsInURL\n",
      "INFO:__main__:Initial missing: 3699\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3699\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfQMarkInURL\n",
      "INFO:__main__:Initial missing: 3751\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3751\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfAmpersandInURL\n",
      "INFO:__main__:Initial missing: 2541\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2541\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfOtherSpecialCharsInURL\n",
      "INFO:__main__:Initial missing: 2846\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2846\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: SpacialCharRatioInURL\n",
      "INFO:__main__:Initial missing: 2908\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2908\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: LineOfCode\n",
      "INFO:__main__:Initial missing: 2302\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2302\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: LargestLineLength\n",
      "INFO:__main__:Initial missing: 3270\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3270\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: DomainTitleMatchScore\n",
      "INFO:__main__:Initial missing: 3752\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3752\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: URLTitleMatchScore\n",
      "INFO:__main__:Initial missing: 3505\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3505\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfURLRedirect\n",
      "INFO:__main__:Initial missing: 3162\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3162\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRedirect\n",
      "INFO:__main__:Initial missing: 2011\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2011\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfPopup\n",
      "INFO:__main__:Initial missing: 3016\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3016\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfiFrame\n",
      "INFO:__main__:Initial missing: 3132\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3132\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfImage\n",
      "INFO:__main__:Initial missing: 3621\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3621\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfCSS\n",
      "INFO:__main__:Initial missing: 3249\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3249\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfJS\n",
      "INFO:__main__:Initial missing: 3955\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 3955\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRef\n",
      "INFO:__main__:Initial missing: 2714\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2714\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfEmptyRef\n",
      "INFO:__main__:Initial missing: 2349\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2349\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:\n",
      "Column: NoOfExternalRef\n",
      "INFO:__main__:Initial missing: 2112\n",
      "INFO:__main__:Final missing: 0\n",
      "INFO:__main__:Values imputed: 2112\n",
      "INFO:__main__:Strategy used: Median Imputation\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_missing_handled_test_20241229_122449.csv\n",
      "INFO:__main__:\n",
      "Outlier Processing Summary\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "Numerical Features:\n",
      "INFO:__main__:\n",
      "Column: URLLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 939\n",
      "INFO:__main__:\n",
      "Column: DomainLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 441\n",
      "INFO:__main__:\n",
      "Column: CharContinuationRate\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 2659\n",
      "INFO:__main__:\n",
      "Column: TLDLegitimateProb\n",
      "INFO:__main__:Distribution: Moderately-Skewed\n",
      "INFO:__main__:Method used: iqr\n",
      "INFO:__main__:Outliers detected: 0\n",
      "INFO:__main__:\n",
      "Column: URLCharProb\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1120\n",
      "INFO:__main__:\n",
      "Column: TLDLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 3151\n",
      "INFO:__main__:\n",
      "Column: NoOfSubDomain\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 4328\n",
      "INFO:__main__:\n",
      "Column: NoOfObfuscatedChar\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 18\n",
      "INFO:__main__:\n",
      "Column: ObfuscationRatio\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 12\n",
      "INFO:__main__:\n",
      "Column: NoOfLettersInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 757\n",
      "INFO:__main__:\n",
      "Column: LetterRatioInURL\n",
      "INFO:__main__:Distribution: Normal\n",
      "INFO:__main__:Method used: zscore\n",
      "INFO:__main__:Outliers detected: 12\n",
      "INFO:__main__:\n",
      "Column: NoOfDegitsInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 2324\n",
      "INFO:__main__:\n",
      "Column: DegitRatioInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 2371\n",
      "INFO:__main__:\n",
      "Column: NoOfEqualsInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 177\n",
      "INFO:__main__:\n",
      "Column: NoOfQMarkInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 182\n",
      "INFO:__main__:\n",
      "Column: NoOfAmpersandInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 32\n",
      "INFO:__main__:\n",
      "Column: NoOfOtherSpecialCharsInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 3410\n",
      "INFO:__main__:\n",
      "Column: SpacialCharRatioInURL\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 974\n",
      "INFO:__main__:\n",
      "Column: LineOfCode\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1305\n",
      "INFO:__main__:\n",
      "Column: LargestLineLength\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 2091\n",
      "INFO:__main__:\n",
      "Column: DomainTitleMatchScore\n",
      "INFO:__main__:Distribution: Moderately-Skewed\n",
      "INFO:__main__:Method used: iqr\n",
      "INFO:__main__:Outliers detected: 0\n",
      "INFO:__main__:\n",
      "Column: URLTitleMatchScore\n",
      "INFO:__main__:Distribution: Moderately-Skewed\n",
      "INFO:__main__:Method used: iqr\n",
      "INFO:__main__:Outliers detected: 0\n",
      "INFO:__main__:\n",
      "Column: NoOfURLRedirect\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 887\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRedirect\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 357\n",
      "INFO:__main__:\n",
      "Column: NoOfPopup\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 309\n",
      "INFO:__main__:\n",
      "Column: NoOfiFrame\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 2043\n",
      "INFO:__main__:\n",
      "Column: NoOfImage\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1674\n",
      "INFO:__main__:\n",
      "Column: NoOfCSS\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 1647\n",
      "INFO:__main__:\n",
      "Column: NoOfJS\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 989\n",
      "INFO:__main__:\n",
      "Column: NoOfSelfRef\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 3173\n",
      "INFO:__main__:\n",
      "Column: NoOfEmptyRef\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 2051\n",
      "INFO:__main__:\n",
      "Column: NoOfExternalRef\n",
      "INFO:__main__:Distribution: Highly-Skewed\n",
      "INFO:__main__:Method used: modified_zscore\n",
      "INFO:__main__:Outliers detected: 2565\n",
      "INFO:__main__:\n",
      "URL Patterns:\n",
      "INFO:__main__:char_substitution: 4760 occurrences\n",
      "INFO:__main__:url_encoding: 26 occurrences\n",
      "INFO:__main__:punycode: 7 occurrences\n",
      "INFO:__main__:\n",
      "Average URL outlier ratio: 0.1129\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_outliers_processed_test_20241229_122450.csv\n",
      "INFO:__main__:\n",
      "Duplicate Processing Summary\n",
      "INFO:__main__:------------------------------\n",
      "INFO:__main__:Initial number of rows: 10,000\n",
      "INFO:__main__:Exact duplicates removed: 0\n",
      "INFO:__main__:URL-based duplicates handled: 0\n",
      "INFO:__main__:Domain-based duplicates found: 312\n",
      "INFO:__main__:Total rows removed: 0\n",
      "INFO:__main__:Final number of rows: 10,000\n",
      "INFO:__main__:Percentage of data retained: 100.00%\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_duplicates_handled_test_20241229_122450.csv\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_features_engineered_test_20241229_122451.csv\n",
      "WARNING:__main__:Missing features in transform input: {'label'}\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_features_scaled_test_20241229_122451.csv\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\intermediate\\intermediate_features_encoded_test_20241229_122451.csv\n",
      "INFO:__main__:Saved dataset to: preprocessing_output\\final\\final_validation_external_final_20241229_122451.csv\n"
     ]
    }
   ],
   "source": [
    "# Process testing data\n",
    "# Load Kaggle test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "test_ids = test_df['id'].copy()\n",
    "\n",
    "# Modify dataset\n",
    "test_df = test_df.drop(['id', 'FILENAME', 'Title'], axis=1)\n",
    "\n",
    "# Apply type-correcting function\n",
    "test_df = correct_data_types(test_df, is_training=False)\n",
    "\n",
    "# Number of rows and columns\n",
    "print(f\"Dataset shape: \\n{test_df.shape}\")\n",
    "\n",
    "# Column names and their data types\n",
    "print(\"\\nColumn info:\")\n",
    "print(test_df.info())\n",
    "\n",
    "# Load saved pipeline state\n",
    "new_pipeline = DataPreprocessingPipeline(config=config)\n",
    "new_pipeline.load_pipeline_state('pipeline_state.pkl')\n",
    "warnings.filterwarnings('ignore')\n",
    "processed_test = new_pipeline.process_validation_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A3adbZXLfHe"
   },
   "source": [
    "# 4. Modeling and Validation\n",
    "\n",
    "Modelling is the process of building your own machine learning models to solve specific problems, or in this assignment context, predicting the target feature `label`. Validation is the process of evaluating your trained model using the validation set or cross-validation method and providing some metrics that can help you decide what to do in the next iteration of development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnhMNbBILfHf"
   },
   "source": [
    "## A. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "KV6ICmFmlqjk"
   },
   "outputs": [],
   "source": [
    "class KNeighborsClassifierScratch:\n",
    "    def __init__(self, n_neighbors=5, metric='euclidean', p=2, weights='uniform'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.p = p\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X_num, X_cat, y):\n",
    "        self.X_num_train = X_num\n",
    "        self.X_cat_train = X_cat\n",
    "        self.y_train = y\n",
    "        return self\n",
    "\n",
    "    def _calculate_numerical_distances(self, x_test):\n",
    "        if self.metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((self.X_num_train - x_test) ** 2, axis=1))\n",
    "        elif self.metric == 'manhattan':\n",
    "            return np.sum(np.abs(self.X_num_train - x_test), axis=1)\n",
    "        elif self.metric == 'minkowski':\n",
    "            return np.sum(np.abs(self.X_num_train - x_test) ** self.p, axis=1) ** (1 / self.p)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported metric for numerical features\")\n",
    "\n",
    "    def _calculate_categorical_distances(self, x_test_cat):\n",
    "        # Check if categorical training or test data is None\n",
    "        if self.X_cat_train is None or x_test_cat is None or self.X_cat_train.shape[1] == 0:\n",
    "            return np.zeros(self.X_num_train.shape[0])  # Return zero distances if no categorical data\n",
    "\n",
    "        # Calculate Hamming distance for categorical features\n",
    "        return np.sum(self.X_cat_train != x_test_cat, axis=1) / self.X_cat_train.shape[1]\n",
    "\n",
    "    def _predict_single(self, x_test_num, x_test_cat):\n",
    "        num_distances = self._calculate_numerical_distances(x_test_num)\n",
    "        cat_distances = self._calculate_categorical_distances(x_test_cat)\n",
    "        distances = num_distances + cat_distances\n",
    "\n",
    "        neighbors_indices = np.argsort(distances)[:self.n_neighbors]\n",
    "        neighbors_labels = self.y_train[neighbors_indices]\n",
    "\n",
    "        if self.weights == 'uniform':\n",
    "            return np.argmax(np.bincount(neighbors_labels))\n",
    "        elif self.weights == 'distance':\n",
    "            weights = 1 / (distances[neighbors_indices] + 1e-5)  # Avoid division by zero\n",
    "            weighted_votes = np.bincount(neighbors_labels, weights=weights)\n",
    "            return np.argmax(weighted_votes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported weights\")\n",
    "\n",
    "    def predict(self, X_num, X_cat=None):\n",
    "        if X_cat is None:  # Handle case with no categorical data\n",
    "            return np.array([self._predict_single(x_num, None) for x_num in X_num])\n",
    "        else:\n",
    "            return np.array([self._predict_single(x_num, x_cat) for x_num, x_cat in zip(X_num, X_cat)])\n",
    "\n",
    "    def score(self, X_num, X_cat, y):\n",
    "        y_pred = self.predict(X_num, X_cat)\n",
    "        return np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIrXJxpDRfd5",
    "outputId": "7552641e-c83e-4e63-8bbc-811892c308bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time Comparison:\n",
      "----------------------------------------\n",
      "Scratch Implementation Training Time: 0.0000 seconds\n",
      "Sklearn Implementation Training Time: 0.0100 seconds\n",
      "\n",
      "Prediction Time Comparison:\n",
      "----------------------------------------\n",
      "Scratch Implementation Prediction Time: 478.5996 seconds\n",
      "Sklearn Implementation Prediction Time: 63.7495 seconds\n",
      "Results for Scratch Implementation:\n",
      "----------------------------------------\n",
      "Accuracy: 0.921172\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.88      0.63      1449\n",
      "           1       0.99      0.92      0.96     17935\n",
      "\n",
      "    accuracy                           0.92     19384\n",
      "   macro avg       0.74      0.90      0.79     19384\n",
      "weighted avg       0.95      0.92      0.93     19384\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1274   175]\n",
      " [ 1353 16582]]\n",
      "\n",
      "\n",
      "Results for Sklearn Implementation:\n",
      "----------------------------------------\n",
      "Accuracy: 0.922204\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.88      0.63      1449\n",
      "           1       0.99      0.93      0.96     17935\n",
      "\n",
      "    accuracy                           0.92     19384\n",
      "   macro avg       0.74      0.90      0.79     19384\n",
      "weighted avg       0.95      0.92      0.93     19384\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1271   178]\n",
      " [ 1330 16605]]\n",
      "\n",
      "Model Comparison:\n",
      "----------------------------------------\n",
      "Number of different predictions: 480\n",
      "Percentage of different predictions: 2.48%\n",
      "\n",
      "Time Performance Summary:\n",
      "----------------------------------------\n",
      "Total Scratch Implementation Time: 478.5996 seconds\n",
      "Total Sklearn Implementation Time: 63.7596 seconds\n",
      "Speed Improvement using Sklearn: 7.51x\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "# Redefine binary features for the dataset\n",
    "binary_features = [\n",
    "    'NoOfiFrame', 'NoOfSelfRef', 'NoOfImage',\n",
    "    'NoOfLettersInURL', 'HasDescription',\n",
    "    'LineOfCode', 'URLLength', 'NoOfJS', 'HasExternalFormSubmit',\n",
    "    'HasObfuscation', 'NoOfExternalRef', 'HasSocialNet', 'IsResponsive',\n",
    "    'HasSubmitButton', 'HasTitle', 'IsHTTPS'\n",
    "]\n",
    "\n",
    "train_normalized = train_data.copy()\n",
    "val_normalized = val_data.copy()\n",
    "\n",
    "# Normalize train_data column names\n",
    "train_normalized.columns = train_normalized.columns.str.strip().str.lower()\n",
    "val_normalized.columns = val_normalized.columns.str.strip().str.lower()\n",
    "\n",
    "# Normalize binary features\n",
    "binary_features_normalized = [col.lower() for col in binary_features]\n",
    "\n",
    "# Prepare training data\n",
    "X_train = train_normalized[binary_features_normalized].values\n",
    "y_train = train_normalized['label'].values\n",
    "\n",
    "# Prepare validation data\n",
    "X_test = val_normalized[binary_features_normalized].values\n",
    "y_test = val_normalized['label'].values\n",
    "\n",
    "# Define the KNN class\n",
    "class KNeighborsClassifierScratch:\n",
    "    def __init__(self, n_neighbors=5, metric='euclidean', p=2, weights='uniform'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.p = p\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X_num, X_cat, y):\n",
    "        self.X_num_train = X_num\n",
    "        self.X_cat_train = X_cat\n",
    "        self.y_train = y\n",
    "        return self\n",
    "\n",
    "    def _calculate_numerical_distances(self, x_test):\n",
    "        if self.metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((self.X_num_train - x_test) ** 2, axis=1))\n",
    "        elif self.metric == 'manhattan':\n",
    "            return np.sum(np.abs(self.X_num_train - x_test), axis=1)\n",
    "        elif self.metric == 'minkowski':\n",
    "            return np.sum(np.abs(self.X_num_train - x_test) ** self.p, axis=1) ** (1 / self.p)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported metric for numerical features\")\n",
    "\n",
    "    def _predict_single(self, x_test_num, x_test_cat):\n",
    "        num_distances = self._calculate_numerical_distances(x_test_num)\n",
    "        distances = num_distances  # No categorical features, so directly use numerical distances\n",
    "\n",
    "        neighbors_indices = np.argsort(distances)[:self.n_neighbors]\n",
    "        neighbors_labels = self.y_train[neighbors_indices]\n",
    "\n",
    "        if self.weights == 'uniform':\n",
    "            return np.argmax(np.bincount(neighbors_labels))\n",
    "        elif self.weights == 'distance':\n",
    "            weights = 1 / (distances[neighbors_indices] + 1e-5)\n",
    "            weighted_votes = np.bincount(neighbors_labels, weights=weights)\n",
    "            return np.argmax(weighted_votes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported weights\")\n",
    "\n",
    "    def predict(self, X_num, X_cat=None):\n",
    "        return np.array([self._predict_single(x_num, None) for x_num in X_num])\n",
    "\n",
    "# Time comparison for training\n",
    "print(\"Training Time Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Scratch implementation timing\n",
    "start_time = time.time()\n",
    "knn_scratch = KNeighborsClassifierScratch(n_neighbors=5, metric='euclidean', weights='uniform')\n",
    "knn_scratch.fit(X_train, None, y_train)  # X_cat set to None\n",
    "scratch_train_time = time.time() - start_time\n",
    "print(f\"Scratch Implementation Training Time: {scratch_train_time:.4f} seconds\")\n",
    "\n",
    "# Sklearn implementation timing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "start_time = time.time()\n",
    "knn_sklearn = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='uniform')\n",
    "knn_sklearn.fit(X_train, y_train)\n",
    "sklearn_train_time = time.time() - start_time\n",
    "print(f\"Sklearn Implementation Training Time: {sklearn_train_time:.4f} seconds\")\n",
    "\n",
    "# Time comparison for prediction\n",
    "print(\"\\nPrediction Time Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Scratch implementation timing\n",
    "start_time = time.time()\n",
    "y_pred_scratch = knn_scratch.predict(X_test)\n",
    "scratch_pred_time = time.time() - start_time\n",
    "print(f\"Scratch Implementation Prediction Time: {scratch_pred_time:.4f} seconds\")\n",
    "\n",
    "# Sklearn implementation timing\n",
    "start_time = time.time()\n",
    "y_pred_sklearn = knn_sklearn.predict(X_test)\n",
    "sklearn_pred_time = time.time() - start_time\n",
    "print(f\"Sklearn Implementation Prediction Time: {sklearn_pred_time:.4f} seconds\")\n",
    "\n",
    "# Calculate accuracies\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "# Print results for scratch implementation\n",
    "print(\"Results for Scratch Implementation:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Accuracy: {accuracy_scratch:.6f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_scratch))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_scratch))\n",
    "\n",
    "# Print results for sklearn implementation\n",
    "print(\"\\n\\nResults for Sklearn Implementation:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Accuracy: {accuracy_sklearn:.6f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_sklearn))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_sklearn))\n",
    "\n",
    "# Compare predictions\n",
    "disagreements = np.sum(y_pred_scratch != y_pred_sklearn)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of different predictions: {disagreements}\")\n",
    "print(f\"Percentage of different predictions: {(disagreements / len(y_test)) * 100:.2f}%\")\n",
    "\n",
    "# Add time comparison summary\n",
    "print(\"\\nTime Performance Summary:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Scratch Implementation Time: {scratch_train_time + scratch_pred_time:.4f} seconds\")\n",
    "print(f\"Total Sklearn Implementation Time: {sklearn_train_time + sklearn_pred_time:.4f} seconds\")\n",
    "print(f\"Speed Improvement using Sklearn: {((scratch_train_time + scratch_pred_time) / (sklearn_train_time + sklearn_pred_time)):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW0bMzkDLfHf"
   },
   "source": [
    "## B. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "C_XwsN_-LfHg"
   },
   "outputs": [],
   "source": [
    "class BernoulliNB_Scratch(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, alpha=1.0):  # alpha for Laplace smoothing\n",
    "        self.alpha = alpha\n",
    "        self.classes_ = None\n",
    "        self.class_priors_ = None\n",
    "        self.feature_probs_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "\n",
    "        # Calculate class priors (prior[c])\n",
    "        self.class_priors_ = np.zeros(n_classes)\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            self.class_priors_[i] = np.sum(y == c) / n_samples\n",
    "\n",
    "        # Calculate conditional probabilities for each feature\n",
    "        self.feature_probs_ = np.zeros((n_classes, n_features))\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            X_c = X[y == c]\n",
    "            Nc = X_c.shape[0]\n",
    "\n",
    "            # Add Laplace smoothing (alpha)\n",
    "            Nct = X_c.sum(axis=0) + self.alpha\n",
    "            self.feature_probs_[i] = Nct / (Nc + 2 * self.alpha)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "\n",
    "        # Calculate scores for each class\n",
    "        scores = np.zeros((n_samples, n_classes))\n",
    "\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            # Start with log of class prior\n",
    "            scores[:, i] = np.log(self.class_priors_[i])\n",
    "\n",
    "            # Add log probabilities for each feature\n",
    "            # If feature present (1), add log(condprob)\n",
    "            # If feature absent (0), add log(1-condprob)\n",
    "            feature_probs = self.feature_probs_[i]\n",
    "            scores[:, i] += np.sum(\n",
    "                X * np.log(feature_probs) +\n",
    "                (1 - X) * np.log(1 - feature_probs),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrhhNa-cPXQS",
    "outputId": "b5899a18-ede7-4db0-de6b-a81a167f2ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time Comparison:\n",
      "----------------------------------------\n",
      "Scratch Implementation Training Time: 0.1872 seconds\n",
      "Sklearn Implementation Training Time: 0.2208 seconds\n",
      "\n",
      "Prediction Time Comparison:\n",
      "----------------------------------------\n",
      "Scratch Implementation Prediction Time: 0.1302 seconds\n",
      "Sklearn Implementation Prediction Time: 0.0792 seconds\n",
      "Results for Scratch Implementation:\n",
      "----------------------------------------\n",
      "Accuracy: 0.895378\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.91      0.56      1449\n",
      "           1       0.99      0.89      0.94     17935\n",
      "\n",
      "    accuracy                           0.90     19384\n",
      "   macro avg       0.70      0.90      0.75     19384\n",
      "weighted avg       0.95      0.90      0.91     19384\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1316   133]\n",
      " [ 1895 16040]]\n",
      "\n",
      "Probability Distribution:\n",
      "Mean probability for class 0: -17.37285174793647\n",
      "Mean probability for class 1: -8.311798230281203\n",
      "\n",
      "\n",
      "Results for Sklearn Implementation:\n",
      "----------------------------------------\n",
      "Accuracy: 0.895378\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.91      0.56      1449\n",
      "           1       0.99      0.89      0.94     17935\n",
      "\n",
      "    accuracy                           0.90     19384\n",
      "   macro avg       0.70      0.90      0.75     19384\n",
      "weighted avg       0.95      0.90      0.91     19384\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1316   133]\n",
      " [ 1895 16040]]\n",
      "\n",
      "Probability Distribution:\n",
      "Mean probability for class 0: 0.17542731227248715\n",
      "Mean probability for class 1: 0.8245726877275129\n",
      "\n",
      "Model Comparison:\n",
      "----------------------------------------\n",
      "Number of different predictions: 0\n",
      "Percentage of different predictions: 0.00%\n",
      "\n",
      "Time Performance Summary:\n",
      "----------------------------------------\n",
      "Total Scratch Implementation Time: 0.3173 seconds\n",
      "Total Sklearn Implementation Time: 0.3000 seconds\n",
      "Speed Improvement using Sklearn: 1.06x\n"
     ]
    }
   ],
   "source": [
    "# Define binary features\n",
    "binary_features_all = ['NoOfiFrame', 'NoOfSelfRef', 'NoOfImage', 'HasFavicon',\n",
    "       'HasExternalFormSubmit', 'NoOfLettersInURL', 'Pay', 'HasDescription',\n",
    "       'TLDLength', 'Crypto', 'LineOfCode', 'Robots', 'URLLength',\n",
    "       'HasHiddenFields', 'LargestLineLength', 'HasObfuscation', 'NoOfJS',\n",
    "       'NoOfExternalRef', 'HasSocialNet', 'HasPasswordField', 'IsResponsive',\n",
    "       'HasSubmitButton', 'HasTitle', 'Bank', 'HasCopyrightInfo', 'IsHTTPS',]\n",
    "\n",
    "binary_features = ['NoOfiFrame', 'NoOfSelfRef', 'NoOfImage',\n",
    "        'NoOfLettersInURL', 'HasDescription',\n",
    "        'LineOfCode', 'URLLength','NoOfJS', 'HasExternalFormSubmit',\n",
    "       'HasObfuscation',\n",
    "       'NoOfExternalRef', 'HasSocialNet', 'IsResponsive',\n",
    "       'HasSubmitButton', 'HasTitle', 'IsHTTPS']\n",
    "\n",
    "# Prepare training data (ensuring column order)\n",
    "X_train = train_data[binary_features]\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "# Prepare validation data (ensuring same column order as training)\n",
    "X_test = val_data[binary_features]\n",
    "y_test = val_data['label'].values\n",
    "\n",
    "# Time comparison for training\n",
    "print(\"Training Time Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Scratch implementation timing\n",
    "start_time = time.time()\n",
    "model_scratch = BernoulliNB_Scratch(alpha=0.0001)\n",
    "model_scratch.fit(X_train, y_train)\n",
    "scratch_train_time = time.time() - start_time\n",
    "print(f\"Scratch Implementation Training Time: {scratch_train_time:.4f} seconds\")\n",
    "\n",
    "# Sklearn implementation timing\n",
    "start_time = time.time()\n",
    "model_sklearn = BernoulliNB(alpha=0.0001)\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "sklearn_train_time = time.time() - start_time\n",
    "print(f\"Sklearn Implementation Training Time: {sklearn_train_time:.4f} seconds\")\n",
    "\n",
    "# Time comparison for prediction\n",
    "print(\"\\nPrediction Time Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Scratch implementation timing\n",
    "start_time = time.time()\n",
    "y_pred_scratch = model_scratch.predict(X_test)\n",
    "y_pred_proba_scratch = model_scratch.predict_proba(X_test)\n",
    "scratch_pred_time = time.time() - start_time\n",
    "print(f\"Scratch Implementation Prediction Time: {scratch_pred_time:.4f} seconds\")\n",
    "\n",
    "# Sklearn implementation timing\n",
    "start_time = time.time()\n",
    "y_pred_sklearn = model_sklearn.predict(X_test)\n",
    "y_pred_proba_sklearn = model_sklearn.predict_proba(X_test)\n",
    "sklearn_pred_time = time.time() - start_time\n",
    "print(f\"Sklearn Implementation Prediction Time: {sklearn_pred_time:.4f} seconds\")\n",
    "\n",
    "# Calculate accuracies\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "# Print results for scratch implementation\n",
    "print(\"Results for Scratch Implementation:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Accuracy: {accuracy_scratch:.6f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_scratch))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_scratch))\n",
    "print(\"\\nProbability Distribution:\")\n",
    "print(\"Mean probability for class 0:\", np.mean(y_pred_proba_scratch[:, 0]))\n",
    "print(\"Mean probability for class 1:\", np.mean(y_pred_proba_scratch[:, 1]))\n",
    "\n",
    "# Print results for sklearn implementation\n",
    "print(\"\\n\\nResults for Sklearn Implementation:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Accuracy: {accuracy_sklearn:.6f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_sklearn))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_sklearn))\n",
    "print(\"\\nProbability Distribution:\")\n",
    "print(\"Mean probability for class 0:\", np.mean(y_pred_proba_sklearn[:, 0]))\n",
    "print(\"Mean probability for class 1:\", np.mean(y_pred_proba_sklearn[:, 1]))\n",
    "\n",
    "# Compare predictions\n",
    "disagreements = np.sum(y_pred_scratch != y_pred_sklearn)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of different predictions: {disagreements}\")\n",
    "print(f\"Percentage of different predictions: {(disagreements/len(y_test))*100:.2f}%\")\n",
    "\n",
    "# Add time comparison summary\n",
    "print(\"\\nTime Performance Summary:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Scratch Implementation Time: {scratch_train_time + scratch_pred_time:.4f} seconds\")\n",
    "print(f\"Total Sklearn Implementation Time: {sklearn_train_time + sklearn_pred_time:.4f} seconds\")\n",
    "print(f\"Speed Improvement using Sklearn: {((scratch_train_time + scratch_pred_time)/(sklearn_train_time + sklearn_pred_time)):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoH2u6fOLfHh"
   },
   "source": [
    "## C. Improvements (Optional)\n",
    "\n",
    "- **Visualize the model evaluation result**\n",
    "\n",
    "This will help you to understand the details more clearly about your model's performance. From the visualization, you can see clearly if your model is leaning towards a class than the others. (Hint: confusion matrix, ROC-AUC curve, etc.)\n",
    "\n",
    "- **Explore the hyperparameters of your models**\n",
    "\n",
    "Each models have their own hyperparameters. And each of the hyperparameter have different effects on the model behaviour. You can optimize the model performance by finding the good set of hyperparameters through a process called **hyperparameter tuning**. (Hint: Grid search, random search, bayesian optimization)\n",
    "\n",
    "- **Cross-validation**\n",
    "\n",
    "Cross-validation is a critical technique in machine learning and data science for evaluating and validating the performance of predictive models. It provides a more **robust** and **reliable** evaluation method compared to a hold-out (single train-test set) validation. Though, it requires more time and computing power because of how cross-validation works. (Hint: k-fold cross-validation, stratified k-fold cross-validation, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 757
    },
    "id": "DDXdj7jyahLU",
    "outputId": "cbc423c5-3204-4825-f2f3-65455ea8d937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "Randomized Search Results for KNN:\n",
      "Best parameters: {'weights': 'uniform', 'n_neighbors': 9, 'metric': 'euclidean'}\n",
      "Best cross-validation score: nan\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.89      0.61      1449\n",
      "           1       0.99      0.92      0.95     17935\n",
      "\n",
      "    accuracy                           0.92     19384\n",
      "   macro avg       0.73      0.90      0.78     19384\n",
      "weighted avg       0.95      0.92      0.93     19384\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1284   165]\n",
      " [ 1447 16488]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def tune_knn(X_train, y_train, cv=3, n_iter=10):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for KNN using RandomizedSearchCV.\n",
    "    \"\"\"\n",
    "    # Define the KNN model\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Define the hyperparameter space\n",
    "    param_distributions = {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'metric': ['euclidean', 'manhattan'],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "\n",
    "    # Create RandomizedSearchCV object\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        knn,\n",
    "        param_distributions,\n",
    "        n_iter=n_iter,  # Limit number of iterations\n",
    "        cv=cv,  # Cross-validation folds\n",
    "        scoring='accuracy',\n",
    "        return_train_score=True,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Perform randomized search\n",
    "    randomized_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nRandomized Search Results for KNN:\")\n",
    "    print(f\"Best parameters: {randomized_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {randomized_search.best_score_:.4f}\")\n",
    "\n",
    "    return randomized_search.best_estimator_\n",
    "\n",
    "def train_and_evaluate_knn_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the KNN model and visualize the results.\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Print confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Visualize confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# Tuning hyperparameters with a fast method\n",
    "best_knn_model = tune_knn(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "train_and_evaluate_knn_model(best_knn_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pg-A54yELfHh",
    "outputId": "296a0f3a-ed97-45df-a489-35d7a91c3631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 19 candidates, totalling 95 fits\n",
      "\n",
      "Grid Search Results:\n",
      "Best parameters: {'classifier__alpha': 0.001}\n",
      "Best cross-validation score: 0.9524\n",
      "\n",
      "Detailed Cross-validation Results:\n",
      "   alpha  mean_cv_score  std_cv_score\n",
      "       0       0.520212      0.000015\n",
      "   0.001       0.952391      0.002629\n",
      "     0.1       0.952391      0.002629\n",
      "     0.5       0.952391      0.002629\n",
      "     1.0       0.952391      0.002629\n",
      "     5.0       0.952383      0.002623\n",
      "    10.0       0.952376      0.002618\n",
      "   100.0       0.952248      0.002604\n",
      "   200.0       0.951954      0.002662\n",
      "   500.0       0.950005      0.002069\n",
      "   750.0       0.950088      0.001887\n",
      "   900.0       0.950336      0.002207\n",
      "  1000.0       0.950156      0.002064\n",
      "  1100.0       0.950028      0.002012\n",
      "  1250.0       0.949810      0.002003\n",
      "  1500.0       0.949795      0.002126\n",
      "  2500.0       0.949020      0.002103\n",
      "  5000.0       0.947048      0.002897\n",
      " 10000.0       0.944753      0.002818\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.91      0.56      1449\n",
      "           1       0.99      0.89      0.94     17935\n",
      "\n",
      "    accuracy                           0.90     19384\n",
      "   macro avg       0.70      0.90      0.75     19384\n",
      "weighted avg       0.95      0.90      0.91     19384\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1316   133]\n",
      " [ 1895 16040]]\n"
     ]
    }
   ],
   "source": [
    "def tune_bernoulli_nb(X_train, y_train, cv=5):\n",
    "    # Create the pipeline with imputer and classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('classifier', BernoulliNB_Scratch())\n",
    "    ])\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'classifier__alpha': [0, 0.001, 0.1, 0.5, 1.0, 5.0, 10.0, 100.0, 200.0, 500.0, 750.0, 900.0, 1000.0, 1100.0, 1250.0, 1500.0, 2500.0, 5000.0, 10000.0]\n",
    "    }\n",
    "\n",
    "    # Create GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        return_train_score=True,\n",
    "        verbose=1,\n",
    "        n_jobs=-1  # Use all available CPU cores\n",
    "    )\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nGrid Search Results:\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Create a summary of all results\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    cv_results = cv_results[['param_classifier__alpha', 'mean_test_score', 'std_test_score']]\n",
    "    cv_results.columns = ['alpha', 'mean_cv_score', 'std_cv_score']\n",
    "\n",
    "    print(\"\\nDetailed Cross-validation Results:\")\n",
    "    print(cv_results.to_string(index=False))\n",
    "\n",
    "    return grid_search.best_estimator_, cv_results\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train the model using grid search and evaluate on test set.\n",
    "    \"\"\"\n",
    "    # Perform grid search\n",
    "    best_model, cv_results = tune_bernoulli_nb(X_train, y_train)\n",
    "\n",
    "    # Make predictions on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report on Test Set:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "best_model = train_and_evaluate_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li4l53DjLfHh"
   },
   "source": [
    "## D. Submission\n",
    "To predict the test set target feature and submit the results to the kaggle competition platform, do the following:\n",
    "1. Create a new pipeline instance identical to the first in Data Preprocessing\n",
    "2. With the pipeline, apply `fit_transform` to the original training set before splitting, then only apply `transform` to the test set.\n",
    "3. Retrain the model on the preprocessed training set\n",
    "4. Predict the test set\n",
    "5. Make sure the submission contains the `id` and `label` column.\n",
    "\n",
    "Note: Adjust step 1 and 2 to your implementation of the preprocessing step if you don't use pipeline API from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hEqiK7jRdq99",
    "outputId": "9b6d3d13-b8cd-459d-df8c-48f8eea53b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to model_knn_submission.pkl!\n",
      "Model loaded successfully!\n",
      "\n",
      "Submission file has been created successfully!\n",
      "\n",
      "First few rows of submission file:\n",
      "    id  label\n",
      "0   48      1\n",
      "1   68      0\n",
      "2   76      1\n",
      "3  155      1\n",
      "4  167      1\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data\n",
    "X_train_submission = pd.concat([\n",
    "    train_data[binary_features],\n",
    "    val_data[binary_features]\n",
    "])\n",
    "\n",
    "y_train_submission = pd.concat([\n",
    "    train_data['label'],\n",
    "    val_data['label']\n",
    "])\n",
    "\n",
    "# Create and fit the KNN model\n",
    "model_scratch_submission = KNeighborsClassifierScratch(\n",
    "    n_neighbors=5, metric='euclidean', weights='uniform'\n",
    ")\n",
    "model_scratch_submission.fit(X_train_submission.values, None, y_train_submission.values)\n",
    "\n",
    "# Save the trained model to a pickle file\n",
    "with open('model_knn_submission.pkl', 'wb') as file:\n",
    "    pickle.dump(model_scratch_submission, file)\n",
    "print(\"Model saved successfully to model_knn_submission.pkl!\")\n",
    "\n",
    "# Load the model from the pickle file\n",
    "with open('model_knn_submission.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Use the loaded model to make predictions on df_kaggle\n",
    "kaggle_predictions = loaded_model.predict(processed_test[binary_features].values)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,  # Assuming `test_ids` contains the IDs for df_kaggle\n",
    "    'label': kaggle_predictions\n",
    "})\n",
    "\n",
    "# Save predictions to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nSubmission file has been created successfully!\")\n",
    "print(\"\\nFirst few rows of submission file:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LeqnfWc-LfHi",
    "outputId": "63cc39e4-6e76-43ba-bd23-331bff1c65cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to model_scratch_submission.pkl!\n",
      "Model loaded successfully!\n",
      "\n",
      "Submission file has been created successfully!\n",
      "\n",
      "First few rows of submission file:\n",
      "    id  label\n",
      "0   48      1\n",
      "1   68      0\n",
      "2   76      1\n",
      "3  155      1\n",
      "4  167      1\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data\n",
    "X_train_submission = pd.concat([\n",
    "    train_data[binary_features],\n",
    "    val_data[binary_features]\n",
    "])\n",
    "\n",
    "y_train_submission = pd.concat([\n",
    "    train_data['label'],\n",
    "    val_data['label']\n",
    "])\n",
    "\n",
    "# Create and fit the model\n",
    "model_scratch_submission = BernoulliNB_Scratch(alpha=0.001)\n",
    "model_scratch_submission.fit(X_train_submission, y_train_submission)\n",
    "\n",
    "# Save the trained model to a pickle file\n",
    "with open('model_scratch_submission.pkl', 'wb') as file:\n",
    "    pickle.dump(model_scratch_submission, file)\n",
    "print(\"Model saved successfully to model_scratch_submission.pkl!\")\n",
    "\n",
    "# Load the model from the pickle file\n",
    "with open('model_scratch_submission.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Use the loaded model to make predictions on df_kaggle\n",
    "kaggle_predictions = loaded_model.predict(processed_test[binary_features])\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': kaggle_predictions\n",
    "})\n",
    "\n",
    "# Save predictions to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nSubmission file has been created successfully!\")\n",
    "print(\"\\nFirst few rows of submission file:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTlqqmsqclBF"
   },
   "source": [
    "# 6. Error Analysis\n",
    "\n",
    "Based on all the process you have done until the modeling and evaluation step, write an analysis to support each steps you have taken to solve this problem. Write the analysis using the markdown block. Some questions that may help you in writing the analysis:\n",
    "\n",
    "- Does my model perform better in predicting one class than the other? If so, why is that?\n",
    "- To each models I have tried, which performs the best and what could be the reason?\n",
    "- Is it better for me to impute or drop the missing data? Why?\n",
    "- Does feature scaling help improve my model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJO56-WUcsLn"
   },
   "source": [
    "### Number 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN\n",
    "Based on the confusion matrix and classification report, our KNN model demonstrates significantly better performance in identifying non-phishing URLs (Class 1) compared to phishing URLs (Class 0). The model achieves a precision of 99% and a recall of 93% for non-phishing URLs, indicating high reliability in its positive predictions for safe URLs. However, when it comes to identifying phishing URLs, the model's performance drops considerably, with only 50% precision while maintaining a recall of 88%. This substantial performance difference is likely due to the imbalanced nature of our training dataset, where non-phishing URLs vastly outnumber phishing URLs. Although hyperparameter tuning improved the overall accuracy, the results suggest that further efforts are needed to achieve more balanced performance across both classes. Explanation of Recall and Precision for Class 1 and Class 0\n",
    "\n",
    "*   When the model says a URL is safe, it is correct 99% of the time (high precision)\n",
    "*   Out of all the actual non-phishing URLs, the model correctly identifies 93% of them (high recall)\n",
    "*   When the model says a URL is phishing, it is correct only 50% of the time (low precision)\n",
    "*   Out of all the actual phishing URLs, the model correctly identifies 88% of them (high recall).\n",
    "\n",
    "So, we can conclude that the KNN model is much better at predicting safe URLs (Class 1) compared to phishing URLs (Class 0). This is likely due to the imbalanced dataset, where the model is biased towards the majority class (safe URLs). Despite improvements through hyperparameter tuning, additional steps like class-balancing techniques or further feature engineering are necessary to improve the model's ability to identify phishing URLs while maintaining high performance for safe URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes\n",
    "Based on the confusion matrix and classification report, our Naive Bayes model demonstrates significantly better performance in identifying non-phishing URLs (Class 1) compared to phishing URLs (Class 0). The model achieves a precision of 99% and recall of 89% for non-phishing URLs, indicating high reliability in its positive predictions. However, when it comes to identifying phishing URLs, the model's performance drops considerably, with only 41% precision while maintaining a high recall of 91%.\n",
    "This substantial performance difference is likely due too the imbalanced nature of our training dataset, where non-phishing URLs have a lot more data than the phishing URLs. Although we implemented preprocessing steps to address this imbalance, the results suggest that our current approach requires further optimization to achieve more balanced performance across both classes.\n",
    "Explanation regarding the recall and precision of class 1 and class 0\n",
    "* When the model says a URL is safe, it's right 99% of the time\n",
    "* Out of all the actually safe URLs, it correctly caught 89% of them\n",
    "* When the model says a URL is phishing, it's only right 41% of the time\n",
    "* Out of all the actual phishing URLs, it caught 91% of them\n",
    "So we can conclude that the model is better at predicting a safe URL because of the imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul-Pfcw9PXQU"
   },
   "source": [
    "### Number 2\n",
    "The KNN model achieves slightly better performance than Naive Bayes on the phishing detection dataset due to its ability to capture complex feature relationships without relying on independence assumptions. Features like URLLength and ObfuscationRatio are interdependent, which KNN models effectively through distance calculations, unlike Naive Bayes. KNN also adapts better to non-Gaussian feature distributions and imbalanced data by using distance-weighted voting, improving its ability to detect phishing URLs (Class 0). Its flexibility in hyperparameter tuning and robustness to noisy features further contribute to its slight edge in performance for the dataset.\n",
    "<br><br>\n",
    "The Naive Bayes performed worse than The KNN likely due to the nature of Naive Bayes that assumes features do not correlate with other features. Theoritically, each features in the dataset has some correlations that will affect the outcome of the label, but as the name Naive suggest, Naive Bayes don't account any correlation between each feature. This is probably the main reason why Naive Bayes performs worse in this context. Although it has a worse public score, Naive Bayes performs at a much faster pace when predicting the data as it uses probability while training and only needs 1 pass for predicting unseen data.\n",
    "<br><br>\n",
    "So to conclude, KNN performs better than Naive Bayes on predicting Phishing URL due to it's algorithm, but Naive Bayes have a much faster time to train and predict unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 3\n",
    "For the dataset that is given, it's better to impute the data because of the huge amount of missing data (almost all features has a 30-50% rate of missing values from the EDA we have done earlier). If we decided to drop all of the data with missing values, we would lose a lot of the data that is important for training and validating our models. Hence, this will results to a more inaccurate machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 4\n",
    "Feature scaling is a crucial preprocessing step in building machine learning models because it normalizes the range of independent variables, ensuring all features are on a comparable scale. This is especially important for models like K-Nearest Neighbors (KNN), which rely on distance metrics to calculate proximity. Without scaling, features with larger ranges dominate distance calculations, leading to biased results and reduced accuracy. By standardizing the ranges of features, scaling ensures balanced contributions, enhancing the model's performance and reliability.\n",
    "\n",
    "In contrast, feature scaling has minimal to no direct impact on Bernoulli Naive Bayes since it operates on binary data (0s and 1s) that represent the presence or absence of features. However, when numerical data requires binarization as part of preprocessing, scaling can ensure consistent thresholds, improving the binarization process and preventing skewed results. This makes scaling indirectly beneficial for datasets with non-binary features.\n",
    "\n",
    "For our case, we use specific scaling methods to address various data characteristics: power transformers for skewed features to normalize distributions, standard scalers for general numerical features to standardize ranges, MinMax scalers for ratio features (e.g., [0,1]) to preserve relationships, and type conversion for boolean features. These techniques prevent potential issues like longer model convergence times, local optima, biased predictions favoring high-scale features, and poor interaction handling. Properly applying these scaling methods ensures our models perform efficiently and reliably across different feature types."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
